{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1728adb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import pickle\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "import gymnasium as gym\n",
    "import highway_env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9e6114",
   "metadata": {},
   "source": [
    "### T-REX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d907d7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as tdist\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim=25, hidden_dim=64, encoding_dims=10, action_dims=5):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Encoder (MLP for flattened 5x5 state)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),  # input_dim = 5 * 5 = 25\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(hidden_dim, encoding_dims)\n",
    "        self.fc_var = nn.Linear(hidden_dim, encoding_dims)  # Outputs log-variance\n",
    "        \n",
    "        # Decoder (reconstructs flattened 5x5 state)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dims, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)  # Output 25-dim state\n",
    "        )\n",
    "\n",
    "        # Reward predictor\n",
    "        self.fc_reward = nn.Linear(encoding_dims, 1)\n",
    "\n",
    "        # Temporal difference\n",
    "        self.temporal_difference = nn.Linear(2 * encoding_dims, 1, bias=False)\n",
    "        \n",
    "        # Inverse dynamics (predicts 5 discrete actions)\n",
    "        self.inverse_dynamics = nn.Linear(2 * encoding_dims, action_dims, bias=False)\n",
    "        \n",
    "        # Forward dynamics (takes 5-dim one-hot actions)\n",
    "        self.forward_dynamics = nn.Linear(encoding_dims + action_dims, encoding_dims, bias=False)\n",
    "        \n",
    "        # Normal distribution for VAE reparameterization\n",
    "        self.normal = tdist.Normal(0, 1)\n",
    "        \n",
    "        # Device setup\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "\n",
    "    def reparameterize(self, mu, var):\n",
    "        \"\"\"Reparameterize for VAE: z = mu + std * eps\"\"\"\n",
    "        if self.training:\n",
    "            std = var.mul(0.5).exp()\n",
    "            eps = self.normal.sample(mu.shape).to(self.device)\n",
    "            return eps.mul(std).add(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"Encode state to latent space\"\"\"\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        var = self.fc_var(h)  # Log-variance\n",
    "        z = self.reparameterize(mu, var)\n",
    "        return z, mu, var\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"Decode latent encoding to reconstructed state\"\"\"\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def predict_reward(self, z):\n",
    "        \"\"\"Predict per-frame reward from latent encoding\"\"\"\n",
    "        return self.fc_reward(z)\n",
    "    \n",
    "    def estimate_temporal_difference(self, z1, z2):\n",
    "        \"\"\"Predict time difference between two latent encodings\"\"\"\n",
    "        x = torch.cat((z1, z2), dim=1)\n",
    "        return self.temporal_difference(x)\n",
    " \n",
    "    def estimate_inverse_dynamics(self, z1, z2):\n",
    "        \"\"\"Predict action (logits for 5 actions) from two consecutive latent encodings\"\"\"\n",
    "        x = torch.cat((z1, z2), dim=1)\n",
    "        return self.inverse_dynamics(x)\n",
    "\n",
    "    def estimate_forward_dynamics(self, z1, action):\n",
    "        \"\"\"Predict next latent encoding from current encoding and one-hot action\"\"\"\n",
    "        x = torch.cat((z1, action), dim=1)\n",
    "        return self.forward_dynamics(x)\n",
    "\n",
    "    def cum_return(self, traj):\n",
    "        \"\"\"Calculate cumulative return and related outputs for a trajectory\"\"\"\n",
    "        # Reshape traj from (T, 1, 5, 5) to (T, 25)\n",
    "        traj = traj.view(traj.size(0), -1)  # Shape: (T, 25)\n",
    "        z, mu, var = self.encode(traj)  # Shape: (T, encoding_dims)\n",
    "        rewards = self.predict_reward(z)  # Shape: (T, 1)\n",
    "        sum_rewards = torch.sum(rewards)  # Scalar\n",
    "        sum_abs_rewards = torch.sum(torch.abs(rewards))  # Scalar\n",
    "        return sum_rewards, sum_abs_rewards, z, mu, var\n",
    "    \n",
    "    def forward(self, traj_i, traj_j):\n",
    "        \"\"\"Compute outputs for two trajectories (for T-REX ranking)\"\"\"\n",
    "        # Compute cumulative returns and latent encodings\n",
    "        cum_r_i, abs_r_i, z_i, mu_i, var_i = self.cum_return(traj_i)  # traj_i: (T_i, 1, 5, 5)\n",
    "        cum_r_j, abs_r_j, z_j, mu_j, var_j = self.cum_return(traj_j)  # traj_j: (T_j, 1, 5, 5)\n",
    "        \n",
    "        # Reconstructed states\n",
    "        recon_i = self.decode(z_i)  # Shape: (T_i, 25)\n",
    "        recon_j = self.decode(z_j)  # Shape: (T_j, 25)\n",
    "        \n",
    "        # Return logits for ranking, absolute rewards, and latent encodings\n",
    "        return (\n",
    "            torch.cat((cum_r_i.unsqueeze(0), cum_r_j.unsqueeze(0)), 0),  # Logits for ranking\n",
    "            abs_r_i + abs_r_j,  # Sum of absolute rewards\n",
    "            z_i, z_j, mu_i, mu_j, var_i, var_j,  # Latent encodings\n",
    "            recon_i, recon_j  # Reconstructed states\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00ac385",
   "metadata": {},
   "source": [
    "### Data preparation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7692a1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data_for_preferences(segments, preferences):\n",
    "    \"\"\"\n",
    "    Prepare training data using full segments and provided pairwise preferences.\n",
    "    \n",
    "    Args:\n",
    "        segments: List of segments, each a list of (state, action, reward, done) tuples.\n",
    "        preferences: List of (seg_i, seg_j, label) tuples, where label=1 means seg_i is better.\n",
    "    \n",
    "    Returns:\n",
    "        training_obs: List of (traj_i, traj_j) tuples, each traj of shape (T', 1, 5, 5).\n",
    "        training_labels: List of binary labels (1 if traj_i better, 0 if traj_j better).\n",
    "        times: List of (time_i, time_j) tuples, where time_i, time_j are lists of indices.\n",
    "        actions: List of (actions_i, actions_j) tuples, each of shape (T', 5).\n",
    "    \"\"\"\n",
    "    max_traj_length = 0\n",
    "    training_obs = []\n",
    "    training_labels = []\n",
    "    times = []\n",
    "    actions = []\n",
    "    \n",
    "    for seg_i_idx, seg_j_idx, label in preferences:\n",
    "        # Get segments\n",
    "        seg_i = segments[seg_i_idx]\n",
    "        seg_j = segments[seg_j_idx]\n",
    "        \n",
    "        # Extract full trajectories (all steps, ignoring done)\n",
    "        traj_i = np.array([seg_i[i][0] for i in range(len(seg_i))])  # Shape: (T_i', 1, 5, 5)\n",
    "        traj_j = np.array([seg_j[i][0] for i in range(len(seg_j))])  # Shape: (T_j', 1, 5, 5)\n",
    "        actions_i = np.array([seg_i[i][1] for i in range(len(seg_i))])  # Shape: (T_i', 5)\n",
    "        actions_j = np.array([seg_j[i][1] for i in range(len(seg_j))])  # Shape: (T_j', 5)\n",
    "        \n",
    "        # Store data\n",
    "        training_obs.append((traj_i, traj_j))\n",
    "        training_labels.append(label)  # 0 if seg_i better, 1 if seg_j better\n",
    "        times.append((list(range(len(seg_i))), list(range(len(seg_j)))))  # All indices\n",
    "        actions.append((actions_i, actions_j))\n",
    "        max_traj_length = max(max_traj_length, len(traj_i), len(traj_j))\n",
    "    \n",
    "    print(\"maximum traj length\", max_traj_length)\n",
    "    return training_obs, training_labels, times, actions\n",
    "\n",
    "def create_training_data_for_corrections(corrections):\n",
    "    \"\"\"\n",
    "    Prepare training data using correction feedback pairs.\n",
    "    \n",
    "    Args:\n",
    "        corrections: List of (worse_segment, better_segment) tuples, where each segment is a list of (state, action, reward, done) tuples.\n",
    "    \n",
    "    Returns:\n",
    "        training_obs: List of (traj_i, traj_j) tuples, where traj_i is from better_segment, traj_j from worse_segment, each traj of shape (T', 1, 5, 5).\n",
    "        training_labels: List of binary labels (1 meaning traj_i is better).\n",
    "        times: List of (time_i, time_j) tuples, where time_i, time_j are lists of indices.\n",
    "        actions: List of (actions_i, actions_j) tuples, each of shape (T', 5).\n",
    "    \"\"\"\n",
    "    max_traj_length = 0\n",
    "    training_obs = []\n",
    "    training_labels = []\n",
    "    times = []\n",
    "    actions = []\n",
    "    \n",
    "    for worse_seg, better_seg in corrections:\n",
    "  \n",
    "        traj_i = np.array([better_seg[i][0] for i in range(len(better_seg))])  # better: Shape: (T_i', 1, 5, 5)\n",
    "        traj_j = np.array([worse_seg[i][0] for i in range(len(worse_seg))])  # worse: Shape: (T_j', 1, 5, 5)\n",
    "        actions_i = np.array([better_seg[i][1] for i in range(len(better_seg))])  # Shape: (T_i', 5)\n",
    "        actions_j = np.array([worse_seg[i][1] for i in range(len(worse_seg))])  # Shape: (T_j', 5)\n",
    "        \n",
    "        # Store data\n",
    "        training_obs.append((traj_i, traj_j))\n",
    "        training_labels.append(1)  # 1 since traj_i (better) is preferred\n",
    "        times.append((list(range(len(better_seg))), list(range(len(worse_seg)))))  # All indices\n",
    "        actions.append((actions_i, actions_j))\n",
    "        max_traj_length = max(max_traj_length, len(traj_i), len(traj_j))\n",
    "    \n",
    "    print(\"maximum traj length\", max_traj_length)\n",
    "    return training_obs, training_labels, times, actions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c24b56",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32164306",
   "metadata": {},
   "source": [
    "#### To do\n",
    "- In correction feedback we have trajs with length one, I should change the dynamics for them, beacuse dynamics need traj with at least len 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6f14e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "def reconstruction_loss(decoded, target, mu, logvar):\n",
    "    \"\"\"Compute reconstruction loss (MSE + KL-divergence) for VAE.\"\"\"\n",
    "    mse = F.mse_loss(decoded, target, reduction='sum') / target.numel()\n",
    "    kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / target.numel()\n",
    "    return mse + kld\n",
    "\n",
    "def learn_reward(reward_network, optimizer, training_inputs, training_outputs, training_times, training_actions, num_iter, l1_reg, checkpoint_dir, loss_fn, wandb_project_name):\n",
    "    \"\"\"\n",
    "    Train the reward network using T-REX and self-supervised losses, logging to W&B.\n",
    "    \n",
    "    Args:\n",
    "        reward_network: Net model instance (input_dim=25, action_dims=5).\n",
    "        optimizer: PyTorch optimizer (e.g., Adam).\n",
    "        training_inputs: List of (traj_i, traj_j) tuples, each traj of shape (T', 1, 5, 5).\n",
    "        training_outputs: List of binary labels (1 if traj_i better, 0 if traj_j better).\n",
    "        training_times: List of (time_i, time_j) tuples.\n",
    "        training_actions: List of (actions_i, actions_j) tuples, each of shape (T', 5).\n",
    "        num_iter: Number of epochs.\n",
    "        l1_reg: L1 regularization weight (unused).\n",
    "        checkpoint_dir: Path to save model checkpoints.\n",
    "        loss_fn: Loss function type ('trex', 'ss', 'trex+ss').\n",
    "    \n",
    "    Returns:\n",
    "        reward_network: Trained Net model.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    \n",
    "    # Initialize W&B\n",
    "    wandb.init(project=wandb_project_name, config={\n",
    "        \"num_iter\": num_iter,\n",
    "        \"loss_fn\": loss_fn,\n",
    "        \"lr\": optimizer.param_groups[0]['lr'],\n",
    "        \"weight_decay\": optimizer.param_groups[0].get('weight_decay', 0.0)\n",
    "    })\n",
    "    \n",
    "    loss_criterion = nn.CrossEntropyLoss()\n",
    "    temporal_difference_loss = nn.MSELoss()\n",
    "    inverse_dynamics_loss = nn.CrossEntropyLoss()\n",
    "    forward_dynamics_loss = nn.MSELoss()\n",
    "    \n",
    "    training_data = list(zip(training_inputs, training_outputs, training_times, training_actions))\n",
    "    for epoch in range(num_iter):\n",
    "        np.random.shuffle(training_data)\n",
    "        cum_loss = 0.0\n",
    "        cum_trex_loss = 0.0\n",
    "        cum_recon_loss = 0.0\n",
    "        cum_inv_loss = 0.0\n",
    "        cum_fwd_loss = 0.0\n",
    "        cum_dt_loss = 0.0\n",
    "        \n",
    "        for i, (obs, label, times, actions) in enumerate(training_data):\n",
    "            traj_i, traj_j = torch.tensor(obs[0], dtype=torch.float32).to(device), torch.tensor(obs[1], dtype=torch.float32).to(device)  # (T_i', 1, 5, 5), (T_j', 1, 5, 5)\n",
    "            label = torch.tensor([label], dtype=torch.long).to(device)  # 1 if traj_i better\n",
    "            actions_i, actions_j = torch.tensor(actions[0], dtype=torch.float32).to(device), torch.tensor(actions[1], dtype=torch.float32).to(device)  # (T_i', 5), (T_j', 5)\n",
    "            times_i, times_j = times\n",
    "            \n",
    "            #if len(traj_i) < 2 or len(traj_j) < 2:  # Need at least 2 steps for dynamics\n",
    "            #    continue\n",
    "            ## I should change \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs, abs_rewards, z1, z2, mu1, mu2, var1, var2, recon_i, recon_j = reward_network(traj_i, traj_j)\n",
    "            \n",
    "            # Reconstruction loss (MSE + KL-divergence)\n",
    "            recon_loss_i = reconstruction_loss(recon_i, traj_i.view(traj_i.size(0), -1), mu1, var1)\n",
    "            recon_loss_j = reconstruction_loss(recon_j, traj_j.view(traj_j.size(0), -1), mu2, var2)\n",
    "            recon_loss = 10 * (recon_loss_i + recon_loss_j)\n",
    "            \n",
    "            # Inverse dynamics\n",
    "            actions_1 = reward_network.estimate_inverse_dynamics(mu1[:-1], mu1[1:])  # (T_i'-1, 5)\n",
    "            actions_2 = reward_network.estimate_inverse_dynamics(mu2[:-1], mu2[1:])  # (T_j'-1, 5)\n",
    "            target_actions_1 = torch.argmax(actions_i[1:], dim=1)  # Convert one-hot to indices\n",
    "            target_actions_2 = torch.argmax(actions_j[1:], dim=1)\n",
    "            inv_loss = (inverse_dynamics_loss(actions_1, target_actions_1) + inverse_dynamics_loss(actions_2, target_actions_2)) / 1.9\n",
    "            \n",
    "            # Forward dynamics (single-step)\n",
    "            forward_dynamics_1 = reward_network.estimate_forward_dynamics(mu1[:-1], actions_i[:-1])  # (T_i'-1, encoding_dims)\n",
    "            forward_dynamics_2 = reward_network.estimate_forward_dynamics(mu2[:-1], actions_j[:-1])  # (T_j'-1, encoding_dims)\n",
    "            fwd_loss = 100 * (forward_dynamics_loss(forward_dynamics_1, mu1[1:]) + forward_dynamics_loss(forward_dynamics_2, mu2[1:]))\n",
    "            \n",
    "            # Temporal difference\n",
    "            t1_i, t2_i = np.random.randint(0, len(times_i)), np.random.randint(0, len(times_i))\n",
    "            t1_j, t2_j = np.random.randint(0, len(times_j)), np.random.randint(0, len(times_j))\n",
    "            est_dt_i = reward_network.estimate_temporal_difference(mu1[t1_i].unsqueeze(0), mu1[t2_i].unsqueeze(0))\n",
    "            est_dt_j = reward_network.estimate_temporal_difference(mu2[t1_j].unsqueeze(0), mu2[t2_j].unsqueeze(0))\n",
    "            real_dt_i = (times_i[t2_i] - times_i[t1_i]) / 100.0\n",
    "            real_dt_j = (times_j[t2_j] - times_j[t1_j]) / 100.0\n",
    "            dt_loss = 4 * (temporal_difference_loss(est_dt_i, torch.tensor([[real_dt_i]], dtype=torch.float32, device=device)) +\n",
    "                           temporal_difference_loss(est_dt_j, torch.tensor([[real_dt_j]], dtype=torch.float32, device=device)))\n",
    "            \n",
    "            # T-REX loss\n",
    "            trex_loss = loss_criterion(outputs.unsqueeze(0), label)\n",
    "            \n",
    "            # Combine losses\n",
    "            if loss_fn == \"trex\":\n",
    "                loss = trex_loss\n",
    "            elif loss_fn == \"ss\":\n",
    "                loss = recon_loss + inv_loss + fwd_loss + dt_loss\n",
    "            elif loss_fn == \"trex+ss\":\n",
    "                loss = trex_loss + recon_loss + inv_loss + fwd_loss + dt_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Log losses to W&B\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch,\n",
    "                \"iteration\": i,\n",
    "                \"total_loss\": loss.item(),\n",
    "                \"trex_loss\": trex_loss.item(),\n",
    "                \"reconstruction_loss\": recon_loss.item(),\n",
    "                \"inverse_dynamics_loss\": inv_loss.item(),\n",
    "                \"forward_dynamics_loss\": fwd_loss.item(),\n",
    "                \"temporal_difference_loss\": dt_loss.item()\n",
    "            })\n",
    "            \n",
    "            cum_loss += loss.item()\n",
    "            cum_trex_loss += trex_loss.item()\n",
    "            cum_recon_loss += recon_loss.item()\n",
    "            cum_inv_loss += inv_loss.item()\n",
    "            cum_fwd_loss += fwd_loss.item()\n",
    "            cum_dt_loss += dt_loss.item()\n",
    "            \n",
    "            if i % 500 == 499:\n",
    "                avg_loss = cum_loss / 500\n",
    "                avg_trex_loss = cum_trex_loss / 500\n",
    "                avg_recon_loss = cum_recon_loss / 500\n",
    "                avg_inv_loss = cum_inv_loss / 500\n",
    "                avg_fwd_loss = cum_fwd_loss / 500\n",
    "                avg_dt_loss = cum_dt_loss / 500\n",
    "                print(f\"epoch {epoch}:{i} total_loss {avg_loss:.4f} \"\n",
    "                      f\"trex {avg_trex_loss:.4f} recon {avg_recon_loss:.4f} \"\n",
    "                      f\"inv {avg_inv_loss:.4f} fwd {avg_fwd_loss:.4f} dt {avg_dt_loss:.4f}\")\n",
    "                wandb.log({\n",
    "                    \"epoch\": epoch,\n",
    "                    \"iteration\": i,\n",
    "                    \"avg_total_loss\": avg_loss,\n",
    "                    \"avg_trex_loss\": avg_trex_loss,\n",
    "                    \"avg_reconstruction_loss\": avg_recon_loss,\n",
    "                    \"avg_inverse_dynamics_loss\": avg_inv_loss,\n",
    "                    \"avg_forward_dynamics_loss\": avg_fwd_loss,\n",
    "                    \"avg_temporal_difference_loss\": avg_dt_loss\n",
    "                })\n",
    "                cum_loss = 0.0\n",
    "                cum_trex_loss = 0.0\n",
    "                cum_recon_loss = 0.0\n",
    "                cum_inv_loss = 0.0\n",
    "                cum_fwd_loss = 0.0\n",
    "                cum_dt_loss = 0.0\n",
    "                torch.save(reward_network.state_dict(), checkpoint_dir)\n",
    "    \n",
    "    print(\"finished training\")\n",
    "    return reward_network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f997bc",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a4e9952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feedback data\n",
    "feedback_path = 'ppo_roundabout-v0_1377.pkl'\n",
    "\n",
    "with open(feedback_path, 'rb') as file:\n",
    "    feedback_data = pickle.load(file)\n",
    "\n",
    "segments = feedback_data['segments']\n",
    "demos = feedback_data['demos']\n",
    "preferences = feedback_data['preferences']\n",
    "corrections = feedback_data['corrections']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38d66f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preferences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "572696e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "for correction in corrections:\n",
    "    print(len(correction[0][0]))\n",
    "\n",
    "\n",
    "#len(corrections[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a5f461ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([[[ 1.0000000e+00,  1.0000000e+00,  5.0000000e-01,  2.5000000e-01,\n",
       "            0.0000000e+00],\n",
       "          [ 1.0000000e+00,  6.5435916e-02, -5.0000000e-01,  1.6522936e-05,\n",
       "            0.0000000e+00],\n",
       "          [ 1.0000000e+00,  1.4763670e-01,  0.0000000e+00,  1.3565752e-05,\n",
       "            0.0000000e+00],\n",
       "          [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "            0.0000000e+00],\n",
       "          [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "            0.0000000e+00]]], dtype=float32),\n",
       "  array([0., 1., 0., 0., 0.]),\n",
       "  0.8888888891249783,\n",
       "  True),\n",
       " (array([[[ 1.        ,  0.15      ,  0.5       ,  0.375     ,\n",
       "            0.        ],\n",
       "          [ 1.        ,  0.17514642,  0.        ,  0.00396742,\n",
       "            0.        ],\n",
       "          [ 1.        ,  0.27960327, -0.5       , -0.02131613,\n",
       "            0.        ],\n",
       "          [ 1.        ,  0.4       ,  1.        , -0.125     ,\n",
       "            0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ]]], dtype=float32),\n",
       "  array([0., 1., 0., 0., 0.]),\n",
       "  1.0,\n",
       "  False),\n",
       " (array([[[ 1.        ,  0.3       ,  0.5       ,  0.375     ,\n",
       "            0.        ],\n",
       "          [ 1.        ,  0.16284594,  0.        , -0.06708364,\n",
       "            0.        ],\n",
       "          [ 1.        ,  0.2582651 , -0.5       , -0.08131111,\n",
       "            0.        ],\n",
       "          [ 1.        ,  0.35      ,  1.        , -0.125     ,\n",
       "            0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "            0.        ]]], dtype=float32),\n",
       "  array([0., 0., 0., 0., 1.]),\n",
       "  0.9539382352570078,\n",
       "  False),\n",
       " (array([[[ 1.0000000e+00,  4.3743667e-01,  5.0000000e-01,  3.2318053e-01,\n",
       "            0.0000000e+00],\n",
       "          [ 1.0000000e+00,  1.4169045e-01,  0.0000000e+00, -4.6742912e-02,\n",
       "            0.0000000e+00],\n",
       "          [ 1.0000000e+00,  2.3337233e-01, -5.0000000e-01, -5.2359842e-02,\n",
       "            0.0000000e+00],\n",
       "          [ 1.0000000e+00,  3.1256330e-01,  1.0000000e+00, -7.3180705e-02,\n",
       "           -3.0933708e-04],\n",
       "          [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "            0.0000000e+00]]], dtype=float32),\n",
       "  array([0., 0., 0., 0., 1.]),\n",
       "  0.9000050568533211,\n",
       "  False),\n",
       " (array([[[ 1.0000000e+00,  5.5199862e-01,  5.0000000e-01,  2.6250568e-01,\n",
       "            0.0000000e+00],\n",
       "          [ 1.0000000e+00,  1.3491067e-01,  0.0000000e+00,  7.6125364e-04,\n",
       "            0.0000000e+00],\n",
       "          [ 1.0000000e+00,  2.2498791e-01, -5.0000000e-01, -1.8822871e-03,\n",
       "            0.0000000e+00],\n",
       "          [ 1.0000000e+00,  2.9785851e-01,  1.0000000e+00, -1.3350459e-02,\n",
       "           -2.0534635e-02],\n",
       "          [ 1.0000000e+00,  9.9800134e-01,  5.0000000e-01, -2.6250568e-01,\n",
       "            0.0000000e+00]]], dtype=float32),\n",
       "  array([0., 0., 0., 0., 1.]),\n",
       "  0.8907885112081384,\n",
       "  False),\n",
       " (array([[[ 1.        ,  0.6544871 ,  0.5       ,  0.25213706,\n",
       "            0.        ],\n",
       "          [ 1.        ,  0.13640009,  0.        ,  0.00478397,\n",
       "            0.        ],\n",
       "          [ 1.        ,  0.2256963 , -0.5       ,  0.00344779,\n",
       "            0.        ],\n",
       "          [ 1.        ,  0.29471397,  0.9100435 , -0.0041298 ,\n",
       "           -0.03150228],\n",
       "          [ 1.        ,  0.8955129 ,  0.5       , -0.25213706,\n",
       "            0.        ]]], dtype=float32),\n",
       "  array([0., 0., 0., 0., 1.]),\n",
       "  0.8892135119934819,\n",
       "  False),\n",
       " (array([[[ 1.        ,  0.7549124 ,  0.5       ,  0.2503652 ,\n",
       "            0.        ],\n",
       "          [ 1.        ,  0.1380681 ,  0.        ,  0.0033126 ,\n",
       "            0.        ],\n",
       "          [ 1.        ,  0.22696324, -0.5       ,  0.00261405,\n",
       "            0.        ],\n",
       "          [ 1.        ,  0.29362103,  0.6228702 , -0.0015469 ,\n",
       "           -0.02427868],\n",
       "          [ 1.        ,  0.79508764,  0.5       , -0.2503652 ,\n",
       "            0.        ]]], dtype=float32),\n",
       "  array([0., 0., 0., 0., 1.]),\n",
       "  0.8889443631581516,\n",
       "  False),\n",
       " (array([[[ 1.        ,  0.854985  ,  0.5       ,  0.2500624 ,\n",
       "            0.        ],\n",
       "          [ 1.        ,  0.13911265,  0.        ,  0.00191034,\n",
       "            0.        ],\n",
       "          [ 1.        ,  0.22779644, -0.5       ,  0.00153886,\n",
       "            0.        ],\n",
       "          [ 1.        ,  0.29339248,  0.4992282 , -0.00322868,\n",
       "           -0.00295626],\n",
       "          [ 1.        ,  0.69501495,  0.5       , -0.2500624 ,\n",
       "            0.        ]]], dtype=float32),\n",
       "  array([0., 0., 0., 0., 1.]),\n",
       "  0.8888983687888303,\n",
       "  False),\n",
       " (array([[[ 1.0000000e+00,  9.5499742e-01,  5.0000000e-01,  2.5001067e-01,\n",
       "            0.0000000e+00],\n",
       "          [ 1.0000000e+00,  1.3970089e-01,  0.0000000e+00,  1.0527031e-03,\n",
       "            0.0000000e+00],\n",
       "          [ 1.0000000e+00,  2.2827180e-01, -5.0000000e-01,  8.5338438e-04,\n",
       "            0.0000000e+00],\n",
       "          [ 1.0000000e+00,  2.8344616e-01,  1.5933651e-01, -3.3687640e-02,\n",
       "           -3.8721014e-02],\n",
       "          [ 1.0000000e+00,  5.9500253e-01,  5.0000000e-01, -2.5001067e-01,\n",
       "            0.0000000e+00]]], dtype=float32),\n",
       "  array([0., 0., 0., 0., 1.]),\n",
       "  0.8888905088922543,\n",
       "  False),\n",
       " (array([[[ 1.0000000e+00,  1.0000000e+00,  5.0000000e-01,  2.5000182e-01,\n",
       "            0.0000000e+00],\n",
       "          [ 1.0000000e+00,  1.3651815e-01,  0.0000000e+00, -3.3047531e-02,\n",
       "            0.0000000e+00],\n",
       "          [ 1.0000000e+00,  2.2853315e-01, -5.0000000e-01,  4.6540916e-04,\n",
       "            0.0000000e+00],\n",
       "          [ 1.0000000e+00,  2.7361616e-01,  1.8942423e-02, -1.7891662e-02,\n",
       "           -5.3614886e-03],\n",
       "          [ 1.0000000e+00,  4.9500045e-01,  5.0000000e-01, -2.5000182e-01,\n",
       "            0.0000000e+00]]], dtype=float32),\n",
       "  array([0., 0., 0., 0., 1.]),\n",
       "  0.888889165728404,\n",
       "  False),\n",
       " (array([[[ 1.0000000e+00,  1.0000000e+00,  5.0000000e-01,  2.5000030e-01,\n",
       "            0.0000000e+00],\n",
       "          [ 1.0000000e+00,  1.1847027e-01,  0.0000000e+00, -5.3125713e-02,\n",
       "            0.0000000e+00],\n",
       "          [ 1.0000000e+00,  2.2867534e-01, -5.0000000e-01,  2.5262986e-04,\n",
       "            0.0000000e+00],\n",
       "          [ 1.0000000e+00,  2.6804242e-01,  2.0435790e-03, -1.0162560e-02,\n",
       "           -5.8776286e-04],\n",
       "          [ 1.0000000e+00,  3.9500007e-01,  5.0000000e-01, -2.5000030e-01,\n",
       "            0.0000000e+00]]], dtype=float32),\n",
       "  array([0., 0., 0., 0., 1.]),\n",
       "  0.888888936197505,\n",
       "  False),\n",
       " (array([[[ 1.0000000e+00,  1.0000000e+00,  5.0000000e-01,  2.5000006e-01,\n",
       "            0.0000000e+00],\n",
       "          [ 1.0000000e+00,  9.6600011e-02,  0.0000000e+00, -5.4868717e-02,\n",
       "            0.0000000e+00],\n",
       "          [ 1.0000000e+00,  2.2875246e-01, -5.0000000e-01,  1.3696357e-04,\n",
       "            0.0000000e+00],\n",
       "          [ 1.0000000e+00,  2.6489857e-01,  2.1316204e-04, -5.6695193e-03,\n",
       "           -6.1606326e-05],\n",
       "          [ 1.0000000e+00,  2.9500002e-01,  5.0000000e-01, -2.5000006e-01,\n",
       "            0.0000000e+00]]], dtype=float32),\n",
       "  array([0., 0., 0., 0., 1.]),\n",
       "  0.8888888969733748,\n",
       "  False),\n",
       " (array([[[ 1.0000000e+00,  1.0000000e+00,  5.0000000e-01,  2.5000000e-01,\n",
       "            0.0000000e+00],\n",
       "          [ 1.0000000e+00,  7.5718693e-02,  0.0000000e+00, -4.4263162e-02,\n",
       "            0.0000000e+00],\n",
       "          [ 1.0000000e+00,  1.9500001e-01,  5.0000000e-01, -2.5000000e-01,\n",
       "            0.0000000e+00],\n",
       "          [ 1.0000000e+00,  2.2879428e-01, -5.0000000e-01,  7.4236719e-05,\n",
       "            0.0000000e+00],\n",
       "          [ 1.0000000e+00,  2.6315477e-01,  2.1854519e-05, -3.1233646e-03,\n",
       "           -6.3308025e-06]]], dtype=float32),\n",
       "  array([1., 0., 0., 0., 0.]),\n",
       "  0.805555556937099,\n",
       "  False),\n",
       " (array([[[ 1.0000000e+00,  1.0000000e+00,  7.7215768e-02,  2.4912418e-01,\n",
       "           -2.0907849e-02],\n",
       "          [ 1.0000000e+00,  6.3059926e-02,  4.2278424e-01, -2.6201162e-02,\n",
       "            2.0907849e-02],\n",
       "          [ 1.0000000e+00,  9.6620753e-02,  9.2278421e-01, -2.4912418e-01,\n",
       "            2.0907849e-02],\n",
       "          [ 1.0000000e+00,  2.3043770e-01, -7.7215768e-02,  9.1604789e-04,\n",
       "            2.0907849e-02],\n",
       "          [ 1.0000000e+00,  2.6381809e-01,  4.2278644e-01, -8.3248049e-04,\n",
       "            2.0907205e-02]]], dtype=float32),\n",
       "  array([1., 0., 0., 0., 0.]),\n",
       "  0.8055555557916451,\n",
       "  False),\n",
       " (array([[[ 1.0000000e+00,  1.0000000e+00,  8.0396505e-03,  2.4998924e-01,\n",
       "           -2.3189764e-03],\n",
       "          [ 1.0000000e+00, -3.3265341e-03,  9.9196035e-01, -2.4998924e-01,\n",
       "            2.3189764e-03],\n",
       "          [ 1.0000000e+00,  5.2465886e-02,  4.9196035e-01, -3.0024292e-02,\n",
       "            2.3189764e-03],\n",
       "          [ 1.0000000e+00,  2.3050268e-01, -8.0396505e-03,  3.2565822e-05,\n",
       "            2.3189764e-03],\n",
       "          [ 1.0000000e+00,  2.6334810e-01,  4.9196059e-01, -9.1981690e-04,\n",
       "            2.3189115e-03]]], dtype=float32),\n",
       "  array([0., 0., 0., 0., 1.]),\n",
       "  0.8333333333736782,\n",
       "  False),\n",
       " (array([[[ 1.0000000e+00,  1.0000000e+00,  8.1050390e-04,  2.4999990e-01,\n",
       "           -2.3523332e-04],\n",
       "          [ 1.0000000e+00,  3.9746180e-02,  4.9918950e-01, -3.3017404e-02,\n",
       "            2.3523332e-04],\n",
       "          [ 1.0000000e+00,  2.3050992e-01, -8.1050390e-04,  1.1933090e-05,\n",
       "            2.3523332e-04],\n",
       "          [ 1.0000000e+00,  2.6306427e-01,  4.9918953e-01, -5.0568406e-04,\n",
       "            2.3522676e-04],\n",
       "          [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "            0.0000000e+00]]], dtype=float32),\n",
       "  array([0., 0., 0., 0., 1.]),\n",
       "  0.8333333333402277,\n",
       "  False)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrections[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57d03f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([(array([[[ 1.0000000e+00,  1.0000000e+00,  5.0000000e-01,  2.5000030e-01,\n",
      "          0.0000000e+00],\n",
      "        [ 1.0000000e+00,  3.1123919e-02, -1.7972669e-02, -7.4623376e-02,\n",
      "          4.7270949e-03],\n",
      "        [ 1.0000000e+00,  1.4058515e-01, -4.9941915e-01, -9.9740596e-03,\n",
      "         -1.6720012e-04],\n",
      "        [ 1.0000000e+00,  2.1489862e-01,  2.1316204e-04, -5.6697777e-03,\n",
      "         -6.1606326e-05],\n",
      "        [ 1.0000000e+00,  2.4500008e-01,  5.0000000e-01, -2.5000030e-01,\n",
      "          0.0000000e+00]]], dtype=float32), array([1., 0., 0., 0., 0.]), 0.13181586689281316, True, 0)], [(array([[[ 1.0000000e+00,  1.0000000e+00,  5.0000000e-01,  2.5000000e-01,\n",
      "          0.0000000e+00],\n",
      "        [ 1.0000000e+00,  1.4385621e-01, -5.0000000e-01,  1.3172857e-05,\n",
      "          0.0000000e+00],\n",
      "        [ 1.0000000e+00,  1.6139020e-01,  2.2602894e-08, -5.0579623e-04,\n",
      "         -6.5621522e-09],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
      "          0.0000000e+00],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
      "          0.0000000e+00]]], dtype=float32), array([0., 0., 1., 0., 0.]), 0.8611111113472006, True), (array([[[ 1.        ,  0.15      ,  0.5       ,  0.375     ,\n",
      "          0.        ],\n",
      "        [ 1.        ,  0.1777136 , -0.5       ,  0.01402031,\n",
      "          0.        ],\n",
      "        [ 1.        ,  0.31838524,  0.        , -0.00314202,\n",
      "          0.        ],\n",
      "        [ 1.        ,  0.4       ,  1.        , -0.125     ,\n",
      "          0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ]]], dtype=float32), array([0., 0., 0., 1., 0.]), 1.0, False), (array([[[ 1.        ,  0.3       ,  0.5       ,  0.375     ,\n",
      "          0.        ],\n",
      "        [ 1.        ,  0.1693336 , -0.5       , -0.05970042,\n",
      "          0.        ],\n",
      "        [ 1.        ,  0.3034108 ,  0.        , -0.07160207,\n",
      "          0.        ],\n",
      "        [ 1.        ,  0.35      ,  1.        , -0.125     ,\n",
      "          0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ]]], dtype=float32), array([0., 1., 0., 0., 0.]), 1.0, False), (array([[[ 1.00000000e+00,  4.49999988e-01,  5.00000000e-01,\n",
      "          3.75000000e-01,  0.00000000e+00],\n",
      "        [ 1.00000000e+00,  1.37486726e-01, -5.00000000e-01,\n",
      "         -9.58540812e-02,  0.00000000e+00],\n",
      "        [ 1.00000000e+00,  2.68523663e-01,  0.00000000e+00,\n",
      "         -1.00288436e-01,  0.00000000e+00],\n",
      "        [ 1.00000000e+00,  2.99999982e-01,  1.00000000e+00,\n",
      "         -1.25000194e-01, -3.09337076e-04],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          0.00000000e+00,  0.00000000e+00]]], dtype=float32), array([0., 0., 0., 0., 1.]), 0.9539382352570078, False), (array([[[ 1.        ,  0.5874367 ,  0.5       ,  0.32318053,\n",
      "          0.        ],\n",
      "        [ 1.        ,  0.10859645, -0.5       , -0.05867178,\n",
      "          0.        ],\n",
      "        [ 1.        ,  0.23837891,  0.        , -0.06071608,\n",
      "          0.        ],\n",
      "        [ 1.        ,  0.26242048,  1.        , -0.07402528,\n",
      "         -0.02053463],\n",
      "        [ 1.        ,  0.96256334,  0.5       , -0.32318053,\n",
      "          0.        ]]], dtype=float32), array([0., 0., 0., 0., 1.]), 0.9000050568533211, False), (array([[[ 1.        ,  0.70199865,  0.5       ,  0.26250568,\n",
      "          0.        ],\n",
      "        [ 1.        ,  0.09837726, -0.5       , -0.00496426,\n",
      "          0.        ],\n",
      "        [ 1.        ,  0.22755836,  0.        , -0.00598803,\n",
      "          0.        ],\n",
      "        [ 1.        ,  0.24720243,  0.9100435 , -0.01449842,\n",
      "         -0.03150228],\n",
      "        [ 1.        ,  0.84800136,  0.5       , -0.26250568,\n",
      "          0.        ]]], dtype=float32), array([0., 0., 0., 0., 1.]), 0.8907885112081384, False), (array([[[ 1.        ,  0.8044871 ,  0.5       ,  0.25213706,\n",
      "          0.        ],\n",
      "        [ 1.        ,  0.09816776, -0.5       ,  0.0018632 ,\n",
      "          0.        ],\n",
      "        [ 1.        ,  0.22704221,  0.        ,  0.00133041,\n",
      "          0.        ],\n",
      "        [ 1.        ,  0.24404629,  0.6228702 , -0.00331878,\n",
      "         -0.02427868],\n",
      "        [ 1.        ,  0.7455129 ,  0.5       , -0.25213706,\n",
      "          0.        ]]], dtype=float32), array([0., 1., 0., 0., 0.]), 0.8892135119934819, False), (array([[[ 1.        ,  0.90491235,  0.5       ,  0.2503652 ,\n",
      "          0.        ],\n",
      "        [ 1.        ,  0.09895717, -0.5       ,  0.00177852,\n",
      "          0.        ],\n",
      "        [ 1.        ,  0.2276706 ,  0.        ,  0.00149586,\n",
      "          0.        ],\n",
      "        [ 1.        ,  0.24346514,  0.4992282 , -0.00353147,\n",
      "         -0.00295626],\n",
      "        [ 1.        ,  0.64508766,  0.5       , -0.2503652 ,\n",
      "          0.        ]]], dtype=float32), array([0., 1., 0., 0., 0.]), 0.7667960695475753, False), (array([[[ 1.0000000e+00,  1.0000000e+00,  5.0000000e-01,  2.5006241e-01,\n",
      "          0.0000000e+00],\n",
      "        [ 1.0000000e+00,  9.9537052e-02, -5.0000000e-01,  1.0925551e-03,\n",
      "          0.0000000e+00],\n",
      "        [ 1.0000000e+00,  2.2816466e-01,  0.0000000e+00,  9.4107946e-04,\n",
      "          0.0000000e+00],\n",
      "        [ 1.0000000e+00,  2.3480178e-01,  4.9935853e-01, -3.9962657e-02,\n",
      "          1.4689186e-04],\n",
      "        [ 1.0000000e+00,  5.4501498e-01,  5.0000000e-01, -2.5006241e-01,\n",
      "          0.0000000e+00]]], dtype=float32), array([0., 0., 0., 0., 1.]), 0.7440299731911453, False), (array([[[ 1.0000000e+00,  1.0000000e+00,  5.0000000e-01,  2.5001067e-01,\n",
      "          0.0000000e+00],\n",
      "        [ 1.0000000e+00,  9.9876672e-02, -5.0000000e-01,  6.1335706e-04,\n",
      "          0.0000000e+00],\n",
      "        [ 1.0000000e+00,  2.1310249e-01,  4.9990821e-01, -7.0583001e-02,\n",
      "          2.2849455e-05],\n",
      "        [ 1.0000000e+00,  2.2845817e-01,  0.0000000e+00,  5.3175091e-04,\n",
      "          0.0000000e+00],\n",
      "        [ 1.0000000e+00,  4.4500256e-01,  5.0000000e-01, -2.5001067e-01,\n",
      "          0.0000000e+00]]], dtype=float32), array([0., 1., 0., 0., 0.]), 0.7192826624831424, False), (array([[[ 1.00000000e+00,  1.00000000e+00,  5.00000000e-01,\n",
      "          2.50001818e-01,  0.00000000e+00],\n",
      "        [ 1.00000000e+00,  1.00064896e-01, -5.00000000e-01,\n",
      "          3.35847377e-04,  0.00000000e+00],\n",
      "        [ 1.00000000e+00,  1.78793058e-01,  4.99984950e-01,\n",
      "         -1.03972413e-01,  3.53902828e-06],\n",
      "        [ 1.00000000e+00,  2.28621513e-01,  0.00000000e+00,\n",
      "          2.91757402e-04,  0.00000000e+00],\n",
      "        [ 1.00000000e+00,  3.45000446e-01,  5.00000000e-01,\n",
      "         -2.50001818e-01,  0.00000000e+00]]], dtype=float32), array([0., 0., 0., 0., 1.]), 0.6911469347249749, False), (array([[[ 1.00000000e+00,  1.00000000e+00,  5.00000000e-01,\n",
      "          2.50000298e-01,  0.00000000e+00],\n",
      "        [ 1.00000000e+00,  1.00167565e-01, -5.00000000e-01,\n",
      "          1.82557938e-04,  0.00000000e+00],\n",
      "        [ 1.00000000e+00,  1.30237564e-01,  4.99997020e-01,\n",
      "         -1.41952321e-01,  6.48452044e-07],\n",
      "        [ 1.00000000e+00,  2.28710741e-01,  0.00000000e+00,\n",
      "          1.58700524e-04,  0.00000000e+00],\n",
      "        [ 1.00000000e+00,  2.45000079e-01,  5.00000000e-01,\n",
      "         -2.50000298e-01,  0.00000000e+00]]], dtype=float32), array([0., 0., 0., 0., 1.]), 0.6622636414179286, False), (array([[[ 1.0000000e+00,  1.0000000e+00,  5.0000000e-01,  2.5000006e-01,\n",
      "          0.0000000e+00],\n",
      "        [ 1.0000000e+00,  6.6107355e-02,  4.9999928e-01, -1.8094420e-01,\n",
      "          1.4442432e-07],\n",
      "        [ 1.0000000e+00,  1.0022332e-01, -5.0000000e-01,  9.9025900e-05,\n",
      "          0.0000000e+00],\n",
      "        [ 1.0000000e+00,  1.4500001e-01,  5.0000000e-01, -2.5000006e-01,\n",
      "          0.0000000e+00],\n",
      "        [ 1.0000000e+00,  2.2875921e-01,  0.0000000e+00,  8.6105778e-05,\n",
      "          0.0000000e+00]]], dtype=float32), array([0., 0., 0., 0., 1.]), 0.6374820248793228, False), (array([[[ 1.0000000e+00,  1.0000000e+00,  5.0000000e-01,  2.5000000e-01,\n",
      "          0.0000000e+00],\n",
      "        [ 1.0000000e+00, -1.2803023e-02,  4.9999979e-01, -2.1439929e-01,\n",
      "          3.8529745e-08],\n",
      "        [ 1.0000000e+00,  4.5000002e-02,  5.0000000e-01, -2.5000000e-01,\n",
      "          0.0000000e+00],\n",
      "        [ 1.0000000e+00,  1.0025355e-01, -5.0000000e-01,  5.3685118e-05,\n",
      "          0.0000000e+00],\n",
      "        [ 1.0000000e+00,  2.2878550e-01,  0.0000000e+00,  4.6685011e-05,\n",
      "          0.0000000e+00]]], dtype=float32), array([1., 0., 0., 0., 0.]), 0.5371299558734653, False), (array([[[ 1.0000000e+00,  1.0000000e+00,  7.7215768e-02,  2.4912418e-01,\n",
      "         -2.0907849e-02],\n",
      "        [ 1.0000000e+00,  1.0189069e-01, -7.7215768e-02,  9.0491120e-04,\n",
      "          2.0907849e-02],\n",
      "        [ 1.0000000e+00,  2.3042050e-01,  4.2278424e-01,  9.0111763e-04,\n",
      "          2.0907849e-02],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
      "          0.0000000e+00],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
      "          0.0000000e+00]]], dtype=float32), array([0., 0., 1., 0., 0.]), 0.5844625097080751, False), (array([[[ 1.0000000e+00,  1.0000000e+00,  4.3076694e-01,  2.4930762e-01,\n",
      "          1.8593293e-02],\n",
      "        [ 1.0000000e+00,  1.0300789e-01, -4.3076694e-01,  7.0815423e-04,\n",
      "         -1.8593293e-02],\n",
      "        [ 1.0000000e+00,  2.3153655e-01,  6.9233067e-02,  7.0609810e-04,\n",
      "         -1.8593293e-02],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
      "          0.0000000e+00],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
      "          0.0000000e+00]]], dtype=float32), array([0., 0., 1., 0., 0.]), 0.5813601538829299, False)])\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 10, 100):\n",
    "    print(corrections[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ac523a",
   "metadata": {},
   "source": [
    "### Making the correct order based on the cummulative reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a508060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "equal\n",
      "14.939567778938978   14.939567778938978\n",
      "equal\n",
      "14.856234445605644   14.856234445605644\n",
      "equal\n",
      "14.467345556716754   14.467345556716754\n",
      "maximum traj length 17\n"
     ]
    }
   ],
   "source": [
    "def process_preferences(preferences, segments):\n",
    "    \"\"\"\n",
    "    Process preferences by comparing segment rewards and creating debugged preferences.\n",
    "    \n",
    "    Args:\n",
    "        preferences: List of tuples (seg_i_idx, seg_j_idx, label)\n",
    "        segments: List of segments containing reward data\n",
    "    \n",
    "    Returns:\n",
    "        List of debugged preferences as tuples (seg_i_idx, seg_j_idx, 1)\n",
    "    \"\"\"\n",
    "    debugged_preferences = []\n",
    "    for seg_i_idx, seg_j_idx, label in preferences:\n",
    "        # Get segments\n",
    "        seg_i = segments[seg_i_idx]\n",
    "        seg_j = segments[seg_j_idx]\n",
    "\n",
    "        # Extract rewards\n",
    "        rewards_i = [r for _, _, r, _ in seg_i]  # Shape: List of scalars\n",
    "        rewards_j = [r for _, _, r, _ in seg_j]  # Shape: List of scalars\n",
    "        \n",
    "        # Compare sum of rewards\n",
    "        if np.sum(rewards_i) == np.sum(rewards_j):\n",
    "            print(\"equal\")\n",
    "            print(np.sum(rewards_i), \" \", np.sum(rewards_j))\n",
    "        elif np.sum(rewards_i) > np.sum(rewards_j):\n",
    "            debugged_preferences.append((seg_i_idx, seg_j_idx, 0))\n",
    "        elif np.sum(rewards_i) < np.sum(rewards_j):\n",
    "            debugged_preferences.append((seg_j_idx, seg_i_idx, 0))\n",
    "    \n",
    "    return debugged_preferences\n",
    "\n",
    "preferences = process_preferences(preferences, segments)\n",
    "training_obs, training_labels, times, actions = create_training_data_for_preferences(segments, preferences)\n",
    "#training_obs, training_labels, times, actions = create_training_data_for_corrections(corrections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b65636a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malilarian23\u001b[0m (\u001b[33ma7a7\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/alilarian/Projects/multi-type-feedback/feedback/wandb/run-20250719_151015-q4hvxedw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/a7a7/preference_model/runs/q4hvxedw' target=\"_blank\">azure-elevator-4</a></strong> to <a href='https://wandb.ai/a7a7/preference_model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/a7a7/preference_model' target=\"_blank\">https://wandb.ai/a7a7/preference_model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/a7a7/preference_model/runs/q4hvxedw' target=\"_blank\">https://wandb.ai/a7a7/preference_model/runs/q4hvxedw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0:499 total_loss 7.2034 trex 1.4687 recon 2.5114 inv 1.6674 fwd 1.5139 dt 0.0420\n",
      "epoch 0:999 total_loss 5.0563 trex 1.4379 recon 1.2214 inv 1.6304 fwd 0.7328 dt 0.0338\n",
      "epoch 0:1499 total_loss 4.4935 trex 1.3020 recon 1.1899 inv 1.5337 fwd 0.4383 dt 0.0296\n",
      "epoch 0:1999 total_loss 3.8764 trex 0.9623 recon 1.2333 inv 1.3534 fwd 0.2988 dt 0.0287\n",
      "epoch 0:2499 total_loss 3.6756 trex 0.9139 recon 1.2644 inv 1.2461 fwd 0.2202 dt 0.0310\n",
      "epoch 0:2999 total_loss 3.4339 trex 0.7647 recon 1.2547 inv 1.2013 fwd 0.1826 dt 0.0305\n",
      "epoch 0:3499 total_loss 3.4184 trex 0.8008 recon 1.2315 inv 1.1853 fwd 0.1694 dt 0.0314\n",
      "epoch 0:3999 total_loss 3.2523 trex 0.6984 recon 1.2091 inv 1.1648 fwd 0.1486 dt 0.0314\n",
      "epoch 0:4499 total_loss 3.2037 trex 0.6570 recon 1.2011 inv 1.1685 fwd 0.1448 dt 0.0322\n",
      "epoch 0:4999 total_loss 3.2520 trex 0.7440 recon 1.1871 inv 1.1482 fwd 0.1406 dt 0.0321\n",
      "epoch 0:5499 total_loss 3.1846 trex 0.6931 recon 1.1735 inv 1.1609 fwd 0.1247 dt 0.0323\n",
      "epoch 0:5999 total_loss 3.1226 trex 0.6633 recon 1.1743 inv 1.1362 fwd 0.1179 dt 0.0309\n",
      "epoch 0:6499 total_loss 3.0841 trex 0.6440 recon 1.1621 inv 1.1357 fwd 0.1141 dt 0.0281\n",
      "epoch 0:6999 total_loss 3.0813 trex 0.6412 recon 1.1574 inv 1.1388 fwd 0.1149 dt 0.0289\n",
      "epoch 0:7499 total_loss 3.0016 trex 0.5829 recon 1.1560 inv 1.1163 fwd 0.1159 dt 0.0305\n",
      "epoch 0:7999 total_loss 3.0793 trex 0.6775 recon 1.1550 inv 1.1060 fwd 0.1130 dt 0.0279\n",
      "epoch 0:8499 total_loss 3.0341 trex 0.6400 recon 1.1376 inv 1.1177 fwd 0.1124 dt 0.0265\n",
      "epoch 0:8999 total_loss 3.0092 trex 0.6369 recon 1.1370 inv 1.0974 fwd 0.1087 dt 0.0292\n",
      "epoch 0:9499 total_loss 2.9634 trex 0.6135 recon 1.1309 inv 1.0941 fwd 0.0993 dt 0.0256\n",
      "epoch 1:499 total_loss 2.8923 trex 0.5565 recon 1.1173 inv 1.0921 fwd 0.1040 dt 0.0223\n",
      "epoch 1:999 total_loss 2.9082 trex 0.5846 recon 1.1192 inv 1.0781 fwd 0.1038 dt 0.0225\n",
      "epoch 1:1499 total_loss 2.8625 trex 0.5837 recon 1.1092 inv 1.0515 fwd 0.0980 dt 0.0200\n",
      "epoch 1:1999 total_loss 2.8664 trex 0.5545 recon 1.1120 inv 1.0797 fwd 0.1026 dt 0.0177\n",
      "epoch 1:2499 total_loss 2.8639 trex 0.5599 recon 1.1155 inv 1.0639 fwd 0.1050 dt 0.0196\n",
      "epoch 1:2999 total_loss 2.8578 trex 0.5743 recon 1.1047 inv 1.0624 fwd 0.0994 dt 0.0170\n",
      "epoch 1:3499 total_loss 2.8176 trex 0.5453 recon 1.1090 inv 1.0547 fwd 0.0916 dt 0.0170\n",
      "epoch 1:3999 total_loss 2.8078 trex 0.5518 recon 1.1002 inv 1.0429 fwd 0.0951 dt 0.0178\n",
      "epoch 1:4499 total_loss 2.7968 trex 0.5405 recon 1.1051 inv 1.0360 fwd 0.0968 dt 0.0184\n",
      "epoch 1:4999 total_loss 2.7747 trex 0.5139 recon 1.1047 inv 1.0399 fwd 0.0980 dt 0.0181\n",
      "epoch 1:5499 total_loss 2.8048 trex 0.5527 recon 1.1051 inv 1.0314 fwd 0.0988 dt 0.0169\n",
      "epoch 1:5999 total_loss 2.8081 trex 0.5365 recon 1.1097 inv 1.0478 fwd 0.0956 dt 0.0184\n",
      "epoch 1:6499 total_loss 2.7530 trex 0.5444 recon 1.1052 inv 0.9898 fwd 0.0947 dt 0.0189\n",
      "epoch 1:6999 total_loss 2.7394 trex 0.5239 recon 1.1000 inv 1.0056 fwd 0.0919 dt 0.0180\n",
      "epoch 1:7499 total_loss 2.6821 trex 0.4774 recon 1.0940 inv 1.0082 fwd 0.0872 dt 0.0154\n",
      "epoch 1:7999 total_loss 2.7297 trex 0.5267 recon 1.0979 inv 0.9950 fwd 0.0913 dt 0.0187\n",
      "epoch 1:8499 total_loss 2.7053 trex 0.5334 recon 1.1021 inv 0.9607 fwd 0.0918 dt 0.0173\n",
      "epoch 1:8999 total_loss 2.7244 trex 0.5370 recon 1.1002 inv 0.9736 fwd 0.0967 dt 0.0169\n",
      "epoch 1:9499 total_loss 2.7250 trex 0.5286 recon 1.0932 inv 0.9916 fwd 0.0929 dt 0.0186\n",
      "epoch 2:499 total_loss 2.6721 trex 0.5093 recon 1.0916 inv 0.9590 fwd 0.0946 dt 0.0176\n",
      "epoch 2:999 total_loss 2.7131 trex 0.5305 recon 1.0986 inv 0.9741 fwd 0.0925 dt 0.0175\n",
      "epoch 2:1499 total_loss 2.6459 trex 0.5218 recon 1.0883 inv 0.9273 fwd 0.0923 dt 0.0162\n",
      "epoch 2:1999 total_loss 2.6790 trex 0.5360 recon 1.0928 inv 0.9408 fwd 0.0916 dt 0.0178\n",
      "epoch 2:2499 total_loss 2.6548 trex 0.5191 recon 1.0919 inv 0.9322 fwd 0.0918 dt 0.0198\n",
      "epoch 2:2999 total_loss 2.6518 trex 0.5113 recon 1.0919 inv 0.9312 fwd 0.0979 dt 0.0195\n",
      "epoch 2:3499 total_loss 2.6551 trex 0.4996 recon 1.1007 inv 0.9372 fwd 0.0978 dt 0.0199\n",
      "epoch 2:3999 total_loss 2.6454 trex 0.5064 recon 1.0947 inv 0.9231 fwd 0.0999 dt 0.0213\n",
      "epoch 2:4499 total_loss 2.6663 trex 0.5378 recon 1.0920 inv 0.9182 fwd 0.0997 dt 0.0187\n",
      "epoch 2:4999 total_loss 2.6373 trex 0.5232 recon 1.0841 inv 0.9127 fwd 0.0979 dt 0.0195\n",
      "epoch 2:5499 total_loss 2.5981 trex 0.4943 recon 1.0964 inv 0.8839 fwd 0.1040 dt 0.0195\n",
      "epoch 2:5999 total_loss 2.6124 trex 0.5095 recon 1.0913 inv 0.8950 fwd 0.0958 dt 0.0208\n",
      "epoch 2:6499 total_loss 2.6082 trex 0.5201 recon 1.0904 inv 0.8752 fwd 0.1008 dt 0.0216\n",
      "epoch 2:6999 total_loss 2.5967 trex 0.5075 recon 1.0921 inv 0.8781 fwd 0.0974 dt 0.0215\n",
      "epoch 2:7499 total_loss 2.5988 trex 0.4905 recon 1.0905 inv 0.8959 fwd 0.1023 dt 0.0197\n",
      "epoch 2:7999 total_loss 2.6062 trex 0.5055 recon 1.0910 inv 0.8848 fwd 0.1037 dt 0.0213\n",
      "epoch 2:8499 total_loss 2.5626 trex 0.5042 recon 1.0916 inv 0.8476 fwd 0.0996 dt 0.0196\n",
      "epoch 2:8999 total_loss 2.5840 trex 0.5021 recon 1.0958 inv 0.8644 fwd 0.1012 dt 0.0205\n",
      "epoch 2:9499 total_loss 2.5934 trex 0.5080 recon 1.0891 inv 0.8734 fwd 0.1015 dt 0.0215\n",
      "epoch 3:499 total_loss 2.5395 trex 0.4687 recon 1.0950 inv 0.8533 fwd 0.1026 dt 0.0199\n",
      "epoch 3:999 total_loss 2.5639 trex 0.5012 recon 1.0958 inv 0.8370 fwd 0.1083 dt 0.0216\n",
      "epoch 3:1499 total_loss 2.5474 trex 0.4909 recon 1.0923 inv 0.8382 fwd 0.1058 dt 0.0201\n",
      "epoch 3:1999 total_loss 2.5753 trex 0.5192 recon 1.0948 inv 0.8320 fwd 0.1084 dt 0.0208\n",
      "epoch 3:2499 total_loss 2.5465 trex 0.4943 recon 1.0910 inv 0.8354 fwd 0.1065 dt 0.0192\n",
      "epoch 3:2999 total_loss 2.5259 trex 0.4848 recon 1.0886 inv 0.8275 fwd 0.1041 dt 0.0209\n",
      "epoch 3:3499 total_loss 2.5445 trex 0.4987 recon 1.0953 inv 0.8225 fwd 0.1066 dt 0.0215\n",
      "epoch 3:3999 total_loss 2.5683 trex 0.5039 recon 1.0867 inv 0.8522 fwd 0.1059 dt 0.0196\n",
      "epoch 3:4499 total_loss 2.5259 trex 0.5090 recon 1.0877 inv 0.8033 fwd 0.1049 dt 0.0210\n",
      "epoch 3:4999 total_loss 2.5024 trex 0.5006 recon 1.0897 inv 0.7881 fwd 0.1030 dt 0.0211\n",
      "epoch 3:5499 total_loss 2.5027 trex 0.4753 recon 1.0945 inv 0.8063 fwd 0.1057 dt 0.0208\n",
      "epoch 3:5999 total_loss 2.5376 trex 0.4973 recon 1.0904 inv 0.8197 fwd 0.1089 dt 0.0213\n",
      "epoch 3:6499 total_loss 2.4999 trex 0.4993 recon 1.0886 inv 0.7866 fwd 0.1044 dt 0.0209\n",
      "epoch 3:6999 total_loss 2.5515 trex 0.5083 recon 1.0912 inv 0.8241 fwd 0.1076 dt 0.0203\n",
      "epoch 3:7499 total_loss 2.4552 trex 0.4635 recon 1.0894 inv 0.7754 fwd 0.1065 dt 0.0204\n",
      "epoch 3:7999 total_loss 2.4988 trex 0.4745 recon 1.0907 inv 0.8060 fwd 0.1063 dt 0.0214\n",
      "epoch 3:8499 total_loss 2.4950 trex 0.4675 recon 1.0923 inv 0.8064 fwd 0.1093 dt 0.0194\n",
      "epoch 3:8999 total_loss 2.5247 trex 0.5105 recon 1.0911 inv 0.7966 fwd 0.1073 dt 0.0192\n",
      "epoch 3:9499 total_loss 2.4718 trex 0.4829 recon 1.0897 inv 0.7693 fwd 0.1084 dt 0.0214\n",
      "epoch 4:499 total_loss 2.4879 trex 0.4832 recon 1.0899 inv 0.7953 fwd 0.1008 dt 0.0187\n",
      "epoch 4:999 total_loss 2.4885 trex 0.4825 recon 1.0896 inv 0.7876 fwd 0.1079 dt 0.0209\n",
      "epoch 4:1499 total_loss 2.4431 trex 0.4354 recon 1.0911 inv 0.7882 fwd 0.1087 dt 0.0197\n",
      "epoch 4:1999 total_loss 2.4523 trex 0.4445 recon 1.0877 inv 0.7928 fwd 0.1078 dt 0.0195\n",
      "epoch 4:2499 total_loss 2.4666 trex 0.4630 recon 1.0906 inv 0.7846 fwd 0.1081 dt 0.0203\n",
      "epoch 4:2999 total_loss 2.4698 trex 0.4731 recon 1.0860 inv 0.7829 fwd 0.1067 dt 0.0211\n",
      "epoch 4:3499 total_loss 2.4547 trex 0.4594 recon 1.0950 inv 0.7736 fwd 0.1079 dt 0.0188\n",
      "epoch 4:3999 total_loss 2.4654 trex 0.4743 recon 1.0962 inv 0.7685 fwd 0.1053 dt 0.0211\n",
      "epoch 4:4499 total_loss 2.5057 trex 0.5029 recon 1.1038 inv 0.7673 fwd 0.1114 dt 0.0203\n",
      "epoch 4:4999 total_loss 2.4271 trex 0.4415 recon 1.0960 inv 0.7615 fwd 0.1074 dt 0.0207\n",
      "epoch 4:5499 total_loss 2.4896 trex 0.5053 recon 1.0973 inv 0.7598 fwd 0.1081 dt 0.0192\n",
      "epoch 4:5999 total_loss 2.4436 trex 0.4465 recon 1.0914 inv 0.7807 fwd 0.1062 dt 0.0188\n",
      "epoch 4:6499 total_loss 2.4707 trex 0.4855 recon 1.0904 inv 0.7728 fwd 0.1012 dt 0.0208\n",
      "epoch 4:6999 total_loss 2.4556 trex 0.4792 recon 1.0880 inv 0.7676 fwd 0.1011 dt 0.0197\n",
      "epoch 4:7499 total_loss 2.4593 trex 0.4657 recon 1.0880 inv 0.7799 fwd 0.1055 dt 0.0203\n",
      "epoch 4:7999 total_loss 2.3995 trex 0.4457 recon 1.0855 inv 0.7445 fwd 0.1030 dt 0.0208\n",
      "epoch 4:8499 total_loss 2.4808 trex 0.4990 recon 1.0897 inv 0.7694 fwd 0.1024 dt 0.0204\n",
      "epoch 4:8999 total_loss 2.4238 trex 0.4553 recon 1.0932 inv 0.7535 fwd 0.1028 dt 0.0189\n",
      "epoch 4:9499 total_loss 2.4107 trex 0.4377 recon 1.0917 inv 0.7596 fwd 0.1026 dt 0.0192\n",
      "epoch 5:499 total_loss 2.4040 trex 0.4324 recon 1.0934 inv 0.7553 fwd 0.1030 dt 0.0199\n",
      "epoch 5:999 total_loss 2.4096 trex 0.4530 recon 1.0951 inv 0.7393 fwd 0.1021 dt 0.0201\n",
      "epoch 5:1499 total_loss 2.4272 trex 0.4338 recon 1.1025 inv 0.7602 fwd 0.1108 dt 0.0199\n",
      "epoch 5:1999 total_loss 2.4077 trex 0.4292 recon 1.0962 inv 0.7570 fwd 0.1065 dt 0.0189\n",
      "epoch 5:2499 total_loss 2.4240 trex 0.4686 recon 1.0955 inv 0.7353 fwd 0.1048 dt 0.0198\n",
      "epoch 5:2999 total_loss 2.4236 trex 0.4516 recon 1.0904 inv 0.7624 fwd 0.0999 dt 0.0194\n",
      "epoch 5:3499 total_loss 2.4202 trex 0.4210 recon 1.0932 inv 0.7817 fwd 0.1057 dt 0.0187\n",
      "epoch 5:3999 total_loss 2.4532 trex 0.4896 recon 1.0894 inv 0.7475 fwd 0.1078 dt 0.0189\n",
      "epoch 5:4499 total_loss 2.3509 trex 0.4130 recon 1.0933 inv 0.7214 fwd 0.1044 dt 0.0188\n",
      "epoch 5:4999 total_loss 2.4104 trex 0.4451 recon 1.0927 inv 0.7503 fwd 0.1023 dt 0.0200\n",
      "epoch 5:5499 total_loss 2.4222 trex 0.4598 recon 1.0934 inv 0.7460 fwd 0.1022 dt 0.0208\n",
      "epoch 5:5999 total_loss 2.4260 trex 0.4643 recon 1.0894 inv 0.7515 fwd 0.1010 dt 0.0199\n",
      "epoch 5:6499 total_loss 2.4509 trex 0.4779 recon 1.0968 inv 0.7522 fwd 0.1031 dt 0.0209\n",
      "epoch 5:6999 total_loss 2.4185 trex 0.4516 recon 1.0951 inv 0.7516 fwd 0.1014 dt 0.0188\n",
      "epoch 5:7499 total_loss 2.3802 trex 0.4174 recon 1.0923 inv 0.7512 fwd 0.1006 dt 0.0187\n",
      "epoch 5:7999 total_loss 2.4418 trex 0.4577 recon 1.0923 inv 0.7658 fwd 0.1076 dt 0.0183\n",
      "epoch 5:8499 total_loss 2.4262 trex 0.4647 recon 1.0912 inv 0.7474 fwd 0.1044 dt 0.0185\n",
      "epoch 5:8999 total_loss 2.4336 trex 0.4597 recon 1.0983 inv 0.7524 fwd 0.1035 dt 0.0196\n",
      "epoch 5:9499 total_loss 2.3731 trex 0.4107 recon 1.0907 inv 0.7516 fwd 0.1002 dt 0.0198\n",
      "epoch 6:499 total_loss 2.3941 trex 0.4269 recon 1.0900 inv 0.7537 fwd 0.1048 dt 0.0187\n",
      "epoch 6:999 total_loss 2.4441 trex 0.4716 recon 1.0931 inv 0.7552 fwd 0.1042 dt 0.0200\n",
      "epoch 6:1499 total_loss 2.3741 trex 0.4264 recon 1.0985 inv 0.7319 fwd 0.0985 dt 0.0189\n",
      "epoch 6:1999 total_loss 2.3865 trex 0.4355 recon 1.0986 inv 0.7276 fwd 0.1061 dt 0.0187\n",
      "epoch 6:2499 total_loss 2.3911 trex 0.4386 recon 1.1008 inv 0.7312 fwd 0.1019 dt 0.0186\n",
      "epoch 6:2999 total_loss 2.3565 trex 0.4080 recon 1.0920 inv 0.7355 fwd 0.1019 dt 0.0191\n",
      "epoch 6:3499 total_loss 2.3770 trex 0.4177 recon 1.0982 inv 0.7383 fwd 0.1037 dt 0.0190\n",
      "epoch 6:3999 total_loss 2.3875 trex 0.4256 recon 1.0952 inv 0.7488 fwd 0.1012 dt 0.0167\n",
      "epoch 6:4499 total_loss 2.3777 trex 0.4221 recon 1.0928 inv 0.7354 fwd 0.1062 dt 0.0213\n",
      "epoch 6:4999 total_loss 2.4006 trex 0.4507 recon 1.0947 inv 0.7342 fwd 0.1022 dt 0.0188\n",
      "epoch 6:5499 total_loss 2.3257 trex 0.4067 recon 1.0962 inv 0.7063 fwd 0.0977 dt 0.0188\n",
      "epoch 6:5999 total_loss 2.3321 trex 0.3872 recon 1.0968 inv 0.7284 fwd 0.1006 dt 0.0190\n",
      "epoch 6:6499 total_loss 2.3589 trex 0.4045 recon 1.0964 inv 0.7345 fwd 0.1057 dt 0.0177\n",
      "epoch 6:6999 total_loss 2.4165 trex 0.4411 recon 1.1015 inv 0.7541 fwd 0.1030 dt 0.0168\n",
      "epoch 6:7499 total_loss 2.3831 trex 0.4375 recon 1.1026 inv 0.7235 fwd 0.0997 dt 0.0198\n",
      "epoch 6:7999 total_loss 2.3912 trex 0.4470 recon 1.0961 inv 0.7231 fwd 0.1058 dt 0.0192\n",
      "epoch 6:8499 total_loss 2.3750 trex 0.4324 recon 1.1023 inv 0.7184 fwd 0.1023 dt 0.0196\n",
      "epoch 6:8999 total_loss 2.3704 trex 0.4322 recon 1.0981 inv 0.7183 fwd 0.1029 dt 0.0189\n",
      "epoch 6:9499 total_loss 2.3767 trex 0.4359 recon 1.1017 inv 0.7196 fwd 0.1014 dt 0.0181\n",
      "epoch 7:499 total_loss 2.3745 trex 0.4299 recon 1.0941 inv 0.7246 fwd 0.1063 dt 0.0197\n",
      "epoch 7:999 total_loss 2.3449 trex 0.4101 recon 1.1008 inv 0.7122 fwd 0.1027 dt 0.0190\n",
      "epoch 7:1499 total_loss 2.3883 trex 0.4185 recon 1.0986 inv 0.7489 fwd 0.1038 dt 0.0185\n",
      "epoch 7:1999 total_loss 2.3139 trex 0.3708 recon 1.1050 inv 0.7221 fwd 0.0985 dt 0.0175\n",
      "epoch 7:2499 total_loss 2.3652 trex 0.4240 recon 1.1021 inv 0.7187 fwd 0.1001 dt 0.0203\n",
      "epoch 7:2999 total_loss 2.3490 trex 0.4001 recon 1.0998 inv 0.7260 fwd 0.1055 dt 0.0176\n",
      "epoch 7:3499 total_loss 2.3819 trex 0.4404 recon 1.1038 inv 0.7170 fwd 0.1037 dt 0.0171\n",
      "epoch 7:3999 total_loss 2.3513 trex 0.3977 recon 1.0980 inv 0.7263 fwd 0.1097 dt 0.0195\n",
      "epoch 7:4499 total_loss 2.4463 trex 0.4937 recon 1.1028 inv 0.7301 fwd 0.0995 dt 0.0201\n",
      "epoch 7:4999 total_loss 2.3623 trex 0.4190 recon 1.1037 inv 0.7166 fwd 0.1042 dt 0.0187\n",
      "epoch 7:5499 total_loss 2.3327 trex 0.4103 recon 1.1021 inv 0.7028 fwd 0.0999 dt 0.0178\n",
      "epoch 7:5999 total_loss 2.3306 trex 0.4003 recon 1.0997 inv 0.7085 fwd 0.1041 dt 0.0180\n",
      "epoch 7:6499 total_loss 2.3964 trex 0.4349 recon 1.1028 inv 0.7440 fwd 0.0965 dt 0.0183\n",
      "epoch 7:6999 total_loss 2.3422 trex 0.4213 recon 1.1053 inv 0.6957 fwd 0.0995 dt 0.0204\n",
      "epoch 7:7499 total_loss 2.3628 trex 0.4400 recon 1.0909 inv 0.7140 fwd 0.1009 dt 0.0170\n",
      "epoch 7:7999 total_loss 2.3293 trex 0.3965 recon 1.0988 inv 0.7153 fwd 0.1011 dt 0.0175\n",
      "epoch 7:8499 total_loss 2.3692 trex 0.4328 recon 1.0997 inv 0.7196 fwd 0.0982 dt 0.0189\n",
      "epoch 7:8999 total_loss 2.3223 trex 0.3914 recon 1.1003 inv 0.7149 fwd 0.0984 dt 0.0173\n",
      "epoch 7:9499 total_loss 2.3291 trex 0.4229 recon 1.0956 inv 0.6948 fwd 0.0968 dt 0.0190\n",
      "epoch 8:499 total_loss 2.3035 trex 0.3709 recon 1.1033 inv 0.7126 fwd 0.0989 dt 0.0177\n",
      "epoch 8:999 total_loss 2.3925 trex 0.4642 recon 1.1042 inv 0.7040 fwd 0.1031 dt 0.0170\n",
      "epoch 8:1499 total_loss 2.3580 trex 0.4121 recon 1.1004 inv 0.7249 fwd 0.1029 dt 0.0177\n",
      "epoch 8:1999 total_loss 2.3214 trex 0.3937 recon 1.1017 inv 0.7096 fwd 0.0987 dt 0.0178\n",
      "epoch 8:2499 total_loss 2.3311 trex 0.4051 recon 1.1001 inv 0.7085 fwd 0.1010 dt 0.0165\n",
      "epoch 8:2999 total_loss 2.3544 trex 0.4075 recon 1.1083 inv 0.7193 fwd 0.1024 dt 0.0168\n",
      "epoch 8:3499 total_loss 2.3371 trex 0.4273 recon 1.1022 inv 0.6898 fwd 0.0995 dt 0.0182\n",
      "epoch 8:3999 total_loss 2.2892 trex 0.3670 recon 1.1050 inv 0.7016 fwd 0.0999 dt 0.0157\n",
      "epoch 8:4499 total_loss 2.3186 trex 0.3913 recon 1.0989 inv 0.7109 fwd 0.0991 dt 0.0184\n",
      "epoch 8:4999 total_loss 2.3172 trex 0.3973 recon 1.0958 inv 0.7106 fwd 0.0951 dt 0.0184\n",
      "epoch 8:5499 total_loss 2.3443 trex 0.4276 recon 1.1049 inv 0.6939 fwd 0.1005 dt 0.0176\n",
      "epoch 8:5999 total_loss 2.3674 trex 0.4377 recon 1.0976 inv 0.7161 fwd 0.0985 dt 0.0174\n",
      "epoch 8:6499 total_loss 2.3107 trex 0.3984 recon 1.1004 inv 0.6952 fwd 0.0990 dt 0.0177\n",
      "epoch 8:6999 total_loss 2.3381 trex 0.4159 recon 1.0958 inv 0.7145 fwd 0.0962 dt 0.0158\n",
      "epoch 8:7499 total_loss 2.3151 trex 0.4170 recon 1.1127 inv 0.6675 fwd 0.0986 dt 0.0193\n",
      "epoch 8:7999 total_loss 2.3264 trex 0.4182 recon 1.1001 inv 0.6950 fwd 0.0975 dt 0.0156\n",
      "epoch 8:8499 total_loss 2.3650 trex 0.4251 recon 1.1057 inv 0.7124 fwd 0.1034 dt 0.0183\n",
      "epoch 8:8999 total_loss 2.3274 trex 0.3852 recon 1.1048 inv 0.7222 fwd 0.0995 dt 0.0157\n",
      "epoch 8:9499 total_loss 2.3104 trex 0.4173 recon 1.1027 inv 0.6754 fwd 0.0960 dt 0.0190\n",
      "epoch 9:499 total_loss 2.2971 trex 0.3794 recon 1.1065 inv 0.6911 fwd 0.1011 dt 0.0190\n",
      "epoch 9:999 total_loss 2.3539 trex 0.4263 recon 1.1050 inv 0.7058 fwd 0.0995 dt 0.0173\n",
      "epoch 9:1499 total_loss 2.3205 trex 0.4066 recon 1.1017 inv 0.6918 fwd 0.1018 dt 0.0187\n",
      "epoch 9:1999 total_loss 2.3435 trex 0.4241 recon 1.1013 inv 0.6992 fwd 0.1011 dt 0.0179\n",
      "epoch 9:2499 total_loss 2.3128 trex 0.4149 recon 1.1095 inv 0.6743 fwd 0.0979 dt 0.0162\n",
      "epoch 9:2999 total_loss 2.3219 trex 0.4062 recon 1.1064 inv 0.6961 fwd 0.0978 dt 0.0154\n",
      "epoch 9:3499 total_loss 2.2958 trex 0.3702 recon 1.1032 inv 0.7057 fwd 0.1001 dt 0.0166\n",
      "epoch 9:3999 total_loss 2.3395 trex 0.4216 recon 1.1039 inv 0.7027 fwd 0.0962 dt 0.0151\n",
      "epoch 9:4499 total_loss 2.3141 trex 0.4186 recon 1.1029 inv 0.6829 fwd 0.0934 dt 0.0163\n",
      "epoch 9:4999 total_loss 2.3128 trex 0.4220 recon 1.0956 inv 0.6807 fwd 0.0979 dt 0.0166\n",
      "epoch 9:5499 total_loss 2.3158 trex 0.3876 recon 1.1031 inv 0.7102 fwd 0.0977 dt 0.0172\n",
      "epoch 9:5999 total_loss 2.3291 trex 0.4322 recon 1.0992 inv 0.6836 fwd 0.0967 dt 0.0175\n",
      "epoch 9:6499 total_loss 2.2639 trex 0.3708 recon 1.1058 inv 0.6719 fwd 0.0993 dt 0.0161\n",
      "epoch 9:6999 total_loss 2.3131 trex 0.3992 recon 1.0997 inv 0.6996 fwd 0.0981 dt 0.0166\n",
      "epoch 9:7499 total_loss 2.3466 trex 0.4229 recon 1.1039 inv 0.7036 fwd 0.1000 dt 0.0162\n",
      "epoch 9:7999 total_loss 2.3477 trex 0.4390 recon 1.1051 inv 0.6899 fwd 0.0971 dt 0.0166\n",
      "epoch 9:8499 total_loss 2.3522 trex 0.4225 recon 1.0993 inv 0.7154 fwd 0.0997 dt 0.0153\n",
      "epoch 9:8999 total_loss 2.3191 trex 0.4082 recon 1.1099 inv 0.6917 fwd 0.0940 dt 0.0154\n",
      "epoch 9:9499 total_loss 2.3423 trex 0.4154 recon 1.1097 inv 0.7019 fwd 0.0963 dt 0.0191\n",
      "finished training\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize model and optimizer\n",
    "reward_net = Net(input_dim=25, hidden_dim=64, encoding_dims=10, action_dims=5)\n",
    "optimizer = optim.Adam(reward_net.parameters(), lr=0.0001, weight_decay=0.001)\n",
    "\n",
    "# Train and get the trained model\n",
    "trained_model = learn_reward(\n",
    "    reward_network=reward_net,\n",
    "    optimizer=optimizer,\n",
    "    training_inputs=training_obs,\n",
    "    training_outputs=training_labels,\n",
    "    training_times=times,\n",
    "    training_actions=actions,\n",
    "    num_iter=10,\n",
    "    l1_reg=0.0,\n",
    "    checkpoint_dir=\"preference_model.pth\",\n",
    "    loss_fn=\"trex+ss\",\n",
    "    wandb_project_name=\"preference_model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fb188f",
   "metadata": {},
   "source": [
    "## Prediction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88b26f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "def predict_reward_sequence(net, segment, log_to_wandb=False):\n",
    "    \"\"\"\n",
    "    Predict per-state rewards for a segment from the Highway environment and extract true rewards.\n",
    "    \n",
    "    Args:\n",
    "        net: Net model instance (input_dim=25, action_dims=5).\n",
    "        segment: List of (state, action, reward, done) tuples, where state has shape (1, 5, 5).\n",
    "        log_to_wandb: If True, log true vs. predicted rewards to W&B.\n",
    "    \n",
    "    Returns:\n",
    "        rewards_from_obs: List of predicted per-state rewards (scalars).\n",
    "        true_rewards: List of true per-state rewards (scalars).\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Extract all states and true rewards (no filtering based on done)\n",
    "    valid_states = [s for s, _, r, _ in segment]  # Shape: List of (1, 5, 5) arrays\n",
    "    true_rewards = [r for _, _, r, _ in segment]  # Shape: List of scalars\n",
    "    \n",
    "    if not valid_states:\n",
    "        return [], []\n",
    "    \n",
    "    # Convert states to tensor\n",
    "    traj = torch.tensor(np.array(valid_states), dtype=torch.float32).to(device)  # Shape: (T, 1, 5, 5)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Compute predicted rewards for all states in a single forward pass\n",
    "        rewards, _, _, _, _ = net.cum_return(traj)  # rewards: (T, 1)\n",
    "        rewards_from_obs = rewards.squeeze(-1)  # Shape: (T,) or scalar if T=1\n",
    "        # Ensure rewards_from_obs is a list\n",
    "        if rewards_from_obs.dim() == 0:  # Single state case\n",
    "            rewards_from_obs = [rewards_from_obs.item()]\n",
    "        else:\n",
    "            rewards_from_obs = rewards_from_obs.cpu().numpy().tolist()  # Shape: (T,)\n",
    "    \n",
    "    # Log to W&B if requested\n",
    "    if log_to_wandb:\n",
    "        wandb.log({\n",
    "            \"per_state_true_rewards\": true_rewards,\n",
    "            \"per_state_predicted_rewards\": rewards_from_obs,\n",
    "            \"per_state_mae\": np.mean(np.abs(np.array(true_rewards) - np.array(rewards_from_obs))) if true_rewards else 0.0\n",
    "        })\n",
    "    \n",
    "    return rewards_from_obs, true_rewards\n",
    "\n",
    "def predict_traj_return(net, segment, log_to_wandb=False):\n",
    "    \"\"\"\n",
    "    Predict the cumulative return for a segment from the Highway environment and compute true return.\n",
    "    \n",
    "    Args:\n",
    "        net: Net model instance (input_dim=25, action_dims=5).\n",
    "        segment: List of (state, action, reward, done) tuples, where state has shape (1, 5, 5).\n",
    "        log_to_wandb: If True, log true vs. predicted cumulative return to W&B.\n",
    "    \n",
    "    Returns:\n",
    "        pred_cum_return: Scalar, the sum of predicted per-state rewards.\n",
    "        true_cum_return: Scalar, the sum of true per-state rewards.\n",
    "    \"\"\"\n",
    "    rewards, true_rewards = predict_reward_sequence(net, segment, log_to_wandb)\n",
    "    pred_cum_return = sum(rewards) if rewards else 0.0\n",
    "    true_cum_return = sum(true_rewards) if true_rewards else 0.0\n",
    "    \n",
    "    # Log to W&B if requested\n",
    "    if log_to_wandb:\n",
    "        wandb.log({\n",
    "            \"true_cumulative_return\": true_cum_return,\n",
    "            \"predicted_cumulative_return\": pred_cum_return,\n",
    "            \"cumulative_mae\": abs(true_cum_return - pred_cum_return)\n",
    "        })\n",
    "    \n",
    "    return pred_cum_return, true_cum_return\n",
    "\n",
    "def predict_all_segments(net, segments, log_to_wandb=False):\n",
    "    \"\"\"\n",
    "    Predict per-state and cumulative returns for all segments and compare with true rewards.\n",
    "    \n",
    "    Args:\n",
    "        net: Net model instance (input_dim=25, action_dims=5).\n",
    "        segments: List of segments, each a list of (state, action, reward, done) tuples.\n",
    "        log_to_wandb: If True, log results to W&B.\n",
    "    \n",
    "    Returns:\n",
    "        all_pred_rewards: List of lists, predicted per-state rewards for each segment.\n",
    "        all_true_rewards: List of lists, true per-state rewards for each segment.\n",
    "        all_pred_cum_returns: List of predicted cumulative returns for each segment.\n",
    "        all_true_cum_returns: List of true cumulative returns for each segment.\n",
    "    \"\"\"\n",
    "    all_pred_rewards = []\n",
    "    all_true_rewards = []\n",
    "    all_pred_cum_returns = []\n",
    "    all_true_cum_returns = []\n",
    "    \n",
    "    # For W&B logging\n",
    "    if log_to_wandb:\n",
    "        table_data = []\n",
    "    \n",
    "    for idx, segment in enumerate(segments):\n",
    "        # Predict rewards and cumulative return for the segment\n",
    "        pred_rewards, true_rewards = predict_reward_sequence(net, segment, log_to_wandb=False)  # Avoid redundant per-segment logging\n",
    "        pred_cum_return, true_cum_return = predict_traj_return(net, segment, log_to_wandb=False)\n",
    "        \n",
    "        # Store results\n",
    "        all_pred_rewards.append(pred_rewards)\n",
    "        all_true_rewards.append(true_rewards)\n",
    "        all_pred_cum_returns.append(pred_cum_return)\n",
    "        all_true_cum_returns.append(true_cum_return)\n",
    "        \n",
    "        # Prepare W&B table data\n",
    "        if log_to_wandb:\n",
    "            per_state_mae = np.mean(np.abs(np.array(true_rewards) - np.array(pred_rewards))) if true_rewards else 0.0\n",
    "            cum_mae = abs(true_cum_return - pred_cum_return)\n",
    "            table_data.append([idx, true_cum_return, pred_cum_return, cum_mae, per_state_mae])\n",
    "    \n",
    "    # Log aggregated results to W&B\n",
    "    if log_to_wandb:\n",
    "        # Create a W&B table for per-segment results\n",
    "        table = wandb.Table(columns=[\"segment_idx\", \"true_cum_return\", \"pred_cum_return\", \"cumulative_mae\", \"per_state_mae\"])\n",
    "        for row in table_data:\n",
    "            table.add_data(*row)\n",
    "        \n",
    "        # Log table and overall metrics\n",
    "        wandb.log({\n",
    "            \"segment_results_table\": table,\n",
    "            \"overall_per_state_mae\": np.mean([np.mean(np.abs(np.array(true) - np.array(pred))) for true, pred in zip(all_true_rewards, all_pred_rewards) if true]),\n",
    "            \"overall_cumulative_mae\": np.mean([abs(t - p) for t, p in zip(all_true_cum_returns, all_pred_cum_returns) if t != 0.0]),\n",
    "            \"true_cum_returns\": all_true_cum_returns,\n",
    "            \"pred_cum_returns\": all_pred_cum_returns\n",
    "        })\n",
    "    \n",
    "    return all_pred_rewards, all_true_rewards, all_pred_cum_returns, all_true_cum_returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e421be",
   "metadata": {},
   "source": [
    "## Test prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c7984ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_forward_dynamics_loss</td><td>█▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>avg_inverse_dynamics_loss</td><td>███▇▇▆▅▄▄▄▃▃▂▃▃▂▂▂▂▂▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>avg_reconstruction_loss</td><td>█▆▅▃▂▂▂▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▂▂▂▁▂▂▂▁▂▂▂▂</td></tr><tr><td>avg_temporal_difference_loss</td><td>██▅▄▄▂▂▂▂▁▂▃▃▄▃▃▄▃▃▂▃▃▃▂▃▃▂▂▃▂▂▂▂▃▂▂▂▁▁▁</td></tr><tr><td>avg_total_loss</td><td>█▇▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>avg_trex_loss</td><td>█▇▇▇▅▅▄▄▄▄▄▄▃▃▄▃▃▄▃▃▃▃▂▂▂▂▃▂▂▁▁▁▂▂▁▂▂▁▁▂</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>forward_dynamics_loss</td><td>▃▄▂▃▂▅▂▃▂▄▃▄▂▃▂▂▁▄▅▂▂▃▃▂▂▄▂▃▇▂▂▄▁▅█▃▃▄▂▂</td></tr><tr><td>inverse_dynamics_loss</td><td>▆▄▅▄▅▃▃▄▄▄▃▂▄▃▃▂▃▁▄▄▆▁▄▂▃▁▂▂▂▂▃▂▂▂▁▂█▂▂▂</td></tr><tr><td>iteration</td><td>▄▄▆▇▂▆▇█▁▃▆▁▁▁▅█▂▂▄▅▆▆▂▂▃▇▁▁▅▅▇█▁▂▃▄▅▇▄▇</td></tr><tr><td>reconstruction_loss</td><td>▅▂▃▅█▅▆▅▂▄▅▄▆▂▂█▆▄▄▇▆▇▃▆▆▆▃▁▄▄▅▂▄▅▂▆▆▄▇▄</td></tr><tr><td>temporal_difference_loss</td><td>▂▃▁▄▃▃▃▂▁▄▁▁▃█▃▁▁▁▂▂▄▂▁▁▂▅▂▂█▁▂▄▂▁▆▁▂▃▁▁</td></tr><tr><td>total_loss</td><td>█▃▂▃▂▂▂▂▁▂▂▁▃▂▂▁▁▂▁▂▂▂▂▂▁▂▃▄▁▃▃▂▁▁▃▂▃▂▂▂</td></tr><tr><td>trex_loss</td><td>▁▁▄▁▁▇▁▄▆▁▅▂▂▂▃▃▃▂▁▁▁▅▁▁▁▁▂▁▃▄▂▁▁▁▁█▂▆▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_forward_dynamics_loss</td><td>0.09628</td></tr><tr><td>avg_inverse_dynamics_loss</td><td>0.70193</td></tr><tr><td>avg_reconstruction_loss</td><td>1.10967</td></tr><tr><td>avg_temporal_difference_loss</td><td>0.01907</td></tr><tr><td>avg_total_loss</td><td>2.34235</td></tr><tr><td>avg_trex_loss</td><td>0.41541</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>forward_dynamics_loss</td><td>0.0762</td></tr><tr><td>inverse_dynamics_loss</td><td>0.48736</td></tr><tr><td>iteration</td><td>9996</td></tr><tr><td>reconstruction_loss</td><td>1.21493</td></tr><tr><td>temporal_difference_loss</td><td>0.02453</td></tr><tr><td>total_loss</td><td>1.81945</td></tr><tr><td>trex_loss</td><td>0.01644</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">azure-elevator-4</strong> at: <a href='https://wandb.ai/a7a7/preference_model/runs/q4hvxedw' target=\"_blank\">https://wandb.ai/a7a7/preference_model/runs/q4hvxedw</a><br> View project at: <a href='https://wandb.ai/a7a7/preference_model' target=\"_blank\">https://wandb.ai/a7a7/preference_model</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250719_151015-q4hvxedw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/alilarian/Projects/multi-type-feedback/feedback/wandb/run-20250719_220206-l2rh3z7b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/a7a7/preference_model/runs/l2rh3z7b' target=\"_blank\">all_segments_prediction</a></strong> to <a href='https://wandb.ai/a7a7/preference_model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/a7a7/preference_model' target=\"_blank\">https://wandb.ai/a7a7/preference_model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/a7a7/preference_model/runs/l2rh3z7b' target=\"_blank\">https://wandb.ai/a7a7/preference_model/runs/l2rh3z7b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment 0:\n",
      "  Predicted rewards: [13.46632194519043]\n",
      "  True rewards: [0.8888888891249783, 1.0, 0.9539382352570078, 0.9000050568533211, 0.8907885112081384, 0.8892135119934819, 0.8889443631581516, 0.8888983687888303, 0.8888905088922543, 0.888889165728404, 0.888888936197505, 0.8888888969733748, 0.805555556937099, 0.8055555557916451, 0.8333333333736782, 0.8333333333402277]\n",
      "  Predicted cumulative return: 13.46632194519043\n",
      "  True cumulative return: 14.1340122236181\n",
      "  Per-state MAE: 12.5829461812143\n",
      "  Cumulative MAE: 0.6676902784276706\n",
      "Segment 1:\n",
      "  Predicted rewards: [12.107418060302734]\n",
      "  True rewards: [0.8611111113472006, 1.0, 1.0, 0.9539382352570078, 0.9000050568533211, 0.8907885112081384, 0.8892135119934819, 0.7667960695475753, 0.7440299731911453, 0.7192826624831424, 0.6911469347249749, 0.6622636414179286, 0.6374820248793228, 0.5371299558734653, 0.5844625097080751, 0.5813601538829299]\n",
      "  Predicted cumulative return: 12.107418060302734\n",
      "  True cumulative return: 12.419010352367708\n",
      "  Per-state MAE: 11.331229913279753\n",
      "  Cumulative MAE: 0.3115922920649741\n",
      "Segment 2:\n",
      "  Predicted rewards: [12.673589706420898]\n",
      "  True rewards: [0.942822067528665, 0.9539382352570078, 0.9460668215963133, 0.944721689611825, 0.9444918223816567, 0.8983907760335026, 0.8905126496086488, 0.889166370491744, 0.8889363072301419, 0.8888969921255145, 0.8888902736366906, 0.8055557921926627, 0.8333333737718308, 0.8333333402437968, 0.8333333345142503, 0.833333333535138]\n",
      "  Predicted cumulative return: 12.673589706420898\n",
      "  True cumulative return: 14.215723179759392\n",
      "  Per-state MAE: 11.785107007685937\n",
      "  Cumulative MAE: 1.5421334733384935\n",
      "Segment 3:\n",
      "  Predicted rewards: [13.46632194519043]\n",
      "  True rewards: [0.8888888891249783, 1.0, 0.9539382352570078, 0.9000050568533211, 0.8907885112081384, 0.8892135119934819, 0.8889443631581516, 0.8888983687888303, 0.8888905088922543, 0.888889165728404, 0.888888936197505, 0.8888888969733748, 0.805555556937099, 0.8055555557916451, 0.8333333333736782, 0.8333333333402277]\n",
      "  Predicted cumulative return: 13.46632194519043\n",
      "  True cumulative return: 14.1340122236181\n",
      "  Per-state MAE: 12.5829461812143\n",
      "  Cumulative MAE: 0.6676902784276706\n",
      "Segment 4:\n",
      "  Predicted rewards: [13.594369888305664]\n",
      "  True rewards: [0.8888888891249783, 1.0, 1.0, 0.9539382352570078, 0.9000050568533211, 0.8907885112081384, 0.8892135119934819, 0.8889443631581516, 0.8888983687888303, 0.8888905088922543, 0.888889165728404, 0.888888936197505, 0.8888888969733748, 0.8888888902704324, 0.8888888891249783, 0.8888888889292338]\n",
      "  Predicted cumulative return: 13.594369888305664\n",
      "  True cumulative return: 14.522901112500094\n",
      "  Per-state MAE: 12.686688568774409\n",
      "  Cumulative MAE: 0.9285312241944297\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# # Load feedback data\n",
    "# feedback_path = 'ppo_merge-v0_1377.pkl'\n",
    "# with open(feedback_path, 'rb') as file:\n",
    "#     feedback_data = pickle.load(file)\n",
    "\n",
    "# segments = feedback_data['segments']\n",
    "# demos = feedback_data['demos']\n",
    "# preferences = feedback_data['preferences']\n",
    "\n",
    "# Initialize W&B\n",
    "wandb.init(project=\"preference_model\", name=\"all_segments_prediction\")\n",
    "\n",
    "# Load trained model\n",
    "reward_net = Net(input_dim=25, hidden_dim=64, encoding_dims=10, action_dims=5)\n",
    "reward_net.load_state_dict(torch.load(\"preference_model.pth\"))\n",
    "reward_net.eval()\n",
    "\n",
    "# Predict over all segments\n",
    "all_pred_rewards, all_true_rewards, all_pred_cum_returns, all_true_cum_returns = predict_all_segments(\n",
    "    reward_net, segments, log_to_wandb=True\n",
    ")\n",
    "\n",
    "# Print results for a few segments\n",
    "for idx, (pred_r, true_r, pred_cum, true_cum) in enumerate(zip(all_pred_rewards, all_true_rewards, all_pred_cum_returns, all_true_cum_returns)):\n",
    "    if idx >= 5:  # Limit to first 5 for brevity\n",
    "        break\n",
    "    print(f\"Segment {idx}:\")\n",
    "    print(f\"  Predicted rewards: {pred_r}\")\n",
    "    print(f\"  True rewards: {true_r}\")\n",
    "    print(f\"  Predicted cumulative return: {pred_cum}\")\n",
    "    print(f\"  True cumulative return: {true_cum}\")\n",
    "    print(f\"  Per-state MAE: {np.mean(np.abs(np.array(true_r) - np.array(pred_r))) if true_r else 0.0}\")\n",
    "    print(f\"  Cumulative MAE: {abs(true_cum - pred_cum)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37052619",
   "metadata": {},
   "source": [
    "## Computing accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "079d3ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(reward_network, training_inputs, training_outputs, log_to_wandb=False):\n",
    "    \"\"\"\n",
    "    Calculate the accuracy of the reward network in predicting trajectory preferences.\n",
    "    \n",
    "    Args:\n",
    "        reward_network: Net model instance (input_dim=25, action_dims=5).\n",
    "        training_inputs: List of (traj_i, traj_j) tuples, each traj of shape (T', 1, 5, 5).\n",
    "        training_outputs: List of binary labels (1 if traj_i better, 0 if traj_j better).\n",
    "        log_to_wandb: If True, log accuracy to W&B.\n",
    "    \n",
    "    Returns:\n",
    "        accuracy: Fraction of correctly predicted preferences (float).\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    reward_network.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    num_correct = 0.0\n",
    "    total_valid_pairs = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (traj_pair, label) in enumerate(zip(training_inputs, training_outputs)):\n",
    "            traj_i, traj_j = traj_pair\n",
    "            if len(traj_i) < 2 or len(traj_j) < 2:  # Need at least 2 steps for dynamics\n",
    "                continue\n",
    "            \n",
    "            total_valid_pairs += 1\n",
    "            traj_i = torch.tensor(traj_i, dtype=torch.float32).to(device)  # (T_i', 1, 5, 5)\n",
    "            traj_j = torch.tensor(traj_j, dtype=torch.float32).to(device)  # (T_j', 1, 5, 5)\n",
    "            label = torch.tensor([label], dtype=torch.long).to(device)  # 1 if traj_i better\n",
    "            \n",
    "            # Forward pass to get cumulative rewards\n",
    "            outputs, _, _, _, _, _, _, _, _, _ = reward_network.forward(traj_i, traj_j)  # outputs: (2,)\n",
    "            #print(outputs)\n",
    "         \n",
    "            _, pred_label = torch.max(outputs, 0)  # Predict 0 (traj_i better) or 1 (traj_j better)\n",
    "            \n",
    "  \n",
    "\n",
    "            # Compare predicted and true labels\n",
    "            if pred_label == label.item():\n",
    "                num_correct += 1.0\n",
    "        \n",
    "    # Compute accuracy\n",
    "    accuracy = num_correct / total_valid_pairs if total_valid_pairs > 0 else 0.0\n",
    "    \n",
    "    # Log to W&B if requested\n",
    "    if log_to_wandb:\n",
    "        wandb.log({\n",
    "            \"accuracy\": accuracy,\n",
    "            \"num_correct\": num_correct,\n",
    "            \"total_valid_pairs\": total_valid_pairs\n",
    "        })\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51100a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8981\n"
     ]
    }
   ],
   "source": [
    "# Load trained model\n",
    "reward_net = Net(input_dim=25, hidden_dim=64, encoding_dims=10, action_dims=5)\n",
    "reward_net.load_state_dict(torch.load(\"preference_model.pth\"))\n",
    "reward_net.eval()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = calc_accuracy(reward_net, training_obs, training_labels, log_to_wandb=False)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258de51b",
   "metadata": {},
   "source": [
    "## Entropy with importance sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1330b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Bayesian REX MCMC with increasing preference sizes...\n",
      "\n",
      "Processing preferences 0:1000 (Total: 1000 preferences)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCMC Step 0, Acceptance Rate: 1.0000\n",
      "MCMC Step 10000, Acceptance Rate: 0.9914\n",
      "MCMC Step 20000, Acceptance Rate: 0.9927\n",
      "MCMC Step 30000, Acceptance Rate: 0.9931\n",
      "MCMC Step 40000, Acceptance Rate: 0.9937\n",
      "MCMC Step 50000, Acceptance Rate: 0.9939\n",
      "MCMC Step 60000, Acceptance Rate: 0.9936\n",
      "MCMC Step 70000, Acceptance Rate: 0.9935\n",
      "MCMC Step 80000, Acceptance Rate: 0.9933\n",
      "MCMC Step 90000, Acceptance Rate: 0.9933\n",
      "MCMC Step 100000, Acceptance Rate: 0.9934\n",
      "MCMC Step 110000, Acceptance Rate: 0.9933\n",
      "MCMC Step 120000, Acceptance Rate: 0.9932\n",
      "MCMC Step 130000, Acceptance Rate: 0.9932\n",
      "MCMC Step 140000, Acceptance Rate: 0.9932\n",
      "MCMC Step 150000, Acceptance Rate: 0.9930\n",
      "MCMC Step 160000, Acceptance Rate: 0.9926\n",
      "MCMC Step 170000, Acceptance Rate: 0.9921\n",
      "MCMC Step 180000, Acceptance Rate: 0.9917\n",
      "MCMC Step 190000, Acceptance Rate: 0.9914\n",
      "MCMC Step 200000, Acceptance Rate: 0.9913\n",
      "Final Acceptance Rate: 0.9911\n",
      "MAP Log Posterior: -617.5220947265625\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 293. GiB for an array with shape (198222, 198222) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 282\u001b[0m\n\u001b[1;32m    279\u001b[0m Phi_tau_i, Phi_tau_j \u001b[38;5;241m=\u001b[39m encode_preferences(net, pref_subset, segments)\n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m# Step 2: Perform MCMC sampling and compute entropy\u001b[39;00m\n\u001b[0;32m--> 282\u001b[0m posterior_samples, map_solution, entropy, sample_based_entropy \u001b[38;5;241m=\u001b[39m \u001b[43mbayesian_rex_mcmc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mPhi_tau_i\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mPhi_tau_j\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mburn_in\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproposal_std\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\n\u001b[1;32m    290\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;66;03m# Store results\u001b[39;00m\n\u001b[1;32m    293\u001b[0m entropies\u001b[38;5;241m.\u001b[39mappend(entropy)\n",
      "Cell \u001b[0;32mIn[25], line 233\u001b[0m, in \u001b[0;36mbayesian_rex_mcmc\u001b[0;34m(net, Phi_tau_i, Phi_tau_j, num_samples, burn_in, beta, proposal_std, update_model_with_map)\u001b[0m\n\u001b[1;32m    231\u001b[0m samples \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(samples)\n\u001b[1;32m    232\u001b[0m entropy \u001b[38;5;241m=\u001b[39m compute_entropy_importance_sampling(Phi_tau_i, Phi_tau_j, samples, beta)\n\u001b[0;32m--> 233\u001b[0m sample_based_entropy \u001b[38;5;241m=\u001b[39m \u001b[43mkozachenko_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m w_map \u001b[38;5;241m=\u001b[39m w_map\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m update_model_with_map:\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;66;03m# Update net with MAP weights\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[25], line 147\u001b[0m, in \u001b[0;36mkozachenko_entropy\u001b[0;34m(samples, epsilon)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNeed at least 2 samples for entropy estimation.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# Compute pairwise Euclidean distances\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m distances \u001b[38;5;241m=\u001b[39m \u001b[43mcdist\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meuclidean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# Set diagonal to infinity to exclude self-distances\u001b[39;00m\n\u001b[1;32m    149\u001b[0m np\u001b[38;5;241m.\u001b[39mfill_diagonal(distances, np\u001b[38;5;241m.\u001b[39minf)\n",
      "File \u001b[0;32m~/Projects/multi-type-feedback/env/lib/python3.9/site-packages/scipy/spatial/distance.py:2980\u001b[0m, in \u001b[0;36mcdist\u001b[0;34m(XA, XB, metric, out, **kwargs)\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metric_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2979\u001b[0m     cdist_fn \u001b[38;5;241m=\u001b[39m metric_info\u001b[38;5;241m.\u001b[39mcdist_func\n\u001b[0;32m-> 2980\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcdist_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2981\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mstr\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   2982\u001b[0m     metric_info \u001b[38;5;241m=\u001b[39m _TEST_METRICS\u001b[38;5;241m.\u001b[39mget(mstr, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 293. GiB for an array with shape (198222, 198222) and data type float64"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as tdist\n",
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.special import gammaln\n",
    "\n",
    "# Assuming device is defined\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Function to encode preferences and compute Phi_tau_i, Phi_tau_j\n",
    "def encode_preferences(net, preferences, segments):\n",
    "    # Ensure net is frozen\n",
    "    for param in net.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Precompute Phi_tau (sum of embeddings over each trajectory using mean vector mu)\n",
    "    Phi_tau_list = []\n",
    "    for segment in segments:\n",
    "        # Use all states, ignoring done\n",
    "        seg_states = torch.tensor(np.array([state_t.flatten() for state_t, _, _, _ in segment]), dtype=torch.float32).to(device)  # (T, 25)\n",
    "        if not seg_states.size(0):  # Check if segment is empty\n",
    "            Phi_tau_list.append(None)\n",
    "            continue\n",
    "        with torch.no_grad():\n",
    "            _, mu, _ = net.encode(seg_states)  # mu: (T, encoding_dims)\n",
    "            Phi_tau = mu.sum(dim=0)/mu.size(0)  # Using avg instead of sum over trajectory: (encoding_dims,)\n",
    "        Phi_tau_list.append(Phi_tau)\n",
    "    \n",
    "    # Precompute Phi_tau_i (better) and Phi_tau_j (worse) for valid preference pairs\n",
    "    # Assuming preferences = [(better_idx, worse_idx, label)]\n",
    "    valid_pairs = [(b, w) for b, w, _ in preferences if Phi_tau_list[b] is not None and Phi_tau_list[w] is not None]\n",
    "    if not valid_pairs:\n",
    "        raise ValueError(\"No valid preference pairs found after filtering None values.\")\n",
    "    \n",
    "    Phi_tau_i = torch.stack([Phi_tau_list[b] for b, w in valid_pairs])  # better: (num_pairs, encoding_dims)\n",
    "    Phi_tau_j = torch.stack([Phi_tau_list[w] for b, w in valid_pairs])  # worse: (num_pairs, encoding_dims)\n",
    "    \n",
    "    return Phi_tau_i, Phi_tau_j\n",
    "\n",
    "# Function to encode corrections and compute Phi_tau_i, Phi_tau_j\n",
    "def encode_corrections(net, corrections):\n",
    "    # Ensure net is frozen\n",
    "    for param in net.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Precompute Phi_tau for better and worse segments in each correction pair\n",
    "    Phi_tau_better_list = []\n",
    "    Phi_tau_worse_list = []\n",
    "    for worse_seg, better_seg in corrections:\n",
    "        # Compute for better_seg\n",
    "        better_states = torch.tensor(np.array([state_t.flatten() for state_t, _, _, _ in better_seg]), dtype=torch.float32).to(device)  # (T_b, 25)\n",
    "        if not better_states.size(0):  # Check if better_seg is empty\n",
    "            continue  # Skip this pair if better_seg is empty\n",
    "        \n",
    "        # Compute for worse_seg\n",
    "        worse_states = torch.tensor(np.array([state_t.flatten() for state_t, _, _, _ in worse_seg]), dtype=torch.float32).to(device)  # (T_w, 25)\n",
    "        if not worse_states.size(0):  # Check if worse_seg is empty\n",
    "            continue  # Skip this pair if worse_seg is empty\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Encode better_seg\n",
    "            _, mu_better, _ = net.encode(better_states)  # mu_better: (T_b, encoding_dims)\n",
    "            Phi_tau_better = mu_better.sum(dim=0)  # Sum over trajectory: (encoding_dims,)\n",
    "            \n",
    "            # Encode worse_seg\n",
    "            _, mu_worse, _ = net.encode(worse_states)  # mu_worse: (T_w, encoding_dims)\n",
    "            Phi_tau_worse = mu_worse.sum(dim=0)  # Sum over trajectory: (encoding_dims,)\n",
    "        \n",
    "        Phi_tau_better_list.append(Phi_tau_better)\n",
    "        Phi_tau_worse_list.append(Phi_tau_worse)\n",
    "    \n",
    "    if not Phi_tau_better_list:\n",
    "        raise ValueError(\"No valid correction pairs found after filtering empty segments.\")\n",
    "    \n",
    "    Phi_tau_i = torch.stack(Phi_tau_better_list)  # better: (num_pairs, encoding_dims)\n",
    "    Phi_tau_j = torch.stack(Phi_tau_worse_list)  # worse: (num_pairs, encoding_dims)\n",
    "    \n",
    "    return Phi_tau_i, Phi_tau_j\n",
    "\n",
    "\n",
    "# Compute entropy using importance sampling with numerical stability\n",
    "def compute_entropy_importance_sampling(Phi_tau_i, Phi_tau_j, samples, beta):\n",
    "    num_mcmc_samples = len(samples)\n",
    "    \n",
    "    # Convert samples to a tensor for vectorized computation\n",
    "    samples_tensor = torch.tensor(samples, dtype=torch.float32).to(device)  # Shape: (num_samples, encoding_dims)\n",
    "    \n",
    "    # Define the log likelihood function with numerical stability\n",
    "    def log_likelihood_vectorized(w_batch, Phi_tau_i, Phi_tau_j, beta):\n",
    "        R_i = torch.matmul(Phi_tau_i, w_batch.T)  # Shape: (num_pairs, num_samples)\n",
    "        R_j = torch.matmul(Phi_tau_j, w_batch.T)  # Shape: (num_pairs, num_samples)\n",
    "        \n",
    "        # Numerically stable computation of log(exp(beta * R_i) + exp(beta * R_j))\n",
    "        beta_R_i = beta * R_i\n",
    "        beta_R_j = beta * R_j\n",
    "        max_val = torch.maximum(beta_R_i, beta_R_j)\n",
    "        log_sum_exp = max_val + torch.log(\n",
    "            torch.exp(beta_R_i - max_val) + torch.exp(beta_R_j - max_val)\n",
    "        )\n",
    "        \n",
    "        terms = beta * R_i - log_sum_exp  # Favor R_i > R_j (better > worse)\n",
    "        log_likelihoods = torch.sum(terms, dim=0)  # Shape: (num_samples,)\n",
    "        return log_likelihoods\n",
    "    \n",
    "    # Compute log likelihoods for all samples in parallel\n",
    "    log_probs = log_likelihood_vectorized(samples_tensor, Phi_tau_i, Phi_tau_j, beta)\n",
    "    log_probs = log_probs.cpu().numpy()\n",
    "    \n",
    "    # Check for invalid values\n",
    "    if np.any(np.isnan(log_probs)) or np.any(np.isinf(log_probs)):\n",
    "        raise ValueError(\"Log probabilities contain NaN or Inf values, indicating numerical instability.\")\n",
    "    \n",
    "    # First term: - (1 / m) * sum(log P(D, P | w^{(k)}))\n",
    "    first_term = -np.mean(log_probs)\n",
    "    \n",
    "    # Second term: -log( (1 / m) * sum( exp(-log P(D, P | w^{(k)})) ) )\n",
    "    neg_log_probs = -log_probs\n",
    "    second_term = -logsumexp(neg_log_probs) + np.log(num_mcmc_samples)\n",
    "    \n",
    "    # Entropy estimate\n",
    "    entropy = first_term + second_term\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def kozachenko_entropy(samples, epsilon=1e-10):\n",
    "    \"\"\"\n",
    "    Compute entropy using the Kozachenko-Leonenko estimator.\n",
    "    \n",
    "    Parameters:\n",
    "    - samples: numpy array of shape (m, d), where m is the number of samples and d is the dimension\n",
    "    - epsilon: small value to avoid log(0) issues (default: 1e-10)\n",
    "    \n",
    "    Returns:\n",
    "    - entropy: estimated differential entropy\n",
    "    \"\"\"\n",
    "    m, d = samples.shape\n",
    "    if m < 2:\n",
    "        raise ValueError(\"Need at least 2 samples for entropy estimation.\")\n",
    "\n",
    "    # Compute pairwise Euclidean distances\n",
    "    distances = cdist(samples, samples, metric='euclidean')\n",
    "    # Set diagonal to infinity to exclude self-distances\n",
    "    np.fill_diagonal(distances, np.inf)\n",
    "    # Get nearest neighbor distances\n",
    "    rho = np.min(distances, axis=1)\n",
    "\n",
    "    # Replace zero distances with epsilon to avoid log(0)\n",
    "    if np.any(rho == 0):\n",
    "        print(\"Warning: Zero distances found; replacing with epsilon.\")\n",
    "        rho = np.maximum(rho, epsilon)\n",
    "\n",
    "    # Constants for the KL estimator\n",
    "    euler_mascheroni = 0.5772156649  # Euler-Mascheroni constant\n",
    "    log_ball_volume = gammaln(d / 2 + 1) - (d / 2) * np.log(np.pi)  # Log of volume of d-dimensional unit ball\n",
    "\n",
    "    # Compute entropy\n",
    "    entropy = (d / m) * np.sum(np.log(rho)) + log_ball_volume + np.log(m - 1) + euler_mascheroni\n",
    "    return entropy\n",
    "\n",
    "\n",
    "# MCMC for Bayesian REX with parallelized log_likelihood, MAP tracking, and weight normalization\n",
    "def bayesian_rex_mcmc(net, Phi_tau_i, Phi_tau_j, num_samples=10000, burn_in=1000, beta=1.0, proposal_std=0.005, update_model_with_map=False):\n",
    "    # Get initial weights w from net (fc_reward linear layer)\n",
    "    w_current = net.fc_reward.weight.data.clone().squeeze().to(device)  # Shape: (encoding_dims,)\n",
    "    \n",
    "    # Normalize initial weights to have L2 norm of 1\n",
    "    w_current = w_current / torch.norm(w_current, p=2)\n",
    "    \n",
    "    # Prior: Assume a standard normal prior N(0, 1) for each weight\n",
    "    def log_prior(w):\n",
    "        return -0.5 * torch.sum(w ** 2)  # Log of N(0, 1)\n",
    "    \n",
    "    # Parallelized Likelihood: Bradley-Terry model with numerical stability\n",
    "    def log_likelihood(w, Phi_tau_i, Phi_tau_j, beta):\n",
    "        R_i = torch.matmul(Phi_tau_i, w)  # Shape: (num_pairs,)\n",
    "        R_j = torch.matmul(Phi_tau_j, w)  # Shape: (num_pairs,)\n",
    "        \n",
    "        beta_R_i = beta * R_i\n",
    "        beta_R_j = beta * R_j\n",
    "        max_val = torch.maximum(beta_R_i, beta_R_j)\n",
    "        log_sum_exp = max_val + torch.log(\n",
    "            torch.exp(beta_R_i - max_val) + torch.exp(beta_R_j - max_val)\n",
    "        )\n",
    "        \n",
    "        terms = beta * R_i - log_sum_exp  # Favor R_i > R_j (better > worse)\n",
    "        return torch.sum(terms)\n",
    "    \n",
    "    # MCMC sampling using Metropolis-Hastings with MAP tracking\n",
    "    samples = []\n",
    "    accepted = 0\n",
    "    w_map = w_current.clone()\n",
    "    log_posterior_map = log_prior(w_current) + log_likelihood(w_current, Phi_tau_i, Phi_tau_j, beta)\n",
    "    \n",
    "    for step in range(num_samples + burn_in):\n",
    "        # Propose new weights\n",
    "        w_proposed = w_current + torch.normal(mean=0, std=proposal_std, size=w_current.shape).to(device)\n",
    "        \n",
    "        # Normalize proposed weights to have L2 norm of 1\n",
    "        w_proposed = w_proposed / torch.norm(w_proposed, p=2)\n",
    "        \n",
    "        log_posterior_current = log_prior(w_current) + log_likelihood(w_current, Phi_tau_i, Phi_tau_j, beta)\n",
    "        log_posterior_proposed = log_prior(w_proposed) + log_likelihood(w_proposed, Phi_tau_i, Phi_tau_j, beta)\n",
    "        \n",
    "        if log_posterior_proposed > log_posterior_map:\n",
    "            w_map = w_proposed.clone()\n",
    "            log_posterior_map = log_posterior_proposed\n",
    "        \n",
    "        log_alpha = log_posterior_proposed - log_posterior_current\n",
    "        alpha = torch.exp(log_alpha.clamp(max=0.0))  # Clamp for stability\n",
    "        \n",
    "        if torch.rand(1, device=device) < alpha:\n",
    "            w_current = w_proposed\n",
    "            accepted += 1\n",
    "            if step >= burn_in:\n",
    "                samples.append(w_current.cpu().detach().numpy())\n",
    "        \n",
    "        if step % 10000 == 0:\n",
    "            acceptance_rate = accepted / (step + 1)\n",
    "            print(f\"MCMC Step {step}, Acceptance Rate: {acceptance_rate:.4f}\")\n",
    "    \n",
    "    print(f\"Final Acceptance Rate: {accepted / (num_samples + burn_in):.4f}\")\n",
    "    print(f\"MAP Log Posterior: {log_posterior_map.item()}\")\n",
    "    \n",
    "    # Compute entropy\n",
    "    samples = np.array(samples)\n",
    "    entropy = compute_entropy_importance_sampling(Phi_tau_i, Phi_tau_j, samples, beta)\n",
    "    sample_based_entropy = kozachenko_entropy(samples)\n",
    "    \n",
    "    w_map = w_map.cpu().detach().numpy()\n",
    "\n",
    "    if update_model_with_map:\n",
    "        # Update net with MAP weights\n",
    "        net.fc_reward.weight.data = torch.from_numpy(w_map).unsqueeze(0).to(device)  # (1, encoding_dims)\n",
    "\n",
    "    return samples, w_map, entropy, sample_based_entropy\n",
    "\n",
    "# Main execution with model loading\n",
    "if __name__ == \"__main__\":\n",
    "    # Load feedback data\n",
    "    # feedback_path = 'ppo_merge-v0_1377.pkl'\n",
    "    # with open(feedback_path, 'rb') as file:\n",
    "    #     feedback_data = pickle.load(file)\n",
    "    \n",
    "    # segments = feedback_data['segments']\n",
    "    # preferences = feedback_data['preferences']\n",
    "    \n",
    "    # Initialize the single Net model\n",
    "    net = Net(input_dim=25, encoding_dims=10)  # Adjust params as needed\n",
    "    \n",
    "    # Load the saved model (assume saved as 'reward_network.pth' from learn_reward)\n",
    "    net.load_state_dict(torch.load(\"preference_model.pth\", map_location=device))\n",
    "    \n",
    "    # Move model to the appropriate device\n",
    "    net = net.to(device)\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    net.eval()\n",
    "    \n",
    "    # Updated iteration loop\n",
    "    print(\"Running Bayesian REX MCMC with increasing preference sizes...\")\n",
    "    preference_sizes = range(1000, len(preferences) + 1000, 1000)  # 100, 200, ..., up to total preferences\n",
    "    entropies = []\n",
    "    kozachenko_entropies = []\n",
    "    all_posterior_samples = []\n",
    "    all_map_solutions = []\n",
    "    \n",
    "    for pref_size in preference_sizes:\n",
    "        # Take preferences from 0 to pref_size\n",
    "        pref_subset = preferences[:pref_size]\n",
    "        print(f\"\\nProcessing preferences 0:{pref_size} (Total: {len(pref_subset)} preferences)\")\n",
    "        \n",
    "        # Step 1: Encode preferences\n",
    "        Phi_tau_i, Phi_tau_j = encode_preferences(net, pref_subset, segments)\n",
    "        \n",
    "        # Step 2: Perform MCMC sampling and compute entropy\n",
    "        posterior_samples, map_solution, entropy, sample_based_entropy = bayesian_rex_mcmc(\n",
    "            net,\n",
    "            Phi_tau_i,\n",
    "            Phi_tau_j,\n",
    "            num_samples=1000000,\n",
    "            burn_in=5000,\n",
    "            beta=1.0,\n",
    "            proposal_std=0.001\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        entropies.append(entropy)\n",
    "        kozachenko_entropies.append(sample_based_entropy)\n",
    "        all_posterior_samples.append(posterior_samples)\n",
    "        all_map_solutions.append(map_solution)\n",
    "        \n",
    "        # Print entropy for this subset\n",
    "        print(f\"Entropy with {pref_size} preferences: {entropy:.4f}\")\n",
    "        print(f\"Koazachenki Entropy with {pref_size} preferences: {sample_based_entropy:.4f}\")\n",
    "        \n",
    "        \n",
    "    # Save results\n",
    "    np.save(\"entropies.npy\", np.array(entropies))\n",
    "    np.save(\"kozachenko_entropies.npy\", np.array(kozachenko_entropies))\n",
    "    for i, pref_size in enumerate(preference_sizes):\n",
    "        np.save(f\"posterior_samples_{pref_size}.npy\", all_posterior_samples[i])\n",
    "        np.save(f\"map_solution_{pref_size}.npy\", all_map_solutions[i])\n",
    "    \n",
    "    print(\"\\nSummary of Entropies:\")\n",
    "    for pref_size, entropy in zip(preference_sizes, entropies):\n",
    "        print(f\"Preferences 0:{pref_size}: Entropy = {entropy:.4f}\")\n",
    "    \n",
    "    print(\"Results saved: entropies.npy, posterior_samples_*.npy, map_solution_*.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2284d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac93059a",
   "metadata": {},
   "outputs": [],
   "source": [
    "koz_entropy = []\n",
    "\n",
    "for samples in all_posterior_samples:\n",
    "    koz_entropy.append(kozachenko_leonenko_entropy(samples=samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d88c2272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x76c6da377e50>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(koz_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0624f6db",
   "metadata": {},
   "source": [
    "## Recursively compute marginal likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69c7170",
   "metadata": {},
   "source": [
    "## Debugged version to fix entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69d0105c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preferences have been shuffled.\n",
      "Models loaded from 'feature_encoder_pretrain.pth' and 'trex_model_finetune.pth'\n",
      "Running Bayesian REX MCMC with recursive marginal likelihood estimation...\n",
      "\n",
      "Processing preferences 0:1000\n",
      "Log P(H_0:1000 | H_0:0) = -1200.9311\n",
      "Cumulative Log P(H_0:1000) = -1200.9311\n",
      "MCMC Step 0, Acceptance Rate: 0.0000\n",
      "MCMC Step 5000, Acceptance Rate: 0.4487\n",
      "MCMC Step 10000, Acceptance Rate: 0.4510\n",
      "MCMC Step 15000, Acceptance Rate: 0.4524\n",
      "MCMC Step 20000, Acceptance Rate: 0.4517\n",
      "MCMC Step 25000, Acceptance Rate: 0.4528\n",
      "MCMC Step 30000, Acceptance Rate: 0.4532\n",
      "MCMC Step 35000, Acceptance Rate: 0.4529\n",
      "MCMC Step 40000, Acceptance Rate: 0.4523\n",
      "MCMC Step 45000, Acceptance Rate: 0.4514\n",
      "MCMC Step 50000, Acceptance Rate: 0.4515\n",
      "MCMC Step 55000, Acceptance Rate: 0.4506\n",
      "Final Acceptance Rate: 0.4500\n",
      "MAP Log Posterior: -883.9136962890625\n",
      "First term: 887.6669\n",
      "Autocorrelation for first dimension (lags 0-4): [1.         0.6210293  0.5223943  0.4931558  0.48356837]\n",
      "Sample variance: [2.4199954e-04 1.8438399e-04 2.1753600e-05 2.6511546e-04 2.3826120e-04\n",
      " 2.4200027e-04 2.5231193e-04 2.1050197e-04 2.6422937e-04 2.5141760e-04]\n",
      "Log likelihood at MAP: -883.4137\n",
      "Average log likelihood from samples: 887.6669\n",
      "Entropy with 1000 preferences: -313.2642\n",
      "\n",
      "Processing preferences 0:2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1219412/2709741545.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_batch = torch.tensor(w_batch, dtype=torch.float32).to(device)  # Shape: (n_samples, feature_dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log P(H_1000:2000 | H_0:1000) = -1036.4537\n",
      "Cumulative Log P(H_0:2000) = -2237.3848\n",
      "MCMC Step 0, Acceptance Rate: 0.0000\n",
      "MCMC Step 5000, Acceptance Rate: 0.3207\n",
      "MCMC Step 10000, Acceptance Rate: 0.3111\n",
      "MCMC Step 15000, Acceptance Rate: 0.3123\n",
      "MCMC Step 20000, Acceptance Rate: 0.3122\n",
      "MCMC Step 25000, Acceptance Rate: 0.3111\n",
      "MCMC Step 30000, Acceptance Rate: 0.3106\n",
      "MCMC Step 35000, Acceptance Rate: 0.3111\n",
      "MCMC Step 40000, Acceptance Rate: 0.3101\n",
      "MCMC Step 45000, Acceptance Rate: 0.3099\n",
      "MCMC Step 50000, Acceptance Rate: 0.3087\n",
      "MCMC Step 55000, Acceptance Rate: 0.3088\n",
      "Final Acceptance Rate: 0.3085\n",
      "MAP Log Posterior: -1978.475830078125\n",
      "First term: 1982.3916\n",
      "Autocorrelation for first dimension (lags 0-4): [1.         0.9385199  0.93413925 0.93214196 0.93313307]\n",
      "Sample variance: [1.34499700e-04 1.04408056e-04 1.14056593e-05 1.31719949e-04\n",
      " 1.32323607e-04 1.32566522e-04 1.32785994e-04 1.27647319e-04\n",
      " 1.29746346e-04 1.39438227e-04]\n",
      "Log likelihood at MAP: -1977.9758\n",
      "Average log likelihood from samples: 1982.3916\n",
      "Entropy with 2000 preferences: -254.9932\n",
      "\n",
      "Processing preferences 0:3000\n",
      "Log P(H_2000:3000 | H_0:2000) = -976.0148\n",
      "Cumulative Log P(H_0:3000) = -3213.3996\n",
      "MCMC Step 0, Acceptance Rate: 0.0000\n",
      "MCMC Step 5000, Acceptance Rate: 0.2366\n",
      "MCMC Step 10000, Acceptance Rate: 0.2257\n",
      "MCMC Step 15000, Acceptance Rate: 0.2215\n",
      "MCMC Step 20000, Acceptance Rate: 0.2196\n",
      "MCMC Step 25000, Acceptance Rate: 0.2184\n",
      "MCMC Step 30000, Acceptance Rate: 0.2180\n",
      "MCMC Step 35000, Acceptance Rate: 0.2179\n",
      "MCMC Step 40000, Acceptance Rate: 0.2154\n",
      "MCMC Step 45000, Acceptance Rate: 0.2150\n",
      "MCMC Step 50000, Acceptance Rate: 0.2142\n",
      "MCMC Step 55000, Acceptance Rate: 0.2141\n",
      "Final Acceptance Rate: 0.2131\n",
      "MAP Log Posterior: -2934.05859375\n",
      "First term: 2937.9780\n",
      "Autocorrelation for first dimension (lags 0-4): [1.         0.77097964 0.75367504 0.7441658  0.7614803 ]\n",
      "Sample variance: [9.1838323e-05 7.0793576e-05 7.3590827e-06 9.2700779e-05 8.1388134e-05\n",
      " 9.7573975e-05 9.9646633e-05 9.0868307e-05 8.9601861e-05 8.9759262e-05]\n",
      "Log likelihood at MAP: -2933.5586\n",
      "Average log likelihood from samples: 2937.9780\n",
      "Entropy with 3000 preferences: -275.4215\n",
      "\n",
      "Processing preferences 0:4000\n",
      "Log P(H_3000:4000 | H_0:3000) = -1018.4205\n",
      "Cumulative Log P(H_0:4000) = -4231.8201\n",
      "MCMC Step 0, Acceptance Rate: 0.0000\n",
      "MCMC Step 5000, Acceptance Rate: 0.1750\n",
      "MCMC Step 10000, Acceptance Rate: 0.1617\n",
      "MCMC Step 15000, Acceptance Rate: 0.1583\n",
      "MCMC Step 20000, Acceptance Rate: 0.1594\n",
      "MCMC Step 25000, Acceptance Rate: 0.1598\n",
      "MCMC Step 30000, Acceptance Rate: 0.1605\n",
      "MCMC Step 35000, Acceptance Rate: 0.1613\n",
      "MCMC Step 40000, Acceptance Rate: 0.1610\n",
      "MCMC Step 45000, Acceptance Rate: 0.1613\n",
      "MCMC Step 50000, Acceptance Rate: 0.1603\n",
      "MCMC Step 55000, Acceptance Rate: 0.1606\n",
      "Final Acceptance Rate: 0.1601\n",
      "MAP Log Posterior: -4013.23046875\n",
      "First term: 4017.5247\n",
      "Autocorrelation for first dimension (lags 0-4): [ 1.          0.02093384 -0.00808355  0.03526107  0.01384054]\n",
      "Sample variance: [7.3484029e-05 5.9686037e-05 6.2001973e-06 6.9151145e-05 7.6296514e-05\n",
      " 6.8508714e-05 7.1745839e-05 7.1117967e-05 7.3988129e-05 7.3360643e-05]\n",
      "Log likelihood at MAP: -4012.7305\n",
      "Average log likelihood from samples: 4017.5247\n",
      "Entropy with 4000 preferences: -214.2954\n",
      "\n",
      "Processing preferences 0:5000\n",
      "Log P(H_4000:5000 | H_0:4000) = -989.2154\n",
      "Cumulative Log P(H_0:5000) = -5221.0355\n",
      "MCMC Step 0, Acceptance Rate: 0.0000\n",
      "MCMC Step 5000, Acceptance Rate: 0.1410\n",
      "MCMC Step 10000, Acceptance Rate: 0.1339\n",
      "MCMC Step 15000, Acceptance Rate: 0.1318\n",
      "MCMC Step 20000, Acceptance Rate: 0.1297\n",
      "MCMC Step 25000, Acceptance Rate: 0.1250\n",
      "MCMC Step 30000, Acceptance Rate: 0.1255\n",
      "MCMC Step 35000, Acceptance Rate: 0.1239\n",
      "MCMC Step 40000, Acceptance Rate: 0.1235\n",
      "MCMC Step 45000, Acceptance Rate: 0.1234\n",
      "MCMC Step 50000, Acceptance Rate: 0.1239\n",
      "MCMC Step 55000, Acceptance Rate: 0.1248\n",
      "Final Acceptance Rate: 0.1250\n",
      "MAP Log Posterior: -5066.03173828125\n",
      "First term: 5070.3638\n",
      "Autocorrelation for first dimension (lags 0-4): [1.         0.23133382 0.17108685 0.11459081 0.14123622]\n",
      "Sample variance: [5.9960694e-05 4.7322570e-05 4.3976875e-06 5.7472280e-05 6.0777689e-05\n",
      " 6.3773441e-05 6.3325759e-05 5.5183020e-05 5.8762609e-05 5.8618749e-05]\n",
      "Log likelihood at MAP: -5065.5317\n",
      "Average log likelihood from samples: 5070.3638\n",
      "Entropy with 5000 preferences: -150.6717\n",
      "\n",
      "Processing preferences 0:6000\n",
      "Log P(H_5000:6000 | H_0:5000) = -959.6872\n",
      "Cumulative Log P(H_0:6000) = -6180.7227\n",
      "MCMC Step 0, Acceptance Rate: 0.0000\n",
      "MCMC Step 5000, Acceptance Rate: 0.1120\n",
      "MCMC Step 10000, Acceptance Rate: 0.0985\n",
      "MCMC Step 15000, Acceptance Rate: 0.0981\n",
      "MCMC Step 20000, Acceptance Rate: 0.0960\n",
      "MCMC Step 25000, Acceptance Rate: 0.0944\n",
      "MCMC Step 30000, Acceptance Rate: 0.0942\n",
      "MCMC Step 35000, Acceptance Rate: 0.0942\n",
      "MCMC Step 40000, Acceptance Rate: 0.0938\n",
      "MCMC Step 45000, Acceptance Rate: 0.0929\n",
      "MCMC Step 50000, Acceptance Rate: 0.0927\n",
      "MCMC Step 55000, Acceptance Rate: 0.0921\n",
      "Final Acceptance Rate: 0.0922\n",
      "MAP Log Posterior: -5795.841796875\n",
      "First term: 5800.1567\n",
      "Autocorrelation for first dimension (lags 0-4): [1.         0.63562053 0.58176196 0.634706   0.6483808 ]\n",
      "Sample variance: [5.0844606e-05 4.1502532e-05 4.3305013e-06 5.1959265e-05 4.4822824e-05\n",
      " 5.2149844e-05 4.9791051e-05 5.1595405e-05 5.0859300e-05 5.2667707e-05]\n",
      "Log likelihood at MAP: -5795.3418\n",
      "Average log likelihood from samples: 5800.1567\n",
      "Entropy with 6000 preferences: -380.5660\n",
      "\n",
      "Processing preferences 0:7000\n",
      "Log P(H_6000:7000 | H_0:6000) = -950.1584\n",
      "Cumulative Log P(H_0:7000) = -7130.8811\n",
      "MCMC Step 0, Acceptance Rate: 0.0000\n",
      "MCMC Step 5000, Acceptance Rate: 0.0926\n",
      "MCMC Step 10000, Acceptance Rate: 0.0816\n",
      "MCMC Step 15000, Acceptance Rate: 0.0802\n",
      "MCMC Step 20000, Acceptance Rate: 0.0795\n",
      "MCMC Step 25000, Acceptance Rate: 0.0776\n",
      "MCMC Step 30000, Acceptance Rate: 0.0767\n",
      "MCMC Step 35000, Acceptance Rate: 0.0766\n",
      "MCMC Step 40000, Acceptance Rate: 0.0756\n",
      "MCMC Step 45000, Acceptance Rate: 0.0752\n",
      "MCMC Step 50000, Acceptance Rate: 0.0745\n",
      "MCMC Step 55000, Acceptance Rate: 0.0742\n",
      "Final Acceptance Rate: 0.0742\n",
      "MAP Log Posterior: -6923.236328125\n",
      "First term: 6927.9038\n",
      "Autocorrelation for first dimension (lags 0-4): [1.         0.6880139  0.687732   0.7009854  0.70975673]\n",
      "Sample variance: [4.6232122e-05 3.2995427e-05 3.9117895e-06 4.7624908e-05 4.8846498e-05\n",
      " 4.5553279e-05 4.6100326e-05 4.0211780e-05 4.4467644e-05 4.1945113e-05]\n",
      "Log likelihood at MAP: -6922.7363\n",
      "Average log likelihood from samples: 6927.9038\n",
      "Entropy with 7000 preferences: -202.9773\n",
      "\n",
      "Processing preferences 0:8000\n",
      "Log P(H_7000:8000 | H_0:7000) = -979.0080\n",
      "Cumulative Log P(H_0:8000) = -8109.8891\n",
      "MCMC Step 0, Acceptance Rate: 0.0000\n",
      "MCMC Step 5000, Acceptance Rate: 0.0784\n",
      "MCMC Step 10000, Acceptance Rate: 0.0640\n",
      "MCMC Step 15000, Acceptance Rate: 0.0620\n",
      "MCMC Step 20000, Acceptance Rate: 0.0612\n",
      "MCMC Step 25000, Acceptance Rate: 0.0613\n",
      "MCMC Step 30000, Acceptance Rate: 0.0589\n",
      "MCMC Step 35000, Acceptance Rate: 0.0581\n",
      "MCMC Step 40000, Acceptance Rate: 0.0567\n",
      "MCMC Step 45000, Acceptance Rate: 0.0567\n",
      "MCMC Step 50000, Acceptance Rate: 0.0565\n",
      "MCMC Step 55000, Acceptance Rate: 0.0565\n",
      "Final Acceptance Rate: 0.0563\n",
      "MAP Log Posterior: -7804.1708984375\n",
      "First term: 7808.5239\n",
      "Autocorrelation for first dimension (lags 0-4): [1.         0.7813422  0.7632004  0.76631624 0.7615317 ]\n",
      "Sample variance: [3.3683344e-05 3.3843578e-05 3.2256789e-06 3.7809292e-05 3.5388963e-05\n",
      " 3.8182723e-05 3.9646839e-05 3.1524181e-05 3.8384536e-05 3.7049951e-05]\n",
      "Log likelihood at MAP: -7803.6709\n",
      "Average log likelihood from samples: 7808.5239\n",
      "Entropy with 8000 preferences: -301.3652\n",
      "\n",
      "Processing preferences 0:9000\n",
      "Log P(H_8000:9000 | H_0:8000) = -1048.8036\n",
      "Cumulative Log P(H_0:9000) = -9158.6926\n",
      "MCMC Step 0, Acceptance Rate: 0.0000\n",
      "MCMC Step 5000, Acceptance Rate: 0.0644\n",
      "MCMC Step 10000, Acceptance Rate: 0.0549\n",
      "MCMC Step 15000, Acceptance Rate: 0.0480\n",
      "MCMC Step 20000, Acceptance Rate: 0.0461\n",
      "MCMC Step 25000, Acceptance Rate: 0.0482\n",
      "MCMC Step 30000, Acceptance Rate: 0.0472\n",
      "MCMC Step 35000, Acceptance Rate: 0.0483\n",
      "MCMC Step 40000, Acceptance Rate: 0.0478\n",
      "MCMC Step 45000, Acceptance Rate: 0.0490\n",
      "MCMC Step 50000, Acceptance Rate: 0.0486\n",
      "MCMC Step 55000, Acceptance Rate: 0.0483\n",
      "Final Acceptance Rate: 0.0484\n",
      "MAP Log Posterior: -8882.2509765625\n",
      "First term: 8886.8174\n",
      "Autocorrelation for first dimension (lags 0-4): [1.         0.23277894 0.1433036  0.12900326 0.13460717]\n",
      "Sample variance: [3.4109522e-05 3.0512414e-05 3.1355487e-06 3.6602898e-05 2.9077675e-05\n",
      " 3.3131495e-05 4.1393210e-05 3.4199253e-05 3.6163987e-05 3.3321227e-05]\n",
      "Log likelihood at MAP: -8881.7510\n",
      "Average log likelihood from samples: 8886.8174\n",
      "Entropy with 9000 preferences: -271.8753\n",
      "\n",
      "Processing preferences 0:10000\n",
      "Log P(H_9000:10000 | H_0:9000) = -1004.2646\n",
      "Cumulative Log P(H_0:10000) = -10162.9572\n",
      "MCMC Step 0, Acceptance Rate: 0.0000\n",
      "MCMC Step 5000, Acceptance Rate: 0.0640\n",
      "MCMC Step 10000, Acceptance Rate: 0.0526\n",
      "MCMC Step 15000, Acceptance Rate: 0.0509\n",
      "MCMC Step 20000, Acceptance Rate: 0.0475\n",
      "MCMC Step 25000, Acceptance Rate: 0.0457\n",
      "MCMC Step 30000, Acceptance Rate: 0.0439\n",
      "MCMC Step 35000, Acceptance Rate: 0.0428\n",
      "MCMC Step 40000, Acceptance Rate: 0.0427\n",
      "MCMC Step 45000, Acceptance Rate: 0.0424\n",
      "MCMC Step 50000, Acceptance Rate: 0.0421\n",
      "MCMC Step 55000, Acceptance Rate: 0.0418\n",
      "Final Acceptance Rate: 0.0411\n",
      "MAP Log Posterior: -10138.353515625\n",
      "First term: 10142.8926\n",
      "Autocorrelation for first dimension (lags 0-4): [1.         0.5509744  0.55295175 0.55547935 0.57364136]\n",
      "Sample variance: [3.3944434e-05 2.2738181e-05 1.9157922e-06 3.6459660e-05 3.3795506e-05\n",
      " 3.6049085e-05 2.7582641e-05 3.0046172e-05 3.0134173e-05 3.1118918e-05]\n",
      "Log likelihood at MAP: -10137.8535\n",
      "Average log likelihood from samples: 10142.8926\n",
      "Entropy with 10000 preferences: -20.0646\n",
      "\n",
      "Summary of Entropies:\n",
      "Preferences 0:1000: Entropy = -313.2642\n",
      "Preferences 0:2000: Entropy = -254.9932\n",
      "Preferences 0:3000: Entropy = -275.4215\n",
      "Preferences 0:4000: Entropy = -214.2954\n",
      "Preferences 0:5000: Entropy = -150.6717\n",
      "Preferences 0:6000: Entropy = -380.5660\n",
      "Preferences 0:7000: Entropy = -202.9773\n",
      "Preferences 0:8000: Entropy = -301.3652\n",
      "Preferences 0:9000: Entropy = -271.8753\n",
      "Preferences 0:10000: Entropy = -20.0646\n",
      "Results saved: entropies.npy, posterior_samples_*.npy, map_solution_*.npy\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "import pickle\n",
    "\n",
    "# Assuming device is defined\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Function to encode preferences and compute Phi_tau_i, Phi_tau_j\n",
    "def encode_preferences(encoder, preferences, segments):\n",
    "    # Ensure encoder is frozen\n",
    "    for param in encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Precompute Phi_tau (sum of embeddings over each trajectory)\n",
    "    segment_states = [\n",
    "        torch.tensor(\n",
    "            np.array([state_t[0, 0, :] for state_t, _, _, done_t in segment if not done_t]),\n",
    "            dtype=torch.float32\n",
    "        ).to(device) if any(not done_t for _, _, _, done_t in segment) else None\n",
    "        for segment in segments\n",
    "    ]\n",
    "    Phi_tau_list = []\n",
    "    for seg_states in segment_states:\n",
    "        if seg_states is not None:\n",
    "            with torch.no_grad():\n",
    "                phi, _, _ = encoder(seg_states)  # Shape: (T, feature_dim)\n",
    "            Phi_tau = phi.sum(dim=0)  # Sum over trajectory: Shape: (feature_dim,)\n",
    "            Phi_tau_list.append(Phi_tau)\n",
    "        else:\n",
    "            Phi_tau_list.append(None)\n",
    "    \n",
    "    # Precompute Phi_tau_i and Phi_tau_j tensors for valid preference pairs\n",
    "    valid_pairs = [(i, j) for i, j, _ in preferences if Phi_tau_list[i] is not None and Phi_tau_list[j] is not None]\n",
    "    if not valid_pairs:\n",
    "        raise ValueError(\"No valid preference pairs found after filtering None values.\")\n",
    "    \n",
    "    Phi_tau_i = torch.stack([Phi_tau_list[i] for i, j in valid_pairs])  # Shape: (num_valid_pairs, feature_dim)\n",
    "    Phi_tau_j = torch.stack([Phi_tau_list[j] for i, j in valid_pairs])  # Shape: (num_valid_pairs, feature_dim)\n",
    "    \n",
    "    return Phi_tau_i, Phi_tau_j\n",
    "\n",
    "\n",
    "## Replace the following with iteration\n",
    "# Vectorized log likelihood for a batch of weights\n",
    "def log_likelihood_vectorized(w_batch, Phi_tau_i, Phi_tau_j, beta):\n",
    "    w_batch = torch.tensor(w_batch, dtype=torch.float32).to(device)  # Shape: (n_samples, feature_dim)\n",
    "    R_i = torch.matmul(Phi_tau_i, w_batch.T)  # Shape: (num_pairs, n_samples)\n",
    "    R_j = torch.matmul(Phi_tau_j, w_batch.T)  # Shape: (num_pairs, n_samples)\n",
    "    \n",
    "    # Numerically stable computation\n",
    "    beta_R_i = beta * R_i\n",
    "    beta_R_j = beta * R_j\n",
    "    \n",
    "    \n",
    "    terms = beta * R_j - torch.log(torch.exp(beta * R_i) + torch.exp(beta * R_j))\n",
    "    \n",
    "    # max_val = torch.maximum(beta_R_i, beta_R_j)\n",
    "    # log_sum_exp = max_val + torch.log(\n",
    "    #     torch.exp(beta_R_i - max_val) + torch.exp(beta_R_j - max_val)\n",
    "    # )\n",
    "    \n",
    "    #terms = beta * R_j - log_sum_exp\n",
    "    log_likelihoods = torch.sum(terms, dim=0)  # Shape: (n_samples,)\n",
    "    return log_likelihoods.cpu().numpy()\n",
    "\n",
    "# Compute entropy using importance sampling with recursive marginal likelihood\n",
    "def compute_entropy_importance_sampling(Phi_tau_i, Phi_tau_j, samples, beta, log_P_H_cumulative):\n",
    "    # Compute log likelihoods for all samples\n",
    "    log_probs = log_likelihood_vectorized(samples, Phi_tau_i, Phi_tau_j, beta)\n",
    "    \n",
    "    if np.any(np.isnan(log_probs)) or np.any(np.isinf(log_probs)):\n",
    "        raise ValueError(\"Log probabilities contain NaN or Inf values.\")\n",
    "    \n",
    "    # First term: - (1 / m) * sum(log P(D, P | w^{(k)}))\n",
    "    first_term = -np.mean(log_probs)\n",
    "    print(f\"First term: {first_term:.4f}\")\n",
    "    \n",
    "    # Use the recursively estimated log P(H_{1:i}) instead of Harmonic Mean Estimator\n",
    "    entropy = first_term + log_P_H_cumulative\n",
    "    return entropy, log_probs\n",
    "\n",
    "# MCMC for Bayesian REX with parallelized log_likelihood, MAP tracking, and weight normalization\n",
    "def bayesian_rex_mcmc(trex_model, Phi_tau_i, Phi_tau_j, num_samples=10000, burn_in=1000, beta=1.0, proposal_std=0.005):\n",
    "    w_current = trex_model.model.weight.data.clone().squeeze().to(device)\n",
    "    w_current = w_current / torch.norm(w_current, p=2)\n",
    "    \n",
    "    def log_prior(w):\n",
    "        return -0.5 * torch.sum(w ** 2)\n",
    "    \n",
    "    def log_likelihood(w, Phi_tau_i, Phi_tau_j, beta):\n",
    "        R_i = torch.matmul(Phi_tau_i, w)\n",
    "        R_j = torch.matmul(Phi_tau_j, w)\n",
    "        terms = beta * R_j - torch.log(torch.exp(beta * R_i) + torch.exp(beta * R_j))\n",
    "        return torch.sum(terms)\n",
    "    \n",
    "    samples = []\n",
    "    accepted = 0\n",
    "    w_map = w_current.clone()\n",
    "    log_posterior_map = log_prior(w_current) + log_likelihood(w_current, Phi_tau_i, Phi_tau_j, beta)\n",
    "    \n",
    "    for step in range(num_samples + burn_in):\n",
    "        w_proposed = w_current + torch.normal(mean=0, std=proposal_std, size=w_current.shape).to(device)\n",
    "        w_proposed = w_proposed / torch.norm(w_proposed, p=2)\n",
    "        \n",
    "        log_posterior_current = log_prior(w_current) + log_likelihood(w_current, Phi_tau_i, Phi_tau_j, beta)\n",
    "        log_posterior_proposed = log_prior(w_proposed) + log_likelihood(w_proposed, Phi_tau_i, Phi_tau_j, beta)\n",
    "        \n",
    "        if log_posterior_proposed > log_posterior_map:\n",
    "            w_map = w_proposed.clone()\n",
    "            log_posterior_map = log_posterior_proposed\n",
    "        \n",
    "        log_alpha = log_posterior_proposed - log_posterior_current\n",
    "        alpha = torch.exp(log_alpha)\n",
    "        \n",
    "        if torch.rand(1, device=device) < alpha:\n",
    "            w_current = w_proposed\n",
    "            accepted += 1\n",
    "            if step >= burn_in:\n",
    "                samples.append(w_current.cpu().detach().numpy())\n",
    "        \n",
    "        if step % 5000 == 0:\n",
    "            acceptance_rate = accepted / (step + 1) if step > 0 else 0\n",
    "            print(f\"MCMC Step {step}, Acceptance Rate: {acceptance_rate:.4f}\")\n",
    "    \n",
    "    # Thin samples to reduce autocorrelation\n",
    "    thinning = 10\n",
    "    samples = samples[::thinning]\n",
    "    \n",
    "    print(f\"Final Acceptance Rate: {accepted / (num_samples + burn_in):.4f}\")\n",
    "    print(f\"MAP Log Posterior: {log_posterior_map.item()}\")\n",
    "    \n",
    "    samples = np.array(samples)\n",
    "    return samples, w_map\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load feedback data\n",
    "    feedback_path = 'ppo_merge-v0_1377.pkl'\n",
    "    with open(feedback_path, 'rb') as file:\n",
    "        feedback_data = pickle.load(file)\n",
    "    \n",
    "    segments = feedback_data['segments']\n",
    "    preferences = feedback_data['preferences']\n",
    "\n",
    "    # Shuffle the preferences list\n",
    "    preferences = np.random.permutation(preferences).tolist()\n",
    "    print(\"Preferences have been shuffled.\")\n",
    "    \n",
    "    # Initialize models (assuming these classes are defined elsewhere)\n",
    "    state_dim = 5\n",
    "    feature_dim = 10\n",
    "    encoder = FeatureEncoder(input_dim=state_dim, feature_dim=feature_dim)\n",
    "    trex_model = TREXRewardPredictor(feature_dim=feature_dim)\n",
    "    \n",
    "    encoder.load_state_dict(torch.load(\"feature_encoder_pretrain.pth\", map_location=device))\n",
    "    trex_model.load_state_dict(torch.load(\"trex_model_finetune.pth\", map_location=device))\n",
    "    \n",
    "    encoder = encoder.to(device)\n",
    "    trex_model = trex_model.to(device)\n",
    "    encoder.eval()\n",
    "    trex_model.eval()\n",
    "    \n",
    "    print(\"Models loaded from 'feature_encoder_pretrain.pth' and 'trex_model_finetune.pth'\")\n",
    "    \n",
    "    # Generate prior samples\n",
    "    n_prior_samples = 20000\n",
    "    prior_samples = np.random.normal(0, 1, (n_prior_samples, feature_dim))\n",
    "    prior_samples = prior_samples / np.linalg.norm(prior_samples, axis=1, keepdims=True)\n",
    "    \n",
    "    # Initialize variables\n",
    "    log_P_H_cumulative = 0.0\n",
    "    prev_pref_size = 0\n",
    "    preference_sizes = range(1000, len(preferences) + 1000, 1000)\n",
    "    entropies = []\n",
    "    all_posterior_samples = []\n",
    "    all_map_solutions = []\n",
    "    beta = 1.0\n",
    "    \n",
    "    print(\"Running Bayesian REX MCMC with recursive marginal likelihood estimation...\")\n",
    "    for pref_size in preference_sizes:\n",
    "        print(f\"\\nProcessing preferences 0:{pref_size}\")\n",
    "        \n",
    "        # Encode full and new preference subsets\n",
    "        Phi_tau_i_full, Phi_tau_j_full = encode_preferences(encoder, preferences[:pref_size], segments)\n",
    "        Phi_tau_i_new, Phi_tau_j_new = (\n",
    "            encode_preferences(encoder, preferences[prev_pref_size:pref_size], segments)\n",
    "            if prev_pref_size > 0 else (Phi_tau_i_full, Phi_tau_j_full)\n",
    "        )\n",
    "        \n",
    "        # Estimate log P(H_new | H_prev)\n",
    "        if prev_pref_size == 0:\n",
    "            log_probs = log_likelihood_vectorized(prior_samples, Phi_tau_i_new, Phi_tau_j_new, beta)\n",
    "        else:\n",
    "            prev_samples = all_posterior_samples[-1]\n",
    "            log_probs = log_likelihood_vectorized(prev_samples, Phi_tau_i_new, Phi_tau_j_new, beta)\n",
    "        \n",
    "        log_P_H_new_given_prev = logsumexp(log_probs) - np.log(len(log_probs))\n",
    "        log_P_H_cumulative += log_P_H_new_given_prev\n",
    "        print(f\"Log P(H_{prev_pref_size}:{pref_size} | H_0:{prev_pref_size}) = {log_P_H_new_given_prev:.4f}\")\n",
    "        print(f\"Cumulative Log P(H_0:{pref_size}) = {log_P_H_cumulative:.4f}\")\n",
    "        \n",
    "        # Adjust proposal_std and burn_in based on pref_size\n",
    "        #proposal_std = 0.05 if pref_size <= 2000 else 0.01 if pref_size <= 5000 else 0.005\n",
    "        proposal_std = 0.008\n",
    "        #burn_in = 10000 if pref_size <= 2000 else 5000\n",
    "        burn_in = 10000\n",
    "        \n",
    "        # Run MCMC\n",
    "        posterior_samples, map_solution = bayesian_rex_mcmc(\n",
    "            trex_model,\n",
    "            Phi_tau_i_full,\n",
    "            Phi_tau_j_full,\n",
    "            num_samples=50000,\n",
    "            burn_in=burn_in,\n",
    "            beta=beta,\n",
    "            proposal_std=proposal_std\n",
    "        )\n",
    "        \n",
    "        # Compute entropy with recursive marginal likelihood\n",
    "        entropy, log_probs = compute_entropy_importance_sampling(\n",
    "            Phi_tau_i_full, Phi_tau_j_full, posterior_samples, beta, log_P_H_cumulative\n",
    "        )\n",
    "        \n",
    "        # Debug 1: Inspect Autocorrelation\n",
    "        from numpy import correlate\n",
    "        if len(posterior_samples) > 1:  # Ensure enough samples for autocorrelation\n",
    "            autocorr = correlate(posterior_samples[:, 0], posterior_samples[:, 0], mode='full')\n",
    "            autocorr = autocorr[len(autocorr)//2:len(autocorr)//2+5] / autocorr[len(autocorr)//2]\n",
    "            print(f\"Autocorrelation for first dimension (lags 0-4): {autocorr}\")\n",
    "        else:\n",
    "            print(\"Not enough samples to compute autocorrelation.\")\n",
    "        \n",
    "        # Debug 2: Validate Posterior Concentration\n",
    "        sample_variance = np.var(posterior_samples, axis=0)\n",
    "        print(f\"Sample variance: {sample_variance}\")\n",
    "        \n",
    "        # Debug 3: Cross-Check with MAP\n",
    "        log_likelihood_map = log_likelihood_vectorized(map_solution.reshape(1, -1), Phi_tau_i_full, Phi_tau_j_full, beta)\n",
    "        print(f\"Log likelihood at MAP: {log_likelihood_map[0]:.4f}\")\n",
    "        print(f\"Average log likelihood from samples: {-np.mean(log_probs):.4f}\")\n",
    "        \n",
    "        # Store results\n",
    "        entropies.append(entropy)\n",
    "        all_posterior_samples.append(posterior_samples)\n",
    "        all_map_solutions.append(map_solution)\n",
    "        \n",
    "        print(f\"Entropy with {pref_size} preferences: {entropy:.4f}\")\n",
    "        prev_pref_size = pref_size\n",
    "    \n",
    "    # Save results with safeguard for tensors\n",
    "    np.save(\"entropies.npy\", np.array(entropies))\n",
    "    for i, pref_size in enumerate(preference_sizes):\n",
    "        np.save(f\"posterior_samples_{pref_size}.npy\", all_posterior_samples[i])\n",
    "        # Ensure map_solution is a NumPy array\n",
    "        map_solution = all_map_solutions[i]\n",
    "        if isinstance(map_solution, torch.Tensor):\n",
    "            map_solution = map_solution.cpu().detach().numpy()\n",
    "        np.save(f\"map_solution_{pref_size}.npy\", map_solution)\n",
    "\n",
    "    print(\"\\nSummary of Entropies:\")\n",
    "    for pref_size, entropy in zip(preference_sizes, entropies):\n",
    "        print(f\"Preferences 0:{pref_size}: Entropy = {entropy:.4f}\")\n",
    "\n",
    "    print(\"Results saved: entropies.npy, posterior_samples_*.npy, map_solution_*.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5067cd10",
   "metadata": {},
   "source": [
    "## Visualize the entropy with moving average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0e5d41e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of Entropies:\n",
      "Preferences 0:1000: Entropy = -250.1023\n",
      "Preferences 0:2000: Entropy = -199.1307\n",
      "Preferences 0:3000: Entropy = -195.0399\n",
      "Preferences 0:4000: Entropy = -148.8725\n",
      "Preferences 0:5000: Entropy = -275.9835\n",
      "Preferences 0:6000: Entropy = -316.1736\n",
      "Preferences 0:7000: Entropy = -210.6894\n",
      "Preferences 0:8000: Entropy = -340.7805\n",
      "Preferences 0:9000: Entropy = -318.2854\n",
      "Preferences 0:10000: Entropy = -185.5123\n",
      "Results saved: entropies.npy, posterior_samples_*.npy, map_solution_*.npy\n",
      "Plot saved: entropy_moving_average.png\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt  # Added for visualization\n",
    "\n",
    "# Assuming device is defined\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# [Previous functions: encode_preferences, log_likelihood_vectorized, compute_entropy_importance_sampling, bayesian_rex_mcmc]\n",
    "# These remain unchanged from your provided code. For brevity, I'll only show the main execution loop with visualization.\n",
    "\n",
    "# Function to compute moving average\n",
    "def moving_average(data, window_size):\n",
    "    \"\"\"Compute the moving average of a 1D array with a given window size.\"\"\"\n",
    "    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "\n",
    "# Compute moving average of entropies\n",
    "window_size = 2  # Adjust as needed (e.g., 3, 5)\n",
    "if len(entropies) >= window_size:\n",
    "    moving_avg_entropies = moving_average(np.array(entropies), window_size)\n",
    "    # Adjust preference sizes for moving average (shorter due to convolution)\n",
    "    moving_avg_pref_sizes = preference_sizes[window_size-1:]\n",
    "else:\n",
    "    print(f\"Warning: Not enough entropy values ({len(entropies)}) for moving average with window size {window_size}\")\n",
    "    moving_avg_entropies = []\n",
    "    moving_avg_pref_sizes = []\n",
    "\n",
    "# Visualize entropy and moving average\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(preference_sizes, entropies, label='Raw Entropy', marker='o', linestyle='-')\n",
    "if len(moving_avg_entropies) > 0:\n",
    "    plt.plot(moving_avg_pref_sizes, moving_avg_entropies, label=f'Moving Average (window={window_size})', marker='s', linestyle='--')\n",
    "plt.xlabel('Number of Preferences')\n",
    "plt.ylabel('Entropy')\n",
    "plt.title('Entropy vs. Number of Preferences with Moving Average')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('entropy_moving_average.png')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSummary of Entropies:\")\n",
    "for pref_size, entropy in zip(preference_sizes, entropies):\n",
    "    print(f\"Preferences 0:{pref_size}: Entropy = {entropy:.4f}\")\n",
    "\n",
    "print(\"Results saved: entropies.npy, posterior_samples_*.npy, map_solution_*.npy\")\n",
    "print(\"Plot saved: entropy_moving_average.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b9e131",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2d8b71",
   "metadata": {},
   "source": [
    "## Bridge sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47f00579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preferences have been shuffled.\n",
      "Models loaded from 'feature_encoder_pretrain.pth' and 'trex_model_finetune.pth'\n",
      "Running Bayesian REX MCMC with bridge sampling for marginal likelihood...\n",
      "\n",
      "Processing preferences 0:1000\n",
      "MCMC Step 0, Acceptance Rate: 0.0000\n",
      "MCMC Step 5000, Acceptance Rate: 0.4889\n",
      "MCMC Step 10000, Acceptance Rate: 0.4852\n",
      "MCMC Step 15000, Acceptance Rate: 0.4812\n",
      "MCMC Step 20000, Acceptance Rate: 0.4808\n",
      "MCMC Step 25000, Acceptance Rate: 0.4837\n",
      "MCMC Step 30000, Acceptance Rate: 0.4832\n",
      "MCMC Step 35000, Acceptance Rate: 0.4834\n",
      "MCMC Step 40000, Acceptance Rate: 0.4838\n",
      "MCMC Step 45000, Acceptance Rate: 0.4851\n",
      "MCMC Step 50000, Acceptance Rate: 0.4858\n",
      "MCMC Step 55000, Acceptance Rate: 0.4863\n",
      "Final Acceptance Rate: 0.4869\n",
      "MAP Log Posterior: -951.3011474609375\n",
      "Log P(H_0:1000) via Bridge Sampling = 1162.8144\n",
      "First term: 955.1077\n",
      "Prior term: 0.5000\n",
      "Autocorrelation for first dimension (lags 0-4): [1.         0.90996933 0.87553173 0.86182785 0.8550508 ]\n",
      "Sample variance: [3.1483494e-04 2.5571935e-04 3.5892277e-05 2.9269286e-04 3.0288487e-04\n",
      " 2.8821224e-04 2.6647744e-04 2.6966553e-04 2.5067123e-04 2.8514132e-04]\n",
      "Log likelihood at MAP: -950.8012\n",
      "Average log likelihood from samples: 955.1077\n",
      "Entropy with 1000 preferences: 2118.4220\n",
      "\n",
      "Processing preferences 0:2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1267509/2075311122.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_batch = torch.tensor(w_batch, dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCMC Step 0, Acceptance Rate: 0.0000\n",
      "MCMC Step 5000, Acceptance Rate: 0.3055\n",
      "MCMC Step 10000, Acceptance Rate: 0.3000\n",
      "MCMC Step 15000, Acceptance Rate: 0.2943\n",
      "MCMC Step 20000, Acceptance Rate: 0.2940\n",
      "MCMC Step 25000, Acceptance Rate: 0.2928\n",
      "MCMC Step 30000, Acceptance Rate: 0.2924\n",
      "MCMC Step 35000, Acceptance Rate: 0.2922\n",
      "MCMC Step 40000, Acceptance Rate: 0.2921\n",
      "MCMC Step 45000, Acceptance Rate: 0.2929\n",
      "MCMC Step 50000, Acceptance Rate: 0.2934\n",
      "MCMC Step 55000, Acceptance Rate: 0.2938\n",
      "Final Acceptance Rate: 0.2942\n",
      "MAP Log Posterior: -1882.8275146484375\n",
      "Log P(H_0:2000) via Bridge Sampling = 2768.2741\n",
      "First term: 1886.6631\n",
      "Prior term: 0.5000\n",
      "Autocorrelation for first dimension (lags 0-4): [1.         0.25736982 0.18138382 0.19003743 0.15887256]\n",
      "Sample variance: [1.27096078e-04 1.04603205e-04 1.09074545e-05 1.44889462e-04\n",
      " 1.31195338e-04 1.32741116e-04 1.22819125e-04 1.16492229e-04\n",
      " 1.31236360e-04 1.22533354e-04]\n",
      "Log likelihood at MAP: -1882.3276\n",
      "Average log likelihood from samples: 1886.6631\n",
      "Entropy with 2000 preferences: 4655.4372\n",
      "\n",
      "Processing preferences 0:3000\n",
      "MCMC Step 0, Acceptance Rate: 0.0000\n",
      "MCMC Step 5000, Acceptance Rate: 0.2352\n",
      "MCMC Step 10000, Acceptance Rate: 0.2265\n",
      "MCMC Step 15000, Acceptance Rate: 0.2238\n",
      "MCMC Step 20000, Acceptance Rate: 0.2214\n",
      "MCMC Step 25000, Acceptance Rate: 0.2199\n",
      "MCMC Step 30000, Acceptance Rate: 0.2193\n",
      "MCMC Step 35000, Acceptance Rate: 0.2185\n",
      "MCMC Step 40000, Acceptance Rate: 0.2177\n",
      "MCMC Step 45000, Acceptance Rate: 0.2186\n",
      "MCMC Step 50000, Acceptance Rate: 0.2181\n",
      "MCMC Step 55000, Acceptance Rate: 0.2173\n",
      "Final Acceptance Rate: 0.2170\n",
      "MAP Log Posterior: -3014.4326171875\n",
      "Log P(H_0:3000) via Bridge Sampling = 4633.8276\n",
      "First term: 3018.4072\n",
      "Prior term: 0.5000\n",
      "Autocorrelation for first dimension (lags 0-4): [ 1.          0.04343354 -0.06136164 -0.00949182 -0.04727183]\n",
      "Sample variance: [9.3989845e-05 7.4422736e-05 7.4875375e-06 8.6288790e-05 9.3767805e-05\n",
      " 9.6197276e-05 9.4941730e-05 8.4487518e-05 8.9595880e-05 9.5310505e-05]\n",
      "Log likelihood at MAP: -3013.9326\n",
      "Average log likelihood from samples: 3018.4072\n",
      "Entropy with 3000 preferences: 7652.7349\n",
      "\n",
      "Processing preferences 0:4000\n",
      "MCMC Step 0, Acceptance Rate: 0.0000\n",
      "MCMC Step 5000, Acceptance Rate: 0.1720\n",
      "MCMC Step 10000, Acceptance Rate: 0.1658\n",
      "MCMC Step 15000, Acceptance Rate: 0.1617\n",
      "MCMC Step 20000, Acceptance Rate: 0.1605\n",
      "MCMC Step 25000, Acceptance Rate: 0.1591\n",
      "MCMC Step 30000, Acceptance Rate: 0.1591\n",
      "MCMC Step 35000, Acceptance Rate: 0.1570\n",
      "MCMC Step 40000, Acceptance Rate: 0.1569\n",
      "MCMC Step 45000, Acceptance Rate: 0.1560\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 217\u001b[0m\n\u001b[1;32m    214\u001b[0m proposal_std \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.008\u001b[39m\n\u001b[1;32m    215\u001b[0m burn_in \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m\n\u001b[0;32m--> 217\u001b[0m posterior_samples, map_solution \u001b[38;5;241m=\u001b[39m \u001b[43mbayesian_rex_mcmc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrex_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mPhi_tau_i_full\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mPhi_tau_j_full\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mburn_in\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mburn_in\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproposal_std\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproposal_std\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;66;03m# Estimate marginal likelihood using bridge sampling\u001b[39;00m\n\u001b[1;32m    228\u001b[0m log_P_H \u001b[38;5;241m=\u001b[39m bridge_sampling_marginal_likelihood(\n\u001b[1;32m    229\u001b[0m     posterior_samples, prior_samples, Phi_tau_i_full, Phi_tau_j_full, beta\n\u001b[1;32m    230\u001b[0m )\n",
      "Cell \u001b[0;32mIn[3], line 144\u001b[0m, in \u001b[0;36mbayesian_rex_mcmc\u001b[0;34m(trex_model, Phi_tau_i, Phi_tau_j, num_samples, burn_in, beta, proposal_std)\u001b[0m\n\u001b[1;32m    141\u001b[0m w_proposed \u001b[38;5;241m=\u001b[39m w_current \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mnormal(mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, std\u001b[38;5;241m=\u001b[39mproposal_std, size\u001b[38;5;241m=\u001b[39mw_current\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    142\u001b[0m w_proposed \u001b[38;5;241m=\u001b[39m w_proposed \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(w_proposed, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 144\u001b[0m log_posterior_current \u001b[38;5;241m=\u001b[39m log_prior(w_current) \u001b[38;5;241m+\u001b[39m \u001b[43mlog_likelihood\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw_current\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPhi_tau_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPhi_tau_j\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m log_posterior_proposed \u001b[38;5;241m=\u001b[39m log_prior(w_proposed) \u001b[38;5;241m+\u001b[39m log_likelihood(w_proposed, Phi_tau_i, Phi_tau_j, beta)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m log_posterior_proposed \u001b[38;5;241m>\u001b[39m log_posterior_map:\n",
      "Cell \u001b[0;32mIn[3], line 131\u001b[0m, in \u001b[0;36mbayesian_rex_mcmc.<locals>.log_likelihood\u001b[0;34m(w, Phi_tau_i, Phi_tau_j, beta)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlog_likelihood\u001b[39m(w, Phi_tau_i, Phi_tau_j, beta):\n\u001b[1;32m    130\u001b[0m     R_i \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(Phi_tau_i, w)\n\u001b[0;32m--> 131\u001b[0m     R_j \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPhi_tau_j\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m     terms \u001b[38;5;241m=\u001b[39m beta \u001b[38;5;241m*\u001b[39m R_j \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(torch\u001b[38;5;241m.\u001b[39mexp(beta \u001b[38;5;241m*\u001b[39m R_i) \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(beta \u001b[38;5;241m*\u001b[39m R_j))\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msum(terms)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "import pickle\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def encode_preferences(encoder, preferences, segments):\n",
    "    for param in encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    segment_states = [\n",
    "        torch.tensor(\n",
    "            np.array([state_t[0, 0, :] for state_t, _, _, done_t in segment if not done_t]),\n",
    "            dtype=torch.float32\n",
    "        ).to(device) if any(not done_t for _, _, _, done_t in segment) else None\n",
    "        for segment in segments\n",
    "    ]\n",
    "    Phi_tau_list = []\n",
    "    for seg_states in segment_states:\n",
    "        if seg_states is not None:\n",
    "            with torch.no_grad():\n",
    "                phi, _, _ = encoder(seg_states)\n",
    "            Phi_tau = phi.sum(dim=0)\n",
    "            Phi_tau_list.append(Phi_tau)\n",
    "        else:\n",
    "            Phi_tau_list.append(None)\n",
    "    \n",
    "    valid_pairs = [(i, j) for i, j, _ in preferences if Phi_tau_list[i] is not None and Phi_tau_list[j] is not None]\n",
    "    if not valid_pairs:\n",
    "        raise ValueError(\"No valid preference pairs found.\")\n",
    "    \n",
    "    Phi_tau_i = torch.stack([Phi_tau_list[i] for i, j in valid_pairs])\n",
    "    Phi_tau_j = torch.stack([Phi_tau_list[j] for i, j in valid_pairs])\n",
    "    return Phi_tau_i, Phi_tau_j\n",
    "\n",
    "def log_likelihood_vectorized(w_batch, Phi_tau_i, Phi_tau_j, beta):\n",
    "    w_batch = torch.tensor(w_batch, dtype=torch.float32).to(device)\n",
    "    R_i = torch.matmul(Phi_tau_i, w_batch.T)\n",
    "    R_j = torch.matmul(Phi_tau_j, w_batch.T)\n",
    "    \n",
    "    beta_R_i = beta * R_i\n",
    "    beta_R_j = beta * R_j\n",
    "    max_val = torch.maximum(beta_R_i, beta_R_j)\n",
    "    log_sum_exp = max_val + torch.log(\n",
    "        torch.exp(beta_R_i - max_val) + torch.exp(beta_R_j - max_val)\n",
    "    )\n",
    "    \n",
    "    terms = beta * R_j - log_sum_exp\n",
    "    log_likelihoods = torch.sum(terms, dim=0)\n",
    "    \n",
    "    if torch.any(log_likelihoods > 0):\n",
    "        print(\"Warning: Positive log likelihood detected.\")\n",
    "        print(f\"Sample terms: {terms[:5, :5]}\")\n",
    "        print(f\"Log likelihoods: {log_likelihoods[:5]}\")\n",
    "    \n",
    "    return log_likelihoods.cpu().numpy()\n",
    "\n",
    "def log_prior_vectorized(w_batch):\n",
    "    w_batch = torch.tensor(w_batch, dtype=torch.float32).to(device)\n",
    "    return -0.5 * torch.sum(w_batch ** 2, dim=1).cpu().numpy()\n",
    "\n",
    "def bridge_sampling_marginal_likelihood(posterior_samples, prior_samples, Phi_tau_i, Phi_tau_j, beta):\n",
    "    # Subsample to ensure equal sample sizes for simplicity\n",
    "    n_samples = min(len(posterior_samples), len(prior_samples))\n",
    "    posterior_samples = posterior_samples[np.random.choice(len(posterior_samples), n_samples, replace=False)]\n",
    "    prior_samples = prior_samples[np.random.choice(len(prior_samples), n_samples, replace=False)]\n",
    "\n",
    "    # Compute unnormalized posterior for posterior samples: log P(D, P | w) + log P(w)\n",
    "    post_log_likelihoods = log_likelihood_vectorized(posterior_samples, Phi_tau_i, Phi_tau_j, beta)\n",
    "    post_log_priors = log_prior_vectorized(posterior_samples)\n",
    "    post_log_unnorm = post_log_likelihoods + post_log_priors  # log P(D, P | w) P(w)\n",
    "\n",
    "    # Compute unnormalized posterior for prior samples\n",
    "    prior_log_likelihoods = log_likelihood_vectorized(prior_samples, Phi_tau_i, Phi_tau_j, beta)\n",
    "    prior_log_priors = log_prior_vectorized(prior_samples)\n",
    "    prior_log_unnorm = prior_log_likelihoods + prior_log_priors\n",
    "\n",
    "    # Normalize posterior (approximate log P(D, P))\n",
    "    log_Z_approx = logsumexp(post_log_unnorm) - np.log(n_samples)\n",
    "\n",
    "    # Bridge sampling with a simple mixture h(w) = alpha * P(w) + (1-alpha) * P(w | D, P)\n",
    "    alpha = 0.5\n",
    "    # Compute log h(w) for posterior samples\n",
    "    log_h_post = logsumexp([\n",
    "        np.log(alpha) + post_log_priors,\n",
    "        np.log(1 - alpha) + post_log_unnorm - log_Z_approx\n",
    "    ], axis=0)\n",
    "\n",
    "    # Compute log h(w) for prior samples\n",
    "    log_h_prior = logsumexp([\n",
    "        np.log(alpha) + prior_log_priors,\n",
    "        np.log(1 - alpha) + prior_log_unnorm - log_Z_approx\n",
    "    ], axis=0)\n",
    "\n",
    "    # Numerator: E_h [ P(w | D, P) / h(w) ]\n",
    "    num_terms = (post_log_unnorm - log_Z_approx) - log_h_post\n",
    "    numerator = logsumexp(num_terms) - np.log(n_samples)\n",
    "\n",
    "    # Denominator: E_h [ P(D, P | w) P(w) / h(w) ]\n",
    "    denom_terms = prior_log_unnorm - log_h_prior\n",
    "    denominator = logsumexp(denom_terms) - np.log(n_samples)\n",
    "\n",
    "    log_marginal = numerator - denominator\n",
    "    return log_marginal\n",
    "\n",
    "def compute_entropy_importance_sampling(Phi_tau_i, Phi_tau_j, samples, beta, log_P_H):\n",
    "    log_probs = log_likelihood_vectorized(samples, Phi_tau_i, Phi_tau_j, beta)\n",
    "    if np.any(np.isnan(log_probs)) or np.any(np.isinf(log_probs)):\n",
    "        raise ValueError(\"Log probabilities contain NaN or Inf values.\")\n",
    "    \n",
    "    log_prior_probs = log_prior_vectorized(samples)\n",
    "    first_term = -np.mean(log_probs)\n",
    "    prior_term = -np.mean(log_prior_probs)\n",
    "    print(f\"First term: {first_term:.4f}\")\n",
    "    print(f\"Prior term: {prior_term:.4f}\")\n",
    "    \n",
    "    entropy = first_term + prior_term + log_P_H\n",
    "    return entropy, log_probs\n",
    "\n",
    "def bayesian_rex_mcmc(trex_model, Phi_tau_i, Phi_tau_j, num_samples=10000, burn_in=1000, beta=1.0, proposal_std=0.005):\n",
    "    w_current = trex_model.model.weight.data.clone().squeeze().to(device)\n",
    "    w_current = w_current / torch.norm(w_current, p=2)\n",
    "    \n",
    "    def log_prior(w):\n",
    "        return -0.5 * torch.sum(w ** 2)\n",
    "    \n",
    "    def log_likelihood(w, Phi_tau_i, Phi_tau_j, beta):\n",
    "        R_i = torch.matmul(Phi_tau_i, w)\n",
    "        R_j = torch.matmul(Phi_tau_j, w)\n",
    "        terms = beta * R_j - torch.log(torch.exp(beta * R_i) + torch.exp(beta * R_j))\n",
    "        return torch.sum(terms)\n",
    "    \n",
    "    samples = []\n",
    "    accepted = 0\n",
    "    w_map = w_current.clone()\n",
    "    log_posterior_map = log_prior(w_current) + log_likelihood(w_current, Phi_tau_i, Phi_tau_j, beta)\n",
    "    \n",
    "    for step in range(num_samples + burn_in):\n",
    "        w_proposed = w_current + torch.normal(mean=0, std=proposal_std, size=w_current.shape).to(device)\n",
    "        w_proposed = w_proposed / torch.norm(w_proposed, p=2)\n",
    "        \n",
    "        log_posterior_current = log_prior(w_current) + log_likelihood(w_current, Phi_tau_i, Phi_tau_j, beta)\n",
    "        log_posterior_proposed = log_prior(w_proposed) + log_likelihood(w_proposed, Phi_tau_i, Phi_tau_j, beta)\n",
    "        \n",
    "        if log_posterior_proposed > log_posterior_map:\n",
    "            w_map = w_proposed.clone()\n",
    "            log_posterior_map = log_posterior_proposed\n",
    "        \n",
    "        log_alpha = log_posterior_proposed - log_posterior_current\n",
    "        alpha = torch.exp(log_alpha)\n",
    "        \n",
    "        if torch.rand(1, device=device) < alpha:\n",
    "            w_current = w_proposed\n",
    "            accepted += 1\n",
    "            if step >= burn_in:\n",
    "                samples.append(w_current.cpu().detach().numpy())\n",
    "        \n",
    "        if step % 5000 == 0:\n",
    "            acceptance_rate = accepted / (step + 1) if step > 0 else 0\n",
    "            print(f\"MCMC Step {step}, Acceptance Rate: {acceptance_rate:.4f}\")\n",
    "    \n",
    "    thinning = 10\n",
    "    samples = samples[::thinning]\n",
    "    \n",
    "    print(f\"Final Acceptance Rate: {accepted / (num_samples + burn_in):.4f}\")\n",
    "    print(f\"MAP Log Posterior: {log_posterior_map.item()}\")\n",
    "    \n",
    "    samples = np.array(samples)\n",
    "    return samples, w_map\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    feedback_path = 'ppo_merge-v0_1377.pkl'\n",
    "    with open(feedback_path, 'rb') as file:\n",
    "        feedback_data = pickle.load(file)\n",
    "    \n",
    "    segments = feedback_data['segments']\n",
    "    preferences = feedback_data['preferences']\n",
    "    preferences = np.random.permutation(preferences).tolist()\n",
    "    print(\"Preferences have been shuffled.\")\n",
    "    \n",
    "    state_dim = 5\n",
    "    feature_dim = 10\n",
    "    encoder = FeatureEncoder(input_dim=state_dim, feature_dim=feature_dim)\n",
    "    trex_model = TREXRewardPredictor(feature_dim=feature_dim)\n",
    "    \n",
    "    encoder.load_state_dict(torch.load(\"feature_encoder_pretrain.pth\", map_location=device))\n",
    "    trex_model.load_state_dict(torch.load(\"trex_model_finetune.pth\", map_location=device))\n",
    "    \n",
    "    encoder = encoder.to(device)\n",
    "    trex_model = trex_model.to(device)\n",
    "    encoder.eval()\n",
    "    trex_model.eval()\n",
    "    \n",
    "    print(\"Models loaded from 'feature_encoder_pretrain.pth' and 'trex_model_finetune.pth'\")\n",
    "    \n",
    "    n_prior_samples = 20000\n",
    "    prior_samples = np.random.normal(0, 1, (n_prior_samples, feature_dim))\n",
    "    prior_samples = prior_samples / np.linalg.norm(prior_samples, axis=1, keepdims=True)\n",
    "    \n",
    "    preference_sizes = range(1000, len(preferences) + 1000, 1000)\n",
    "    entropies = []\n",
    "    all_posterior_samples = []\n",
    "    all_map_solutions = []\n",
    "    beta = 1.0\n",
    "    \n",
    "    print(\"Running Bayesian REX MCMC with bridge sampling for marginal likelihood...\")\n",
    "    for pref_size in preference_sizes:\n",
    "        print(f\"\\nProcessing preferences 0:{pref_size}\")\n",
    "        \n",
    "        Phi_tau_i_full, Phi_tau_j_full = encode_preferences(encoder, preferences[:pref_size], segments)\n",
    "        \n",
    "        proposal_std = 0.008\n",
    "        burn_in = 10000\n",
    "        \n",
    "        posterior_samples, map_solution = bayesian_rex_mcmc(\n",
    "            trex_model,\n",
    "            Phi_tau_i_full,\n",
    "            Phi_tau_j_full,\n",
    "            num_samples=50000,\n",
    "            burn_in=burn_in,\n",
    "            beta=beta,\n",
    "            proposal_std=proposal_std\n",
    "        )\n",
    "        \n",
    "        # Estimate marginal likelihood using bridge sampling\n",
    "        log_P_H = bridge_sampling_marginal_likelihood(\n",
    "            posterior_samples, prior_samples, Phi_tau_i_full, Phi_tau_j_full, beta\n",
    "        )\n",
    "        print(f\"Log P(H_0:{pref_size}) via Bridge Sampling = {log_P_H:.4f}\")\n",
    "        \n",
    "        entropy, log_probs = compute_entropy_importance_sampling(\n",
    "            Phi_tau_i_full, Phi_tau_j_full, posterior_samples, beta, log_P_H\n",
    "        )\n",
    "        \n",
    "        from numpy import correlate\n",
    "        if len(posterior_samples) > 1:\n",
    "            autocorr = correlate(posterior_samples[:, 0], posterior_samples[:, 0], mode='full')\n",
    "            autocorr = autocorr[len(autocorr)//2:len(autocorr)//2+5] / autocorr[len(autocorr)//2]\n",
    "            print(f\"Autocorrelation for first dimension (lags 0-4): {autocorr}\")\n",
    "        else:\n",
    "            print(\"Not enough samples to compute autocorrelation.\")\n",
    "        \n",
    "        sample_variance = np.var(posterior_samples, axis=0)\n",
    "        print(f\"Sample variance: {sample_variance}\")\n",
    "        \n",
    "        log_likelihood_map = log_likelihood_vectorized(map_solution.reshape(1, -1), Phi_tau_i_full, Phi_tau_j_full, beta)\n",
    "        print(f\"Log likelihood at MAP: {log_likelihood_map[0]:.4f}\")\n",
    "        print(f\"Average log likelihood from samples: {-np.mean(log_probs):.4f}\")\n",
    "        \n",
    "        entropies.append(entropy)\n",
    "        all_posterior_samples.append(posterior_samples)\n",
    "        all_map_solutions.append(map_solution)\n",
    "        \n",
    "        print(f\"Entropy with {pref_size} preferences: {entropy:.4f}\")\n",
    "    \n",
    "    np.save(\"entropies.npy\", np.array(entropies))\n",
    "    for i, pref_size in enumerate(preference_sizes):\n",
    "        np.save(f\"posterior_samples_{pref_size}.npy\", all_posterior_samples[i])\n",
    "        map_solution = all_map_solutions[i]\n",
    "        if isinstance(map_solution, torch.Tensor):\n",
    "            map_solution = map_solution.cpu().detach().numpy()\n",
    "        np.save(f\"map_solution_{pref_size}.npy\", map_solution)\n",
    "\n",
    "    print(\"\\nSummary of Entropies:\")\n",
    "    for pref_size, entropy in zip(preference_sizes, entropies):\n",
    "        print(f\"Preferences 0:{pref_size}: Entropy = {entropy:.4f}\")\n",
    "\n",
    "    print(\"Results saved: entropies.npy, posterior_samples_*.npy, map_solution_*.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1578d565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2355.7934226989746,\n",
       " 4797.456374645233,\n",
       " 7370.742642402649,\n",
       " 8867.904938697815,\n",
       " 13352.862210273743,\n",
       " 14984.11238861084,\n",
       " 17721.714725971222,\n",
       " 20553.741911888123,\n",
       " 20317.327649593353,\n",
       " 25822.959520816803]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301c8ab8",
   "metadata": {},
   "source": [
    "## Harmonic Mean Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff188a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preferences have been shuffled.\n",
      "Models loaded from 'feature_encoder_pretrain.pth' and 'trex_model_finetune.pth'\n",
      "Running Bayesian REX MCMC with harmonic mean and bridge sampling for marginal likelihood...\n",
      "\n",
      "Processing preferences 0:1000\n",
      "MCMC Step 0, Acceptance Rate: 0.0000\n",
      "MCMC Step 5000, Acceptance Rate: 0.4699\n",
      "MCMC Step 10000, Acceptance Rate: 0.4685\n",
      "MCMC Step 15000, Acceptance Rate: 0.4702\n",
      "MCMC Step 20000, Acceptance Rate: 0.4691\n",
      "MCMC Step 25000, Acceptance Rate: 0.4692\n",
      "MCMC Step 30000, Acceptance Rate: 0.4676\n",
      "MCMC Step 35000, Acceptance Rate: 0.4668\n",
      "MCMC Step 40000, Acceptance Rate: 0.4652\n",
      "MCMC Step 45000, Acceptance Rate: 0.4644\n",
      "MCMC Step 50000, Acceptance Rate: 0.4644\n",
      "MCMC Step 55000, Acceptance Rate: 0.4645\n",
      "Final Acceptance Rate: 0.4644\n",
      "MAP Log Posterior: -1004.2062377929688\n",
      "Log P(H_0:1000) via Harmonic Mean = -1011.8469\n",
      "Log P(H_0:1000) via Bridge Sampling = 1157.2259\n",
      "First term: 1007.9698\n",
      "Prior term: 0.5000\n",
      "First term: 1007.9698\n",
      "Prior term: 0.5000\n",
      "Autocorrelation for first dimension (lags 0-4): [1.         0.9436503  0.92589736 0.920076   0.91992176]\n",
      "Sample variance: [2.5116181e-04 2.1482751e-04 2.0857531e-05 2.4622711e-04 2.6180508e-04\n",
      " 2.4483289e-04 2.8500665e-04 2.4879409e-04 2.7613534e-04 2.5395502e-04]\n",
      "Log likelihood at MAP: -1003.7062\n",
      "Average log likelihood from samples: 1007.9698\n",
      "Entropy with 1000 preferences (Harmonic Mean): -3.3771\n",
      "Entropy with 1000 preferences (Bridge Sampling): 2165.6957\n",
      "\n",
      "Processing preferences 0:2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1267509/1240284996.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_batch = torch.tensor(w_batch, dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCMC Step 0, Acceptance Rate: 0.0000\n",
      "MCMC Step 5000, Acceptance Rate: 0.3187\n",
      "MCMC Step 10000, Acceptance Rate: 0.3129\n",
      "MCMC Step 15000, Acceptance Rate: 0.3097\n",
      "MCMC Step 20000, Acceptance Rate: 0.3095\n",
      "MCMC Step 25000, Acceptance Rate: 0.3076\n",
      "MCMC Step 30000, Acceptance Rate: 0.3056\n",
      "MCMC Step 35000, Acceptance Rate: 0.3046\n",
      "MCMC Step 40000, Acceptance Rate: 0.3037\n",
      "MCMC Step 45000, Acceptance Rate: 0.3050\n",
      "MCMC Step 50000, Acceptance Rate: 0.3054\n",
      "MCMC Step 55000, Acceptance Rate: 0.3051\n",
      "Final Acceptance Rate: 0.3051\n",
      "MAP Log Posterior: -1999.2215576171875\n",
      "Log P(H_0:2000) via Harmonic Mean = -2007.7795\n",
      "Log P(H_0:2000) via Bridge Sampling = 2761.0744\n",
      "First term: 2003.1044\n",
      "Prior term: 0.5000\n",
      "First term: 2003.1044\n",
      "Prior term: 0.5000\n",
      "Autocorrelation for first dimension (lags 0-4): [1.         0.942019   0.93438756 0.9328645  0.9314358 ]\n",
      "Sample variance: [1.2476741e-04 1.1029735e-04 1.0779206e-05 1.3718240e-04 1.2993690e-04\n",
      " 1.4057408e-04 1.3762675e-04 1.2083919e-04 1.3601471e-04 1.3250746e-04]\n",
      "Log likelihood at MAP: -1998.7216\n",
      "Average log likelihood from samples: 2003.1044\n",
      "Entropy with 2000 preferences (Harmonic Mean): -4.1751\n",
      "Entropy with 2000 preferences (Bridge Sampling): 4764.6787\n",
      "\n",
      "Processing preferences 0:3000\n",
      "MCMC Step 0, Acceptance Rate: 0.0000\n",
      "MCMC Step 5000, Acceptance Rate: 0.2396\n",
      "MCMC Step 10000, Acceptance Rate: 0.2288\n",
      "MCMC Step 15000, Acceptance Rate: 0.2242\n",
      "MCMC Step 20000, Acceptance Rate: 0.2247\n",
      "MCMC Step 25000, Acceptance Rate: 0.2220\n",
      "MCMC Step 30000, Acceptance Rate: 0.2225\n",
      "MCMC Step 35000, Acceptance Rate: 0.2218\n",
      "MCMC Step 40000, Acceptance Rate: 0.2214\n",
      "MCMC Step 45000, Acceptance Rate: 0.2208\n",
      "MCMC Step 50000, Acceptance Rate: 0.2209\n",
      "MCMC Step 55000, Acceptance Rate: 0.2213\n",
      "Final Acceptance Rate: 0.2214\n",
      "MAP Log Posterior: -3064.755615234375\n",
      "Log P(H_0:3000) via Harmonic Mean = -3074.1817\n",
      "Log P(H_0:3000) via Bridge Sampling = 4556.4780\n",
      "First term: 3068.9651\n",
      "Prior term: 0.5000\n",
      "First term: 3068.9651\n",
      "Prior term: 0.5000\n",
      "Autocorrelation for first dimension (lags 0-4): [1.         0.4117083  0.37318596 0.35477933 0.38403153]\n",
      "Sample variance: [9.64840438e-05 7.58627866e-05 8.41087240e-06 1.00815356e-04\n",
      " 9.50190151e-05 9.82423007e-05 9.42587358e-05 8.72833116e-05\n",
      " 9.62896229e-05 9.52793707e-05]\n",
      "Log likelihood at MAP: -3064.2556\n",
      "Average log likelihood from samples: 3068.9651\n",
      "Entropy with 3000 preferences (Harmonic Mean): -4.7167\n",
      "Entropy with 3000 preferences (Bridge Sampling): 7625.9431\n",
      "\n",
      "Processing preferences 0:4000\n",
      "MCMC Step 0, Acceptance Rate: 0.0000\n",
      "MCMC Step 5000, Acceptance Rate: 0.1610\n",
      "MCMC Step 10000, Acceptance Rate: 0.1603\n",
      "MCMC Step 15000, Acceptance Rate: 0.1562\n",
      "MCMC Step 20000, Acceptance Rate: 0.1554\n",
      "MCMC Step 25000, Acceptance Rate: 0.1564\n",
      "MCMC Step 30000, Acceptance Rate: 0.1558\n",
      "MCMC Step 35000, Acceptance Rate: 0.1563\n",
      "MCMC Step 40000, Acceptance Rate: 0.1553\n",
      "MCMC Step 45000, Acceptance Rate: 0.1566\n",
      "MCMC Step 50000, Acceptance Rate: 0.1585\n",
      "MCMC Step 55000, Acceptance Rate: 0.1578\n",
      "Final Acceptance Rate: 0.1582\n",
      "MAP Log Posterior: -3974.91015625\n",
      "Log P(H_0:4000) via Harmonic Mean = -3982.5997\n",
      "Log P(H_0:4000) via Bridge Sampling = 6252.9991\n",
      "First term: 3979.1309\n",
      "Prior term: 0.5000\n",
      "First term: 3979.1309\n",
      "Prior term: 0.5000\n",
      "Autocorrelation for first dimension (lags 0-4): [1.         0.0541541  0.0687476  0.01435653 0.04500313]\n",
      "Sample variance: [7.3133022e-05 5.6762263e-05 6.2386393e-06 6.6868735e-05 7.1770657e-05\n",
      " 7.1301489e-05 7.2614632e-05 6.8659618e-05 7.5807649e-05 7.2133553e-05]\n",
      "Log likelihood at MAP: -3974.4102\n",
      "Average log likelihood from samples: 3979.1309\n",
      "Entropy with 4000 preferences (Harmonic Mean): -2.9689\n",
      "Entropy with 4000 preferences (Bridge Sampling): 10232.6300\n",
      "\n",
      "Processing preferences 0:5000\n",
      "MCMC Step 0, Acceptance Rate: 0.0000\n",
      "MCMC Step 5000, Acceptance Rate: 0.1268\n",
      "MCMC Step 10000, Acceptance Rate: 0.1195\n",
      "MCMC Step 15000, Acceptance Rate: 0.1239\n",
      "MCMC Step 20000, Acceptance Rate: 0.1225\n",
      "MCMC Step 25000, Acceptance Rate: 0.1215\n",
      "MCMC Step 30000, Acceptance Rate: 0.1220\n",
      "MCMC Step 35000, Acceptance Rate: 0.1200\n",
      "MCMC Step 40000, Acceptance Rate: 0.1198\n",
      "MCMC Step 45000, Acceptance Rate: 0.1183\n",
      "MCMC Step 50000, Acceptance Rate: 0.1180\n",
      "MCMC Step 55000, Acceptance Rate: 0.1182\n",
      "Final Acceptance Rate: 0.1180\n",
      "MAP Log Posterior: -5014.53125\n",
      "Log P(H_0:5000) via Harmonic Mean = -5025.8779\n",
      "Log P(H_0:5000) via Bridge Sampling = 7771.5890\n",
      "First term: 5018.6519\n",
      "Prior term: 0.5000\n",
      "First term: 5018.6519\n",
      "Prior term: 0.5000\n",
      "Autocorrelation for first dimension (lags 0-4): [1.         0.9065543  0.90498483 0.9009487  0.8984919 ]\n",
      "Sample variance: [6.0945375e-05 4.0510171e-05 4.7937247e-06 6.4930668e-05 5.8516191e-05\n",
      " 5.7491266e-05 5.5046330e-05 4.6821038e-05 6.2695835e-05 6.5694141e-05]\n",
      "Log likelihood at MAP: -5014.0312\n",
      "Average log likelihood from samples: 5018.6519\n",
      "Entropy with 5000 preferences (Harmonic Mean): -6.7261\n",
      "Entropy with 5000 preferences (Bridge Sampling): 12790.7409\n",
      "\n",
      "Processing preferences 0:6000\n",
      "MCMC Step 0, Acceptance Rate: 0.0000\n",
      "MCMC Step 5000, Acceptance Rate: 0.1066\n",
      "MCMC Step 10000, Acceptance Rate: 0.1006\n",
      "MCMC Step 15000, Acceptance Rate: 0.0970\n",
      "MCMC Step 20000, Acceptance Rate: 0.0935\n",
      "MCMC Step 25000, Acceptance Rate: 0.0921\n",
      "MCMC Step 30000, Acceptance Rate: 0.0912\n",
      "MCMC Step 35000, Acceptance Rate: 0.0900\n",
      "MCMC Step 40000, Acceptance Rate: 0.0904\n",
      "MCMC Step 45000, Acceptance Rate: 0.0908\n",
      "MCMC Step 50000, Acceptance Rate: 0.0898\n",
      "MCMC Step 55000, Acceptance Rate: 0.0901\n",
      "Final Acceptance Rate: 0.0903\n",
      "MAP Log Posterior: -6171.47607421875\n",
      "Log P(H_0:6000) via Harmonic Mean = -6179.4925\n",
      "Log P(H_0:6000) via Bridge Sampling = 9142.7389\n",
      "First term: 6175.8887\n",
      "Prior term: 0.5000\n",
      "First term: 6175.8887\n",
      "Prior term: 0.5000\n",
      "Autocorrelation for first dimension (lags 0-4): [1.         0.52790004 0.5587596  0.5443619  0.532728  ]\n",
      "Sample variance: [4.8499547e-05 3.8139075e-05 4.4550379e-06 5.2065945e-05 4.9600236e-05\n",
      " 4.5788496e-05 4.6754707e-05 4.7311372e-05 5.2101877e-05 5.1702809e-05]\n",
      "Log likelihood at MAP: -6170.9761\n",
      "Average log likelihood from samples: 6175.8887\n",
      "Entropy with 6000 preferences (Harmonic Mean): -3.1038\n",
      "Entropy with 6000 preferences (Bridge Sampling): 15319.1276\n",
      "\n",
      "Processing preferences 0:7000\n",
      "MCMC Step 0, Acceptance Rate: 0.0000\n",
      "MCMC Step 5000, Acceptance Rate: 0.0926\n",
      "MCMC Step 10000, Acceptance Rate: 0.0820\n",
      "MCMC Step 15000, Acceptance Rate: 0.0772\n",
      "MCMC Step 20000, Acceptance Rate: 0.0793\n",
      "MCMC Step 25000, Acceptance Rate: 0.0767\n",
      "MCMC Step 30000, Acceptance Rate: 0.0770\n",
      "MCMC Step 35000, Acceptance Rate: 0.0765\n",
      "MCMC Step 40000, Acceptance Rate: 0.0761\n",
      "MCMC Step 45000, Acceptance Rate: 0.0756\n",
      "MCMC Step 50000, Acceptance Rate: 0.0759\n",
      "MCMC Step 55000, Acceptance Rate: 0.0763\n",
      "Final Acceptance Rate: 0.0768\n",
      "MAP Log Posterior: -6922.9462890625\n",
      "Log P(H_0:7000) via Harmonic Mean = -6932.3452\n",
      "Log P(H_0:7000) via Bridge Sampling = 10751.1554\n",
      "First term: 6927.4624\n",
      "Prior term: 0.5000\n",
      "First term: 6927.4624\n",
      "Prior term: 0.5000\n",
      "Autocorrelation for first dimension (lags 0-4): [1.         0.14719774 0.13645439 0.08507667 0.1660958 ]\n",
      "Sample variance: [4.7940408e-05 3.3394390e-05 3.8971980e-06 4.0608127e-05 4.0654973e-05\n",
      " 4.8677390e-05 4.7922727e-05 3.8935690e-05 4.4206328e-05 4.3179531e-05]\n",
      "Log likelihood at MAP: -6922.4463\n",
      "Average log likelihood from samples: 6927.4624\n",
      "Entropy with 7000 preferences (Harmonic Mean): -4.3828\n",
      "Entropy with 7000 preferences (Bridge Sampling): 17679.1178\n",
      "\n",
      "Processing preferences 0:8000\n",
      "MCMC Step 0, Acceptance Rate: 0.0000\n",
      "MCMC Step 5000, Acceptance Rate: 0.0830\n",
      "MCMC Step 10000, Acceptance Rate: 0.0718\n",
      "MCMC Step 15000, Acceptance Rate: 0.0697\n",
      "MCMC Step 20000, Acceptance Rate: 0.0675\n",
      "MCMC Step 25000, Acceptance Rate: 0.0638\n",
      "MCMC Step 30000, Acceptance Rate: 0.0619\n",
      "MCMC Step 35000, Acceptance Rate: 0.0614\n",
      "MCMC Step 40000, Acceptance Rate: 0.0611\n",
      "MCMC Step 45000, Acceptance Rate: 0.0606\n",
      "MCMC Step 50000, Acceptance Rate: 0.0597\n",
      "MCMC Step 55000, Acceptance Rate: 0.0593\n",
      "Final Acceptance Rate: 0.0596\n",
      "MAP Log Posterior: -7956.10546875\n",
      "Log P(H_0:8000) via Harmonic Mean = -7965.3089\n",
      "Log P(H_0:8000) via Bridge Sampling = 12553.0609\n",
      "First term: 7960.6240\n",
      "Prior term: 0.5000\n",
      "First term: 7960.6240\n",
      "Prior term: 0.5000\n",
      "Autocorrelation for first dimension (lags 0-4): [1.         0.49464706 0.51681834 0.49350423 0.48239264]\n",
      "Sample variance: [3.4994813e-05 3.3181048e-05 3.5470869e-06 3.7906226e-05 4.4390013e-05\n",
      " 3.5916903e-05 3.6330159e-05 3.4273231e-05 3.9781833e-05 4.2357777e-05]\n",
      "Log likelihood at MAP: -7955.6055\n",
      "Average log likelihood from samples: 7960.6240\n",
      "Entropy with 8000 preferences (Harmonic Mean): -4.1848\n",
      "Entropy with 8000 preferences (Bridge Sampling): 20514.1849\n",
      "\n",
      "Processing preferences 0:9000\n",
      "MCMC Step 0, Acceptance Rate: 0.0000\n",
      "MCMC Step 5000, Acceptance Rate: 0.0692\n",
      "MCMC Step 10000, Acceptance Rate: 0.0525\n",
      "MCMC Step 15000, Acceptance Rate: 0.0500\n",
      "MCMC Step 20000, Acceptance Rate: 0.0519\n",
      "MCMC Step 25000, Acceptance Rate: 0.0496\n",
      "MCMC Step 30000, Acceptance Rate: 0.0487\n",
      "MCMC Step 35000, Acceptance Rate: 0.0478\n",
      "MCMC Step 40000, Acceptance Rate: 0.0479\n",
      "MCMC Step 45000, Acceptance Rate: 0.0483\n",
      "MCMC Step 50000, Acceptance Rate: 0.0481\n",
      "MCMC Step 55000, Acceptance Rate: 0.0481\n",
      "Final Acceptance Rate: 0.0478\n",
      "MAP Log Posterior: -8810.333984375\n",
      "Log P(H_0:9000) via Harmonic Mean = -8820.1762\n",
      "Log P(H_0:9000) via Bridge Sampling = 13504.9911\n",
      "First term: 8814.9785\n",
      "Prior term: 0.5000\n",
      "First term: 8814.9785\n",
      "Prior term: 0.5000\n",
      "Autocorrelation for first dimension (lags 0-4): [ 1.         -0.08369254  0.16253787  0.08553349  0.0474116 ]\n",
      "Sample variance: [3.2789092e-05 2.8656732e-05 3.2432704e-06 3.4474000e-05 3.7100530e-05\n",
      " 3.4170323e-05 3.0506935e-05 3.0244404e-05 3.4964967e-05 3.6684480e-05]\n",
      "Log likelihood at MAP: -8809.8340\n",
      "Average log likelihood from samples: 8814.9785\n",
      "Entropy with 9000 preferences (Harmonic Mean): -4.6976\n",
      "Entropy with 9000 preferences (Bridge Sampling): 22320.4696\n",
      "\n",
      "Processing preferences 0:10000\n",
      "MCMC Step 0, Acceptance Rate: 0.0000\n",
      "MCMC Step 5000, Acceptance Rate: 0.0642\n",
      "MCMC Step 10000, Acceptance Rate: 0.0523\n",
      "MCMC Step 15000, Acceptance Rate: 0.0485\n",
      "MCMC Step 20000, Acceptance Rate: 0.0465\n",
      "MCMC Step 25000, Acceptance Rate: 0.0424\n",
      "MCMC Step 30000, Acceptance Rate: 0.0423\n",
      "MCMC Step 35000, Acceptance Rate: 0.0404\n",
      "MCMC Step 40000, Acceptance Rate: 0.0396\n",
      "MCMC Step 45000, Acceptance Rate: 0.0389\n",
      "MCMC Step 50000, Acceptance Rate: 0.0388\n",
      "MCMC Step 55000, Acceptance Rate: 0.0381\n",
      "Final Acceptance Rate: 0.0378\n",
      "MAP Log Posterior: -9859.4169921875\n",
      "Log P(H_0:10000) via Harmonic Mean = -9867.8245\n",
      "Log P(H_0:10000) via Bridge Sampling = 14963.5454\n",
      "First term: 9863.6758\n",
      "Prior term: 0.5000\n",
      "First term: 9863.6758\n",
      "Prior term: 0.5000\n",
      "Autocorrelation for first dimension (lags 0-4): [1.         0.82637775 0.8132165  0.8107578  0.834031  ]\n",
      "Sample variance: [2.9935782e-05 2.5831156e-05 2.5838785e-06 3.2356678e-05 3.2375174e-05\n",
      " 3.0721283e-05 2.8926208e-05 2.9999261e-05 3.1940599e-05 3.1268803e-05]\n",
      "Log likelihood at MAP: -9858.9170\n",
      "Average log likelihood from samples: 9863.6758\n",
      "Entropy with 10000 preferences (Harmonic Mean): -3.6487\n",
      "Entropy with 10000 preferences (Bridge Sampling): 24827.7211\n",
      "\n",
      "Summary of Entropies:\n",
      "Preferences 0:1000: Entropy (Harmonic Mean) = -3.3771, (Bridge Sampling) = 2165.6957\n",
      "Preferences 0:2000: Entropy (Harmonic Mean) = -4.1751, (Bridge Sampling) = 4764.6787\n",
      "Preferences 0:3000: Entropy (Harmonic Mean) = -4.7167, (Bridge Sampling) = 7625.9431\n",
      "Preferences 0:4000: Entropy (Harmonic Mean) = -2.9689, (Bridge Sampling) = 10232.6300\n",
      "Preferences 0:5000: Entropy (Harmonic Mean) = -6.7261, (Bridge Sampling) = 12790.7409\n",
      "Preferences 0:6000: Entropy (Harmonic Mean) = -3.1038, (Bridge Sampling) = 15319.1276\n",
      "Preferences 0:7000: Entropy (Harmonic Mean) = -4.3828, (Bridge Sampling) = 17679.1178\n",
      "Preferences 0:8000: Entropy (Harmonic Mean) = -4.1848, (Bridge Sampling) = 20514.1849\n",
      "Preferences 0:9000: Entropy (Harmonic Mean) = -4.6976, (Bridge Sampling) = 22320.4696\n",
      "Preferences 0:10000: Entropy (Harmonic Mean) = -3.6487, (Bridge Sampling) = 24827.7211\n",
      "Results saved: entropies_hm.npy, entropies_bridge.npy, posterior_samples_*.npy, map_solution_*.npy\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "import pickle\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def encode_preferences(encoder, preferences, segments):\n",
    "    for param in encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    segment_states = [\n",
    "        torch.tensor(\n",
    "            np.array([state_t[0, 0, :] for state_t, _, _, done_t in segment if not done_t]),\n",
    "            dtype=torch.float32\n",
    "        ).to(device) if any(not done_t for _, _, _, done_t in segment) else None\n",
    "        for segment in segments\n",
    "    ]\n",
    "    Phi_tau_list = []\n",
    "    for seg_states in segment_states:\n",
    "        if seg_states is not None:\n",
    "            with torch.no_grad():\n",
    "                phi, _, _ = encoder(seg_states)\n",
    "            Phi_tau = phi.sum(dim=0)\n",
    "            Phi_tau_list.append(Phi_tau)\n",
    "        else:\n",
    "            Phi_tau_list.append(None)\n",
    "    \n",
    "    valid_pairs = [(i, j) for i, j, _ in preferences if Phi_tau_list[i] is not None and Phi_tau_list[j] is not None]\n",
    "    if not valid_pairs:\n",
    "        raise ValueError(\"No valid preference pairs found.\")\n",
    "    \n",
    "    Phi_tau_i = torch.stack([Phi_tau_list[i] for i, j in valid_pairs])\n",
    "    Phi_tau_j = torch.stack([Phi_tau_list[j] for i, j in valid_pairs])\n",
    "    return Phi_tau_i, Phi_tau_j\n",
    "\n",
    "def log_likelihood_vectorized(w_batch, Phi_tau_i, Phi_tau_j, beta):\n",
    "    w_batch = torch.tensor(w_batch, dtype=torch.float32).to(device)\n",
    "    R_i = torch.matmul(Phi_tau_i, w_batch.T)\n",
    "    R_j = torch.matmul(Phi_tau_j, w_batch.T)\n",
    "    \n",
    "    beta_R_i = beta * R_i\n",
    "    beta_R_j = beta * R_j\n",
    "    max_val = torch.maximum(beta_R_i, beta_R_j)\n",
    "    log_sum_exp = max_val + torch.log(\n",
    "        torch.exp(beta_R_i - max_val) + torch.exp(beta_R_j - max_val)\n",
    "    )\n",
    "    \n",
    "    terms = beta * R_j - log_sum_exp\n",
    "    log_likelihoods = torch.sum(terms, dim=0)\n",
    "    \n",
    "    if torch.any(log_likelihoods > 0):\n",
    "        print(\"Warning: Positive log likelihood detected.\")\n",
    "        print(f\"Sample terms: {terms[:5, :5]}\")\n",
    "        print(f\"Log likelihoods: {log_likelihoods[:5]}\")\n",
    "    \n",
    "    return log_likelihoods.cpu().numpy()\n",
    "\n",
    "def log_prior_vectorized(w_batch):\n",
    "    w_batch = torch.tensor(w_batch, dtype=torch.float32).to(device)\n",
    "    return -0.5 * torch.sum(w_batch ** 2, dim=1).cpu().numpy()\n",
    "\n",
    "def harmonic_mean_marginal_likelihood(posterior_samples, Phi_tau_i, Phi_tau_j, beta):\n",
    "    # Compute log likelihood and log prior for posterior samples\n",
    "    log_likelihoods = log_likelihood_vectorized(posterior_samples, Phi_tau_i, Phi_tau_j, beta)\n",
    "    log_priors = log_prior_vectorized(posterior_samples)\n",
    "    \n",
    "    # Compute log P(D, P | w) P(w)\n",
    "    log_unnorm = log_likelihoods + log_priors\n",
    "    \n",
    "    # Harmonic mean estimator: 1/P(D, P) = E[1/(P(D, P | w) P(w))]\n",
    "    inverse_terms = -log_unnorm  # Since log(1/x) = -log(x)\n",
    "    log_inverse_sum = logsumexp(inverse_terms) - np.log(len(posterior_samples))\n",
    "    \n",
    "    # log P(D, P) = -log(E[1/(P(D, P | w) P(w))])\n",
    "    log_marginal = -log_inverse_sum\n",
    "    return log_marginal\n",
    "\n",
    "def bridge_sampling_marginal_likelihood(posterior_samples, prior_samples, Phi_tau_i, Phi_tau_j, beta):\n",
    "    # Subsample to ensure equal sample sizes for simplicity\n",
    "    n_samples = min(len(posterior_samples), len(prior_samples))\n",
    "    posterior_samples = posterior_samples[np.random.choice(len(posterior_samples), n_samples, replace=False)]\n",
    "    prior_samples = prior_samples[np.random.choice(len(prior_samples), n_samples, replace=False)]\n",
    "\n",
    "    # Compute unnormalized posterior for posterior samples: log P(D, P | w) + log P(w)\n",
    "    post_log_likelihoods = log_likelihood_vectorized(posterior_samples, Phi_tau_i, Phi_tau_j, beta)\n",
    "    post_log_priors = log_prior_vectorized(posterior_samples)\n",
    "    post_log_unnorm = post_log_likelihoods + post_log_priors\n",
    "\n",
    "    # Compute unnormalized posterior for prior samples\n",
    "    prior_log_likelihoods = log_likelihood_vectorized(prior_samples, Phi_tau_i, Phi_tau_j, beta)\n",
    "    prior_log_priors = log_prior_vectorized(prior_samples)\n",
    "    prior_log_unnorm = prior_log_likelihoods + prior_log_priors\n",
    "\n",
    "    # Normalize posterior (approximate log P(D, P))\n",
    "    log_Z_approx = logsumexp(post_log_unnorm) - np.log(n_samples)\n",
    "\n",
    "    # Bridge sampling with a simple mixture h(w) = alpha * P(w) + (1-alpha) * P(w | D, P)\n",
    "    alpha = 0.5\n",
    "    log_h_post = logsumexp([\n",
    "        np.log(alpha) + post_log_priors,\n",
    "        np.log(1 - alpha) + post_log_unnorm - log_Z_approx\n",
    "    ], axis=0)\n",
    "\n",
    "    log_h_prior = logsumexp([\n",
    "        np.log(alpha) + prior_log_priors,\n",
    "        np.log(1 - alpha) + prior_log_unnorm - log_Z_approx\n",
    "    ], axis=0)\n",
    "\n",
    "    num_terms = (post_log_unnorm - log_Z_approx) - log_h_post\n",
    "    numerator = logsumexp(num_terms) - np.log(n_samples)\n",
    "\n",
    "    denom_terms = prior_log_unnorm - log_h_prior\n",
    "    denominator = logsumexp(denom_terms) - np.log(n_samples)\n",
    "\n",
    "    log_marginal = numerator - denominator\n",
    "    return log_marginal\n",
    "\n",
    "def compute_entropy_importance_sampling(Phi_tau_i, Phi_tau_j, samples, beta, log_P_H):\n",
    "    log_probs = log_likelihood_vectorized(samples, Phi_tau_i, Phi_tau_j, beta)\n",
    "    if np.any(np.isnan(log_probs)) or np.any(np.isinf(log_probs)):\n",
    "        raise ValueError(\"Log probabilities contain NaN or Inf values.\")\n",
    "    \n",
    "    log_prior_probs = log_prior_vectorized(samples)\n",
    "    first_term = -np.mean(log_probs)\n",
    "    prior_term = -np.mean(log_prior_probs)\n",
    "    print(f\"First term: {first_term:.4f}\")\n",
    "    print(f\"Prior term: {prior_term:.4f}\")\n",
    "    \n",
    "    entropy = first_term + prior_term + log_P_H\n",
    "    return entropy, log_probs\n",
    "\n",
    "def bayesian_rex_mcmc(trex_model, Phi_tau_i, Phi_tau_j, num_samples=10000, burn_in=1000, beta=1.0, proposal_std=0.005):\n",
    "    w_current = trex_model.model.weight.data.clone().squeeze().to(device)\n",
    "    w_current = w_current / torch.norm(w_current, p=2)\n",
    "    \n",
    "    def log_prior(w):\n",
    "        return -0.5 * torch.sum(w ** 2)\n",
    "    \n",
    "    def log_likelihood(w, Phi_tau_i, Phi_tau_j, beta):\n",
    "        R_i = torch.matmul(Phi_tau_i, w)\n",
    "        R_j = torch.matmul(Phi_tau_j, w)\n",
    "        terms = beta * R_j - torch.log(torch.exp(beta * R_i) + torch.exp(beta * R_j))\n",
    "        return torch.sum(terms)\n",
    "    \n",
    "    samples = []\n",
    "    accepted = 0\n",
    "    w_map = w_current.clone()\n",
    "    log_posterior_map = log_prior(w_current) + log_likelihood(w_current, Phi_tau_i, Phi_tau_j, beta)\n",
    "    \n",
    "    for step in range(num_samples + burn_in):\n",
    "        w_proposed = w_current + torch.normal(mean=0, std=proposal_std, size=w_current.shape).to(device)\n",
    "        w_proposed = w_proposed / torch.norm(w_proposed, p=2)\n",
    "        \n",
    "        log_posterior_current = log_prior(w_current) + log_likelihood(w_current, Phi_tau_i, Phi_tau_j, beta)\n",
    "        log_posterior_proposed = log_prior(w_proposed) + log_likelihood(w_proposed, Phi_tau_i, Phi_tau_j, beta)\n",
    "        \n",
    "        if log_posterior_proposed > log_posterior_map:\n",
    "            w_map = w_proposed.clone()\n",
    "            log_posterior_map = log_posterior_proposed\n",
    "        \n",
    "        log_alpha = log_posterior_proposed - log_posterior_current\n",
    "        alpha = torch.exp(log_alpha)\n",
    "        \n",
    "        if torch.rand(1, device=device) < alpha:\n",
    "            w_current = w_proposed\n",
    "            accepted += 1\n",
    "            if step >= burn_in:\n",
    "                samples.append(w_current.cpu().detach().numpy())\n",
    "        \n",
    "        if step % 5000 == 0:\n",
    "            acceptance_rate = accepted / (step + 1) if step > 0 else 0\n",
    "            print(f\"MCMC Step {step}, Acceptance Rate: {acceptance_rate:.4f}\")\n",
    "    \n",
    "    thinning = 10\n",
    "    samples = samples[::thinning]\n",
    "    \n",
    "    print(f\"Final Acceptance Rate: {accepted / (num_samples + burn_in):.4f}\")\n",
    "    print(f\"MAP Log Posterior: {log_posterior_map.item()}\")\n",
    "    \n",
    "    samples = np.array(samples)\n",
    "    return samples, w_map\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    feedback_path = 'ppo_merge-v0_1377.pkl'\n",
    "    with open(feedback_path, 'rb') as file:\n",
    "        feedback_data = pickle.load(file)\n",
    "    \n",
    "    segments = feedback_data['segments']\n",
    "    preferences = feedback_data['preferences']\n",
    "    preferences = np.random.permutation(preferences).tolist()\n",
    "    print(\"Preferences have been shuffled.\")\n",
    "    \n",
    "    state_dim = 5\n",
    "    feature_dim = 10\n",
    "    encoder = FeatureEncoder(input_dim=state_dim, feature_dim=feature_dim)\n",
    "    trex_model = TREXRewardPredictor(feature_dim=feature_dim)\n",
    "    \n",
    "    encoder.load_state_dict(torch.load(\"feature_encoder_pretrain.pth\", map_location=device))\n",
    "    trex_model.load_state_dict(torch.load(\"trex_model_finetune.pth\", map_location=device))\n",
    "    \n",
    "    encoder = encoder.to(device)\n",
    "    trex_model = trex_model.to(device)\n",
    "    encoder.eval()\n",
    "    trex_model.eval()\n",
    "    \n",
    "    print(\"Models loaded from 'feature_encoder_pretrain.pth' and 'trex_model_finetune.pth'\")\n",
    "    \n",
    "    n_prior_samples = 20000\n",
    "    prior_samples = np.random.normal(0, 1, (n_prior_samples, feature_dim))\n",
    "    prior_samples = prior_samples / np.linalg.norm(prior_samples, axis=1, keepdims=True)\n",
    "    \n",
    "    preference_sizes = range(1000, len(preferences) + 1000, 1000)\n",
    "    entropies_hm = []\n",
    "    entropies_bridge = []\n",
    "    all_posterior_samples = []\n",
    "    all_map_solutions = []\n",
    "    beta = 1.0\n",
    "    \n",
    "    print(\"Running Bayesian REX MCMC with harmonic mean and bridge sampling for marginal likelihood...\")\n",
    "    for pref_size in preference_sizes:\n",
    "        print(f\"\\nProcessing preferences 0:{pref_size}\")\n",
    "        \n",
    "        Phi_tau_i_full, Phi_tau_j_full = encode_preferences(encoder, preferences[:pref_size], segments)\n",
    "        \n",
    "        proposal_std = 0.008\n",
    "        burn_in = 10000\n",
    "        \n",
    "        posterior_samples, map_solution = bayesian_rex_mcmc(\n",
    "            trex_model,\n",
    "            Phi_tau_i_full,\n",
    "            Phi_tau_j_full,\n",
    "            num_samples=50000,\n",
    "            burn_in=burn_in,\n",
    "            beta=beta,\n",
    "            proposal_std=proposal_std\n",
    "        )\n",
    "        \n",
    "        # Estimate marginal likelihood using harmonic mean\n",
    "        log_P_H_hm = harmonic_mean_marginal_likelihood(\n",
    "            posterior_samples, Phi_tau_i_full, Phi_tau_j_full, beta\n",
    "        )\n",
    "        print(f\"Log P(H_0:{pref_size}) via Harmonic Mean = {log_P_H_hm:.4f}\")\n",
    "        \n",
    "        # Estimate marginal likelihood using bridge sampling\n",
    "        log_P_H_bridge = bridge_sampling_marginal_likelihood(\n",
    "            posterior_samples, prior_samples, Phi_tau_i_full, Phi_tau_j_full, beta\n",
    "        )\n",
    "        print(f\"Log P(H_0:{pref_size}) via Bridge Sampling = {log_P_H_bridge:.4f}\")\n",
    "        \n",
    "        # Compute entropy using both estimates\n",
    "        entropy_hm, log_probs = compute_entropy_importance_sampling(\n",
    "            Phi_tau_i_full, Phi_tau_j_full, posterior_samples, beta, log_P_H_hm\n",
    "        )\n",
    "        entropy_bridge, _ = compute_entropy_importance_sampling(\n",
    "            Phi_tau_i_full, Phi_tau_j_full, posterior_samples, beta, log_P_H_bridge\n",
    "        )\n",
    "        \n",
    "        from numpy import correlate\n",
    "        if len(posterior_samples) > 1:\n",
    "            autocorr = correlate(posterior_samples[:, 0], posterior_samples[:, 0], mode='full')\n",
    "            autocorr = autocorr[len(autocorr)//2:len(autocorr)//2+5] / autocorr[len(autocorr)//2]\n",
    "            print(f\"Autocorrelation for first dimension (lags 0-4): {autocorr}\")\n",
    "        else:\n",
    "            print(\"Not enough samples to compute autocorrelation.\")\n",
    "        \n",
    "        sample_variance = np.var(posterior_samples, axis=0)\n",
    "        print(f\"Sample variance: {sample_variance}\")\n",
    "        \n",
    "        log_likelihood_map = log_likelihood_vectorized(map_solution.reshape(1, -1), Phi_tau_i_full, Phi_tau_j_full, beta)\n",
    "        print(f\"Log likelihood at MAP: {log_likelihood_map[0]:.4f}\")\n",
    "        print(f\"Average log likelihood from samples: {-np.mean(log_probs):.4f}\")\n",
    "        \n",
    "        entropies_hm.append(entropy_hm)\n",
    "        entropies_bridge.append(entropy_bridge)\n",
    "        all_posterior_samples.append(posterior_samples)\n",
    "        all_map_solutions.append(map_solution)\n",
    "        \n",
    "        print(f\"Entropy with {pref_size} preferences (Harmonic Mean): {entropy_hm:.4f}\")\n",
    "        print(f\"Entropy with {pref_size} preferences (Bridge Sampling): {entropy_bridge:.4f}\")\n",
    "    \n",
    "    np.save(\"entropies_hm.npy\", np.array(entropies_hm))\n",
    "    np.save(\"entropies_bridge.npy\", np.array(entropies_bridge))\n",
    "    for i, pref_size in enumerate(preference_sizes):\n",
    "        np.save(f\"posterior_samples_{pref_size}.npy\", all_posterior_samples[i])\n",
    "        map_solution = all_map_solutions[i]\n",
    "        if isinstance(map_solution, torch.Tensor):\n",
    "            map_solution = map_solution.cpu().detach().numpy()\n",
    "        np.save(f\"map_solution_{pref_size}.npy\", map_solution)\n",
    "\n",
    "    print(\"\\nSummary of Entropies:\")\n",
    "    for pref_size, entropy_hm, entropy_bridge in zip(preference_sizes, entropies_hm, entropies_bridge):\n",
    "        print(f\"Preferences 0:{pref_size}: Entropy (Harmonic Mean) = {entropy_hm:.4f}, (Bridge Sampling) = {entropy_bridge:.4f}\")\n",
    "\n",
    "    print(\"Results saved: entropies_hm.npy, entropies_bridge.npy, posterior_samples_*.npy, map_solution_*.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426ac48b",
   "metadata": {},
   "source": [
    "## Debuged Bridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ead36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preferences have been shuffled.\n",
      "Models loaded from 'feature_encoder_pretrain.pth' and 'trex_model_finetune.pth'\n",
      "Running Bayesian REX MCMC with harmonic mean and corrected bridge sampling for marginal likelihood...\n",
      "\n",
      "Processing preferences 0:1000\n",
      "MCMC Step 0, Acceptance Rate: 0.0000\n",
      "MCMC Step 5000, Acceptance Rate: 0.4695\n",
      "MCMC Step 10000, Acceptance Rate: 0.4618\n",
      "MCMC Step 15000, Acceptance Rate: 0.4601\n",
      "MCMC Step 20000, Acceptance Rate: 0.4595\n",
      "MCMC Step 25000, Acceptance Rate: 0.4575\n",
      "MCMC Step 30000, Acceptance Rate: 0.4588\n",
      "MCMC Step 35000, Acceptance Rate: 0.4588\n",
      "MCMC Step 40000, Acceptance Rate: 0.4573\n",
      "MCMC Step 45000, Acceptance Rate: 0.4583\n",
      "MCMC Step 50000, Acceptance Rate: 0.4578\n",
      "MCMC Step 55000, Acceptance Rate: 0.4589\n",
      "Final Acceptance Rate: 0.4591\n",
      "MAP Log Posterior: -948.7210083007812\n",
      "Log P(H_0:1000) via Harmonic Mean = -958.5094\n",
      "Initial log Z (harmonic mean): -958.5094\n",
      "Iteration 1, log Z = 1312.0614\n",
      "Iteration 2, log Z = -951.0302\n",
      "Iteration 3, log Z = 1310.9304\n",
      "Iteration 4, log Z = -949.8991\n",
      "Iteration 5, log Z = 1310.3205\n",
      "Iteration 6, log Z = -949.2892\n",
      "Iteration 7, log Z = 1309.9114\n",
      "Iteration 8, log Z = -948.8801\n",
      "Iteration 9, log Z = 1309.6068\n",
      "Iteration 10, log Z = -948.5756\n",
      "Iteration 11, log Z = 1309.3658\n",
      "Iteration 12, log Z = -948.3347\n",
      "Iteration 13, log Z = 1309.1671\n",
      "Iteration 14, log Z = -948.1359\n",
      "Iteration 15, log Z = 1308.9985\n",
      "Iteration 16, log Z = -947.9672\n",
      "Iteration 17, log Z = 1308.8522\n",
      "Iteration 18, log Z = -947.8210\n",
      "Iteration 19, log Z = 1308.7232\n",
      "Iteration 20, log Z = -947.6921\n",
      "Log P(H_0:1000) via Bridge Sampling = -947.6921\n",
      "First term: 952.5142\n",
      "Prior term: 0.5000\n",
      "First term: 952.5142\n",
      "Prior term: 0.5000\n",
      "Autocorrelation for first dimension (lags 0-4): [1.         0.9255425  0.91857904 0.9142523  0.9163591 ]\n",
      "Sample variance: [2.4653945e-04 1.9896221e-04 2.4146462e-05 2.6509329e-04 2.6365367e-04\n",
      " 2.3312349e-04 2.4491220e-04 2.6299767e-04 2.7009833e-04 2.8954106e-04]\n",
      "Log likelihood at MAP: -948.2211\n",
      "Average log likelihood from samples: 952.5142\n",
      "Entropy with 1000 preferences (Harmonic Mean): -5.4953\n",
      "Entropy with 1000 preferences (Bridge Sampling): 5.3221\n",
      "\n",
      "Processing preferences 0:2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1267509/3988585876.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_batch = torch.tensor(w_batch, dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCMC Step 0, Acceptance Rate: 0.0000\n",
      "MCMC Step 5000, Acceptance Rate: 0.3291\n",
      "MCMC Step 10000, Acceptance Rate: 0.3186\n",
      "MCMC Step 15000, Acceptance Rate: 0.3106\n",
      "MCMC Step 20000, Acceptance Rate: 0.3078\n",
      "MCMC Step 25000, Acceptance Rate: 0.3076\n",
      "MCMC Step 30000, Acceptance Rate: 0.3100\n",
      "MCMC Step 35000, Acceptance Rate: 0.3091\n",
      "MCMC Step 40000, Acceptance Rate: 0.3089\n",
      "MCMC Step 45000, Acceptance Rate: 0.3081\n",
      "MCMC Step 50000, Acceptance Rate: 0.3076\n",
      "MCMC Step 55000, Acceptance Rate: 0.3082\n",
      "Final Acceptance Rate: 0.3090\n",
      "MAP Log Posterior: -1967.9453125\n",
      "Log P(H_0:2000) via Harmonic Mean = -1977.2734\n",
      "Initial log Z (harmonic mean): -1977.2734\n",
      "Iteration 1, log Z = 2694.2566\n",
      "Iteration 2, log Z = -1970.3968\n",
      "Iteration 3, log Z = 2693.1327\n",
      "Iteration 4, log Z = -1969.2733\n",
      "Iteration 5, log Z = 2692.5325\n",
      "Iteration 6, log Z = -1968.6727\n",
      "Iteration 7, log Z = 2692.1315\n",
      "Iteration 8, log Z = -1968.2718\n",
      "Iteration 9, log Z = 2691.8337\n",
      "Iteration 10, log Z = -1967.9739\n",
      "Iteration 11, log Z = 2691.5981\n",
      "Iteration 12, log Z = -1967.7386\n",
      "Iteration 13, log Z = 2691.4041\n",
      "Iteration 14, log Z = -1967.5443\n",
      "Iteration 15, log Z = 2691.2391\n",
      "Iteration 16, log Z = -1967.3797\n",
      "Iteration 17, log Z = 2691.0964\n",
      "Iteration 18, log Z = -1967.2366\n",
      "Iteration 19, log Z = 2690.9702\n",
      "Iteration 20, log Z = -1967.1107\n",
      "Log P(H_0:2000) via Bridge Sampling = -1967.1107\n",
      "First term: 1971.8666\n",
      "Prior term: 0.5000\n",
      "First term: 1971.8666\n",
      "Prior term: 0.5000\n",
      "Autocorrelation for first dimension (lags 0-4): [1.        0.5552578 0.5214368 0.4961897 0.5127367]\n",
      "Sample variance: [1.45730985e-04 1.08435655e-04 1.27202293e-05 1.41209268e-04\n",
      " 1.21727731e-04 1.32803660e-04 1.42101155e-04 1.27692430e-04\n",
      " 1.30857748e-04 1.42303048e-04]\n",
      "Log likelihood at MAP: -1967.4453\n",
      "Average log likelihood from samples: 1971.8666\n",
      "Entropy with 2000 preferences (Harmonic Mean): -4.9069\n",
      "Entropy with 2000 preferences (Bridge Sampling): 5.2559\n",
      "\n",
      "Processing preferences 0:3000\n",
      "MCMC Step 0, Acceptance Rate: 0.0000\n",
      "MCMC Step 5000, Acceptance Rate: 0.2432\n",
      "MCMC Step 10000, Acceptance Rate: 0.2352\n",
      "MCMC Step 15000, Acceptance Rate: 0.2279\n",
      "MCMC Step 20000, Acceptance Rate: 0.2272\n",
      "MCMC Step 25000, Acceptance Rate: 0.2262\n",
      "MCMC Step 30000, Acceptance Rate: 0.2251\n",
      "MCMC Step 35000, Acceptance Rate: 0.2227\n",
      "MCMC Step 40000, Acceptance Rate: 0.2225\n",
      "MCMC Step 45000, Acceptance Rate: 0.2223\n",
      "MCMC Step 50000, Acceptance Rate: 0.2218\n",
      "MCMC Step 55000, Acceptance Rate: 0.2210\n",
      "Final Acceptance Rate: 0.2198\n",
      "MAP Log Posterior: -2842.59326171875\n",
      "Log P(H_0:3000) via Harmonic Mean = -2850.2240\n",
      "Initial log Z (harmonic mean): -2850.2240\n",
      "Iteration 1, log Z = 3941.9553\n",
      "Iteration 2, log Z = -2845.0270\n",
      "Iteration 3, log Z = 3940.8407\n",
      "Iteration 4, log Z = -2843.9123\n",
      "Iteration 5, log Z = 3940.2249\n",
      "Iteration 6, log Z = -2843.2966\n",
      "Iteration 7, log Z = 3939.8059\n",
      "Iteration 8, log Z = -2842.8776\n",
      "Iteration 9, log Z = 3939.4917\n",
      "Iteration 10, log Z = -2842.5632\n",
      "Iteration 11, log Z = 3939.2418\n",
      "Iteration 12, log Z = -2842.3132\n",
      "Iteration 13, log Z = 3939.0353\n",
      "Iteration 14, log Z = -2842.1071\n",
      "Iteration 15, log Z = 3938.8603\n",
      "Iteration 16, log Z = -2841.9318\n",
      "Iteration 17, log Z = 3938.7084\n",
      "Iteration 18, log Z = -2841.7800\n",
      "Iteration 19, log Z = 3938.5746\n",
      "Iteration 20, log Z = -2841.6462\n",
      "Log P(H_0:3000) via Bridge Sampling = -2841.6462\n",
      "First term: 2846.5986\n",
      "Prior term: 0.5000\n",
      "First term: 2846.5986\n",
      "Prior term: 0.5000\n",
      "Autocorrelation for first dimension (lags 0-4): [1.         0.43869668 0.43467867 0.48126096 0.47653055]\n",
      "Sample variance: [8.9194975e-05 7.8961406e-05 9.7539596e-06 9.2069582e-05 9.7120392e-05\n",
      " 9.4587827e-05 9.9603443e-05 9.9948447e-05 8.9239802e-05 9.1115959e-05]\n",
      "Log likelihood at MAP: -2842.0933\n",
      "Average log likelihood from samples: 2846.5986\n",
      "Entropy with 3000 preferences (Harmonic Mean): -3.1254\n",
      "Entropy with 3000 preferences (Bridge Sampling): 5.4525\n",
      "\n",
      "Processing preferences 0:4000\n",
      "MCMC Step 0, Acceptance Rate: 0.0000\n",
      "MCMC Step 5000, Acceptance Rate: 0.1882\n",
      "MCMC Step 10000, Acceptance Rate: 0.1653\n",
      "MCMC Step 15000, Acceptance Rate: 0.1607\n",
      "MCMC Step 20000, Acceptance Rate: 0.1608\n",
      "MCMC Step 25000, Acceptance Rate: 0.1616\n",
      "MCMC Step 30000, Acceptance Rate: 0.1601\n",
      "MCMC Step 35000, Acceptance Rate: 0.1619\n",
      "MCMC Step 40000, Acceptance Rate: 0.1615\n",
      "MCMC Step 45000, Acceptance Rate: 0.1610\n",
      "MCMC Step 50000, Acceptance Rate: 0.1609\n",
      "MCMC Step 55000, Acceptance Rate: 0.1608\n",
      "Final Acceptance Rate: 0.1609\n",
      "MAP Log Posterior: -3885.618408203125\n",
      "Log P(H_0:4000) via Harmonic Mean = -3892.7800\n",
      "Initial log Z (harmonic mean): -3892.7800\n",
      "Iteration 1, log Z = 5296.9402\n",
      "Iteration 2, log Z = -3887.8819\n",
      "Iteration 3, log Z = 5295.8733\n",
      "Iteration 4, log Z = -3886.8145\n",
      "Iteration 5, log Z = 5295.2922\n",
      "Iteration 6, log Z = -3886.2335\n",
      "Iteration 7, log Z = 5294.8999\n",
      "Iteration 8, log Z = -3885.8419\n",
      "Iteration 9, log Z = 5294.6069\n",
      "Iteration 10, log Z = -3885.5489\n",
      "Iteration 11, log Z = 5294.3740\n",
      "Iteration 12, log Z = -3885.3155\n",
      "Iteration 13, log Z = 5294.1809\n",
      "Iteration 14, log Z = -3885.1231\n",
      "Iteration 15, log Z = 5294.0172\n",
      "Iteration 16, log Z = -3884.9591\n",
      "Iteration 17, log Z = 5293.8746\n",
      "Iteration 18, log Z = -3884.8165\n",
      "Iteration 19, log Z = 5293.7486\n",
      "Iteration 20, log Z = -3884.6905\n",
      "Log P(H_0:4000) via Bridge Sampling = -3884.6905\n",
      "First term: 3889.3892\n",
      "Prior term: 0.5000\n",
      "First term: 3889.3892\n",
      "Prior term: 0.5000\n",
      "Autocorrelation for first dimension (lags 0-4): [1.         0.77697253 0.7385644  0.73487496 0.716168  ]\n",
      "Sample variance: [6.8039648e-05 5.2851592e-05 6.3957041e-06 7.0410635e-05 6.6728171e-05\n",
      " 6.3692147e-05 7.1438903e-05 5.7618221e-05 7.4496165e-05 6.2746301e-05]\n",
      "Log likelihood at MAP: -3885.1184\n",
      "Average log likelihood from samples: 3889.3892\n",
      "Entropy with 4000 preferences (Harmonic Mean): -2.8909\n",
      "Entropy with 4000 preferences (Bridge Sampling): 5.1986\n",
      "\n",
      "Processing preferences 0:5000\n",
      "MCMC Step 0, Acceptance Rate: 0.0000\n",
      "MCMC Step 5000, Acceptance Rate: 0.1432\n",
      "MCMC Step 10000, Acceptance Rate: 0.1312\n",
      "MCMC Step 15000, Acceptance Rate: 0.1281\n",
      "MCMC Step 20000, Acceptance Rate: 0.1274\n",
      "MCMC Step 25000, Acceptance Rate: 0.1258\n",
      "MCMC Step 30000, Acceptance Rate: 0.1247\n",
      "MCMC Step 35000, Acceptance Rate: 0.1246\n",
      "MCMC Step 40000, Acceptance Rate: 0.1238\n",
      "MCMC Step 45000, Acceptance Rate: 0.1238\n",
      "MCMC Step 50000, Acceptance Rate: 0.1243\n",
      "MCMC Step 55000, Acceptance Rate: 0.1250\n",
      "Final Acceptance Rate: 0.1242\n",
      "MAP Log Posterior: -4907.68603515625\n",
      "Log P(H_0:5000) via Harmonic Mean = -4915.4921\n",
      "Initial log Z (harmonic mean): -4915.4921\n",
      "Iteration 1, log Z = 6854.3385\n",
      "Iteration 2, log Z = -4910.0683\n",
      "Iteration 3, log Z = 6853.1660\n",
      "Iteration 4, log Z = -4908.8955\n",
      "Iteration 5, log Z = 6852.5373\n",
      "Iteration 6, log Z = -4908.2666\n",
      "Iteration 7, log Z = 6852.1164\n",
      "Iteration 8, log Z = -4907.8457\n",
      "Iteration 9, log Z = 6851.8037\n",
      "Iteration 10, log Z = -4907.5332\n",
      "Iteration 11, log Z = 6851.5568\n",
      "Iteration 12, log Z = -4907.2861\n",
      "Iteration 13, log Z = 6851.3534\n",
      "Iteration 14, log Z = -4907.0830\n",
      "Iteration 15, log Z = 6851.1812\n",
      "Iteration 16, log Z = -4906.9111\n",
      "Iteration 17, log Z = 6851.0324\n",
      "Iteration 18, log Z = -4906.7617\n",
      "Iteration 19, log Z = 6850.9008\n",
      "Iteration 20, log Z = -4906.6308\n",
      "Log P(H_0:5000) via Bridge Sampling = -4906.6308\n",
      "First term: 4911.8657\n",
      "Prior term: 0.5000\n",
      "First term: 4911.8657\n",
      "Prior term: 0.5000\n",
      "Autocorrelation for first dimension (lags 0-4): [1.         0.86993587 0.86625975 0.8644207  0.86010784]\n",
      "Sample variance: [5.8593065e-05 4.3973927e-05 5.0014892e-06 6.2317369e-05 6.8096400e-05\n",
      " 6.1201674e-05 6.5159096e-05 3.9903385e-05 5.9726721e-05 5.0720821e-05]\n",
      "Log likelihood at MAP: -4907.1860\n",
      "Average log likelihood from samples: 4911.8657\n",
      "Entropy with 5000 preferences (Harmonic Mean): -3.1263\n",
      "Entropy with 5000 preferences (Bridge Sampling): 5.7349\n",
      "\n",
      "Processing preferences 0:6000\n",
      "MCMC Step 0, Acceptance Rate: 0.0000\n",
      "MCMC Step 5000, Acceptance Rate: 0.1142\n",
      "MCMC Step 10000, Acceptance Rate: 0.1060\n",
      "MCMC Step 15000, Acceptance Rate: 0.1007\n",
      "MCMC Step 20000, Acceptance Rate: 0.0991\n",
      "MCMC Step 25000, Acceptance Rate: 0.0973\n",
      "MCMC Step 30000, Acceptance Rate: 0.0942\n",
      "MCMC Step 35000, Acceptance Rate: 0.0956\n",
      "MCMC Step 40000, Acceptance Rate: 0.0963\n",
      "MCMC Step 45000, Acceptance Rate: 0.0942\n",
      "MCMC Step 50000, Acceptance Rate: 0.0933\n",
      "MCMC Step 55000, Acceptance Rate: 0.0937\n",
      "Final Acceptance Rate: 0.0943\n",
      "MAP Log Posterior: -5926.2099609375\n",
      "Log P(H_0:6000) via Harmonic Mean = -5935.6967\n",
      "Initial log Z (harmonic mean): -5935.6967\n",
      "Iteration 1, log Z = 8312.2566\n",
      "Iteration 2, log Z = -5928.4164\n",
      "Iteration 3, log Z = 8310.9974\n",
      "Iteration 4, log Z = -5927.1567\n",
      "Iteration 5, log Z = 8310.3071\n",
      "Iteration 6, log Z = -5926.4662\n",
      "Iteration 7, log Z = 8309.8469\n",
      "Iteration 8, log Z = -5926.0063\n",
      "Iteration 9, log Z = 8309.5089\n",
      "Iteration 10, log Z = -5925.6684\n",
      "Iteration 11, log Z = 8309.2448\n",
      "Iteration 12, log Z = -5925.4047\n",
      "Iteration 13, log Z = 8309.0298\n",
      "Iteration 14, log Z = -5925.1889\n",
      "Iteration 15, log Z = 8308.8483\n",
      "Iteration 16, log Z = -5925.0082\n",
      "Iteration 17, log Z = 8308.6929\n",
      "Iteration 18, log Z = -5924.8520\n",
      "Iteration 19, log Z = 8308.5560\n",
      "Iteration 20, log Z = -5924.7153\n",
      "Log P(H_0:6000) via Bridge Sampling = -5924.7153\n",
      "First term: 5930.3296\n",
      "Prior term: 0.5000\n",
      "First term: 5930.3296\n",
      "Prior term: 0.5000\n",
      "Autocorrelation for first dimension (lags 0-4): [1.         0.18749091 0.17767283 0.14674233 0.14280224]\n",
      "Sample variance: [4.4525543e-05 3.7580179e-05 4.0699874e-06 4.4406705e-05 4.5954075e-05\n",
      " 5.1745537e-05 5.7535854e-05 4.1281783e-05 5.0961215e-05 5.0419068e-05]\n",
      "Log likelihood at MAP: -5925.7100\n",
      "Average log likelihood from samples: 5930.3296\n",
      "Entropy with 6000 preferences (Harmonic Mean): -4.8671\n",
      "Entropy with 6000 preferences (Bridge Sampling): 6.1143\n",
      "\n",
      "Processing preferences 0:7000\n",
      "MCMC Step 0, Acceptance Rate: 0.0000\n",
      "MCMC Step 5000, Acceptance Rate: 0.1024\n",
      "MCMC Step 10000, Acceptance Rate: 0.0832\n",
      "MCMC Step 15000, Acceptance Rate: 0.0799\n",
      "MCMC Step 20000, Acceptance Rate: 0.0784\n",
      "MCMC Step 25000, Acceptance Rate: 0.0768\n",
      "MCMC Step 30000, Acceptance Rate: 0.0766\n",
      "MCMC Step 35000, Acceptance Rate: 0.0743\n",
      "MCMC Step 40000, Acceptance Rate: 0.0734\n",
      "MCMC Step 45000, Acceptance Rate: 0.0727\n",
      "MCMC Step 50000, Acceptance Rate: 0.0733\n",
      "MCMC Step 55000, Acceptance Rate: 0.0729\n",
      "Final Acceptance Rate: 0.0728\n",
      "MAP Log Posterior: -6905.5859375\n",
      "Log P(H_0:7000) via Harmonic Mean = -6914.3160\n",
      "Initial log Z (harmonic mean): -6914.3160\n",
      "Iteration 1, log Z = 9655.8198\n",
      "Iteration 2, log Z = -6908.4875\n",
      "Iteration 3, log Z = 9654.6288\n",
      "Iteration 4, log Z = -6907.2961\n",
      "Iteration 5, log Z = 9653.9889\n",
      "Iteration 6, log Z = -6906.6574\n",
      "Iteration 7, log Z = 9653.5631\n",
      "Iteration 8, log Z = -6906.2316\n",
      "Iteration 9, log Z = 9653.2479\n",
      "Iteration 10, log Z = -6905.9152\n",
      "Iteration 11, log Z = 9652.9987\n",
      "Iteration 12, log Z = -6905.6672\n",
      "Iteration 13, log Z = 9652.7949\n",
      "Iteration 14, log Z = -6905.4621\n",
      "Iteration 15, log Z = 9652.6215\n",
      "Iteration 16, log Z = -6905.2883\n",
      "Iteration 17, log Z = 9652.4711\n",
      "Iteration 18, log Z = -6905.1379\n",
      "Iteration 19, log Z = 9652.3388\n",
      "Iteration 20, log Z = -6905.0070\n",
      "Log P(H_0:7000) via Bridge Sampling = -6905.0070\n",
      "First term: 6910.4272\n",
      "Prior term: 0.5000\n",
      "First term: 6910.4272\n",
      "Prior term: 0.5000\n",
      "Autocorrelation for first dimension (lags 0-4): [ 1.          0.08061948  0.0432141  -0.00139664  0.14890088]\n",
      "Sample variance: [4.7990652e-05 3.1864180e-05 3.7532795e-06 3.9150120e-05 5.4300122e-05\n",
      " 4.7982438e-05 3.8039299e-05 4.5509831e-05 3.8359951e-05 4.6307436e-05]\n",
      "Log likelihood at MAP: -6905.0854\n",
      "Average log likelihood from samples: 6910.4272\n",
      "Entropy with 7000 preferences (Harmonic Mean): -3.3888\n",
      "Entropy with 7000 preferences (Bridge Sampling): 5.9202\n",
      "\n",
      "Processing preferences 0:8000\n",
      "MCMC Step 0, Acceptance Rate: 0.0000\n",
      "MCMC Step 5000, Acceptance Rate: 0.0820\n",
      "MCMC Step 10000, Acceptance Rate: 0.0703\n",
      "MCMC Step 15000, Acceptance Rate: 0.0647\n",
      "MCMC Step 20000, Acceptance Rate: 0.0636\n",
      "MCMC Step 25000, Acceptance Rate: 0.0631\n",
      "MCMC Step 30000, Acceptance Rate: 0.0619\n",
      "MCMC Step 35000, Acceptance Rate: 0.0614\n",
      "MCMC Step 40000, Acceptance Rate: 0.0621\n",
      "MCMC Step 45000, Acceptance Rate: 0.0616\n",
      "MCMC Step 50000, Acceptance Rate: 0.0616\n",
      "MCMC Step 55000, Acceptance Rate: 0.0613\n",
      "Final Acceptance Rate: 0.0606\n",
      "MAP Log Posterior: -7933.3544921875\n",
      "Log P(H_0:8000) via Harmonic Mean = -7941.8934\n",
      "Initial log Z (harmonic mean): -7941.8934\n",
      "Iteration 1, log Z = 11024.3380\n",
      "Iteration 2, log Z = -7936.0469\n",
      "Iteration 3, log Z = 11023.1523\n",
      "Iteration 4, log Z = -7934.8614\n",
      "Iteration 5, log Z = 11022.4902\n",
      "Iteration 6, log Z = -7934.1992\n",
      "Iteration 7, log Z = 11022.0426\n",
      "Iteration 8, log Z = -7933.7520\n",
      "Iteration 9, log Z = 11021.7108\n",
      "Iteration 10, log Z = -7933.4200\n",
      "Iteration 11, log Z = 11021.4497\n",
      "Iteration 12, log Z = -7933.1602\n",
      "Iteration 13, log Z = 11021.2371\n",
      "Iteration 14, log Z = -7932.9473\n",
      "Iteration 15, log Z = 11021.0577\n",
      "Iteration 16, log Z = -7932.7676\n",
      "Iteration 17, log Z = 11020.9028\n",
      "Iteration 18, log Z = -7932.6114\n",
      "Iteration 19, log Z = 11020.7658\n",
      "Iteration 20, log Z = -7932.4746\n",
      "Log P(H_0:8000) via Bridge Sampling = -7932.4746\n",
      "First term: 7937.8511\n",
      "Prior term: 0.5000\n",
      "First term: 7937.8511\n",
      "Prior term: 0.5000\n",
      "Autocorrelation for first dimension (lags 0-4): [1.         0.9652953  0.9601788  0.95886433 0.9409283 ]\n",
      "Sample variance: [3.4376935e-05 2.7649305e-05 2.9106657e-06 3.7507270e-05 3.5055793e-05\n",
      " 4.4524673e-05 4.1779291e-05 3.3882981e-05 4.1627467e-05 3.5323850e-05]\n",
      "Log likelihood at MAP: -7932.8545\n",
      "Average log likelihood from samples: 7937.8511\n",
      "Entropy with 8000 preferences (Harmonic Mean): -3.5423\n",
      "Entropy with 8000 preferences (Bridge Sampling): 5.8764\n",
      "\n",
      "Processing preferences 0:9000\n",
      "MCMC Step 0, Acceptance Rate: 0.0000\n",
      "MCMC Step 5000, Acceptance Rate: 0.0668\n",
      "MCMC Step 10000, Acceptance Rate: 0.0591\n",
      "MCMC Step 15000, Acceptance Rate: 0.0528\n",
      "MCMC Step 20000, Acceptance Rate: 0.0505\n",
      "MCMC Step 25000, Acceptance Rate: 0.0474\n",
      "MCMC Step 30000, Acceptance Rate: 0.0467\n",
      "MCMC Step 35000, Acceptance Rate: 0.0470\n",
      "MCMC Step 40000, Acceptance Rate: 0.0472\n",
      "MCMC Step 45000, Acceptance Rate: 0.0477\n",
      "MCMC Step 50000, Acceptance Rate: 0.0482\n",
      "MCMC Step 55000, Acceptance Rate: 0.0476\n",
      "Final Acceptance Rate: 0.0467\n",
      "MAP Log Posterior: -8821.4951171875\n",
      "Log P(H_0:9000) via Harmonic Mean = -8829.1850\n",
      "Initial log Z (harmonic mean): -8829.1850\n",
      "Iteration 1, log Z = 12314.1840\n",
      "Iteration 2, log Z = -8824.3236\n",
      "Iteration 3, log Z = 12313.0860\n",
      "Iteration 4, log Z = -8823.2260\n",
      "Iteration 5, log Z = 12312.4979\n",
      "Iteration 6, log Z = -8822.6381\n",
      "Iteration 7, log Z = 12312.1051\n",
      "Iteration 8, log Z = -8822.2455\n",
      "Iteration 9, log Z = 12311.8131\n",
      "Iteration 10, log Z = -8821.9525\n",
      "Iteration 11, log Z = 12311.5811\n",
      "Iteration 12, log Z = -8821.7201\n",
      "Iteration 13, log Z = 12311.3893\n",
      "Iteration 14, log Z = -8821.5287\n",
      "Iteration 15, log Z = 12311.2266\n",
      "Iteration 16, log Z = -8821.3666\n",
      "Iteration 17, log Z = 12311.0859\n",
      "Iteration 18, log Z = -8821.2260\n",
      "Iteration 19, log Z = 12310.9617\n",
      "Iteration 20, log Z = -8821.1010\n",
      "Log P(H_0:9000) via Bridge Sampling = -8821.1010\n",
      "First term: 8826.0186\n",
      "Prior term: 0.5000\n",
      "First term: 8826.0186\n",
      "Prior term: 0.5000\n",
      "Autocorrelation for first dimension (lags 0-4): [1.         0.4453375  0.3917438  0.48693    0.48444504]\n",
      "Sample variance: [3.2604417e-05 2.7713810e-05 3.2634146e-06 4.2198146e-05 3.3800268e-05\n",
      " 2.7503156e-05 4.0948202e-05 3.4232577e-05 3.8239075e-05 3.0958880e-05]\n",
      "Log likelihood at MAP: -8820.9951\n",
      "Average log likelihood from samples: 8826.0186\n",
      "Entropy with 9000 preferences (Harmonic Mean): -2.6664\n",
      "Entropy with 9000 preferences (Bridge Sampling): 5.4176\n",
      "\n",
      "Processing preferences 0:10000\n",
      "MCMC Step 0, Acceptance Rate: 0.0000\n",
      "MCMC Step 5000, Acceptance Rate: 0.0618\n",
      "MCMC Step 10000, Acceptance Rate: 0.0502\n",
      "MCMC Step 15000, Acceptance Rate: 0.0440\n",
      "MCMC Step 20000, Acceptance Rate: 0.0404\n",
      "MCMC Step 25000, Acceptance Rate: 0.0412\n",
      "MCMC Step 30000, Acceptance Rate: 0.0399\n",
      "MCMC Step 35000, Acceptance Rate: 0.0393\n",
      "MCMC Step 40000, Acceptance Rate: 0.0388\n",
      "MCMC Step 45000, Acceptance Rate: 0.0383\n",
      "MCMC Step 50000, Acceptance Rate: 0.0382\n",
      "MCMC Step 55000, Acceptance Rate: 0.0387\n",
      "Final Acceptance Rate: 0.0383\n",
      "MAP Log Posterior: -9840.478515625\n",
      "Log P(H_0:10000) via Harmonic Mean = -9852.0764\n",
      "Initial log Z (harmonic mean): -9852.0764\n",
      "Iteration 1, log Z = 13850.8536\n",
      "Iteration 2, log Z = -9843.2614\n",
      "Iteration 3, log Z = 13849.7107\n",
      "Iteration 4, log Z = -9842.1188\n",
      "Iteration 5, log Z = 13849.1080\n",
      "Iteration 6, log Z = -9841.5153\n",
      "Iteration 7, log Z = 13848.7072\n",
      "Iteration 8, log Z = -9841.1149\n",
      "Iteration 9, log Z = 13848.4108\n",
      "Iteration 10, log Z = -9840.8180\n",
      "Iteration 11, log Z = 13848.1765\n",
      "Iteration 12, log Z = -9840.5836\n",
      "Iteration 13, log Z = 13847.9837\n",
      "Iteration 14, log Z = -9840.3903\n",
      "Iteration 15, log Z = 13847.8198\n",
      "Iteration 16, log Z = -9840.2262\n",
      "Iteration 17, log Z = 13847.6776\n",
      "Iteration 18, log Z = -9840.0856\n",
      "Iteration 19, log Z = 13847.5537\n",
      "Iteration 20, log Z = -9839.9606\n",
      "Log P(H_0:10000) via Bridge Sampling = -9839.9606\n",
      "First term: 9844.9102\n",
      "Prior term: 0.5000\n",
      "First term: 9844.9102\n",
      "Prior term: 0.5000\n",
      "Autocorrelation for first dimension (lags 0-4): [1.         0.22818425 0.12009925 0.13725495 0.04084986]\n",
      "Sample variance: [2.8832021e-05 2.3680061e-05 2.7127878e-06 3.2159322e-05 2.9670719e-05\n",
      " 2.3516537e-05 3.6561676e-05 2.7497023e-05 3.3833378e-05 3.6084191e-05]\n",
      "Log likelihood at MAP: -9839.9785\n",
      "Average log likelihood from samples: 9844.9102\n",
      "Entropy with 10000 preferences (Harmonic Mean): -6.6662\n",
      "Entropy with 10000 preferences (Bridge Sampling): 5.4496\n",
      "\n",
      "Summary of Entropies:\n",
      "Preferences 0:1000: Entropy (Harmonic Mean) = -5.4953, (Bridge Sampling) = 5.3221\n",
      "Preferences 0:2000: Entropy (Harmonic Mean) = -4.9069, (Bridge Sampling) = 5.2559\n",
      "Preferences 0:3000: Entropy (Harmonic Mean) = -3.1254, (Bridge Sampling) = 5.4525\n",
      "Preferences 0:4000: Entropy (Harmonic Mean) = -2.8909, (Bridge Sampling) = 5.1986\n",
      "Preferences 0:5000: Entropy (Harmonic Mean) = -3.1263, (Bridge Sampling) = 5.7349\n",
      "Preferences 0:6000: Entropy (Harmonic Mean) = -4.8671, (Bridge Sampling) = 6.1143\n",
      "Preferences 0:7000: Entropy (Harmonic Mean) = -3.3888, (Bridge Sampling) = 5.9202\n",
      "Preferences 0:8000: Entropy (Harmonic Mean) = -3.5423, (Bridge Sampling) = 5.8764\n",
      "Preferences 0:9000: Entropy (Harmonic Mean) = -2.6664, (Bridge Sampling) = 5.4176\n",
      "Preferences 0:10000: Entropy (Harmonic Mean) = -6.6662, (Bridge Sampling) = 5.4496\n",
      "Results saved: entropies_hm.npy, entropies_bridge.npy, posterior_samples_*.npy, map_solution_*.npy\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "import pickle\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def encode_preferences(encoder, preferences, segments):\n",
    "    for param in encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    segment_states = [\n",
    "        torch.tensor(\n",
    "            np.array([state_t[0, 0, :] for state_t, _, _, done_t in segment if not done_t]),\n",
    "            dtype=torch.float32\n",
    "        ).to(device) if any(not done_t for _, _, _, done_t in segment) else None\n",
    "        for segment in segments\n",
    "    ]\n",
    "    Phi_tau_list = []\n",
    "    for seg_states in segment_states:\n",
    "        if seg_states is not None:\n",
    "            with torch.no_grad():\n",
    "                phi, _, _ = encoder(seg_states)\n",
    "            Phi_tau = phi.sum(dim=0)\n",
    "            Phi_tau_list.append(Phi_tau)\n",
    "        else:\n",
    "            Phi_tau_list.append(None)\n",
    "    \n",
    "    valid_pairs = [(i, j) for i, j, _ in preferences if Phi_tau_list[i] is not None and Phi_tau_list[j] is not None]\n",
    "    if not valid_pairs:\n",
    "        raise ValueError(\"No valid preference pairs found.\")\n",
    "    \n",
    "    Phi_tau_i = torch.stack([Phi_tau_list[i] for i, j in valid_pairs])\n",
    "    Phi_tau_j = torch.stack([Phi_tau_list[j] for i, j in valid_pairs])\n",
    "    return Phi_tau_i, Phi_tau_j\n",
    "\n",
    "def log_likelihood_vectorized(w_batch, Phi_tau_i, Phi_tau_j, beta):\n",
    "    w_batch = torch.tensor(w_batch, dtype=torch.float32).to(device)\n",
    "    R_i = torch.matmul(Phi_tau_i, w_batch.T)\n",
    "    R_j = torch.matmul(Phi_tau_j, w_batch.T)\n",
    "    \n",
    "    beta_R_i = beta * R_i\n",
    "    beta_R_j = beta * R_j\n",
    "    max_val = torch.maximum(beta_R_i, beta_R_j)\n",
    "    log_sum_exp = max_val + torch.log(\n",
    "        torch.exp(beta_R_i - max_val) + torch.exp(beta_R_j - max_val)\n",
    "    )\n",
    "    \n",
    "    terms = beta * R_j - log_sum_exp\n",
    "    log_likelihoods = torch.sum(terms, dim=0)\n",
    "    \n",
    "    if torch.any(log_likelihoods > 0):\n",
    "        print(\"Warning: Positive log likelihood detected.\")\n",
    "        print(f\"Sample terms: {terms[:5, :5]}\")\n",
    "        print(f\"Log likelihoods: {log_likelihoods[:5]}\")\n",
    "    \n",
    "    return log_likelihoods.cpu().numpy()\n",
    "\n",
    "def log_prior_vectorized(w_batch):\n",
    "    w_batch = torch.tensor(w_batch, dtype=torch.float32).to(device)\n",
    "    return -0.5 * torch.sum(w_batch ** 2, dim=1).cpu().numpy()\n",
    "\n",
    "def harmonic_mean_marginal_likelihood(posterior_samples, Phi_tau_i, Phi_tau_j, beta):\n",
    "    log_likelihoods = log_likelihood_vectorized(posterior_samples, Phi_tau_i, Phi_tau_j, beta)\n",
    "    log_priors = log_prior_vectorized(posterior_samples)\n",
    "    log_unnorm = log_likelihoods + log_priors\n",
    "    inverse_terms = -log_unnorm\n",
    "    log_inverse_sum = logsumexp(inverse_terms) - np.log(len(posterior_samples))\n",
    "    log_marginal = -log_inverse_sum\n",
    "    return log_marginal\n",
    "\n",
    "def bridge_sampling_marginal_likelihood(posterior_samples, prior_samples, Phi_tau_i, Phi_tau_j, beta, max_iter=20, tol=1e-3):\n",
    "    # Compute unnormalized posterior for posterior samples\n",
    "    post_log_likelihoods = log_likelihood_vectorized(posterior_samples, Phi_tau_i, Phi_tau_j, beta)\n",
    "    post_log_priors = log_prior_vectorized(posterior_samples)\n",
    "    post_log_unnorm = post_log_likelihoods + post_log_priors\n",
    "\n",
    "    # Compute unnormalized posterior for prior samples\n",
    "    prior_log_likelihoods = log_likelihood_vectorized(prior_samples, Phi_tau_i, Phi_tau_j, beta)\n",
    "    prior_log_priors = log_prior_vectorized(prior_samples)\n",
    "    prior_log_unnorm = prior_log_likelihoods + prior_log_priors\n",
    "\n",
    "    # Initial guess using harmonic mean for better starting point\n",
    "    log_Z = harmonic_mean_marginal_likelihood(posterior_samples, Phi_tau_i, Phi_tau_j, beta)\n",
    "    print(f\"Initial log Z (harmonic mean): {log_Z:.4f}\")\n",
    "\n",
    "    # Iterative bridge sampling\n",
    "    m_post = len(posterior_samples)\n",
    "    m_prior = len(prior_samples)\n",
    "    alpha = 0.5\n",
    "\n",
    "    for iteration in range(max_iter):\n",
    "        # Compute bridge function h(w) for posterior samples\n",
    "        log_h_post = np.log(alpha / m_prior + (1 - alpha) / m_post) + logsumexp([\n",
    "            np.log(alpha) + post_log_priors,\n",
    "            np.log(1 - alpha) + post_log_unnorm - log_Z\n",
    "        ], axis=0)\n",
    "\n",
    "        # Compute bridge function h(w) for prior samples\n",
    "        log_h_prior = np.log(alpha / m_prior + (1 - alpha) / m_post) + logsumexp([\n",
    "            np.log(alpha) + prior_log_priors,\n",
    "            np.log(1 - alpha) + prior_log_unnorm - log_Z\n",
    "        ], axis=0)\n",
    "\n",
    "        # Numerator: E_h [ P(w | D, P) / h(w) ]\n",
    "        num_terms = (post_log_unnorm - log_Z) - log_h_post\n",
    "        numerator = logsumexp(num_terms) - np.log(m_post)\n",
    "\n",
    "        # Denominator: E_h [ P(D, P | w) P(w) / h(w) ]\n",
    "        denom_terms = prior_log_unnorm - log_h_prior\n",
    "        denominator = logsumexp(denom_terms) - np.log(m_prior)\n",
    "\n",
    "        # Update log Z\n",
    "        new_log_Z = numerator - denominator\n",
    "        if abs(new_log_Z - log_Z) < tol:\n",
    "            break\n",
    "        log_Z = new_log_Z\n",
    "        print(f\"Iteration {iteration+1}, log Z = {log_Z:.4f}\")\n",
    "\n",
    "    if log_Z > 0:\n",
    "        print(\"Warning: log P(D, P) is positive, which is incorrect. Check likelihood and prior computations.\")\n",
    "    return log_Z\n",
    "\n",
    "def compute_entropy_importance_sampling(Phi_tau_i, Phi_tau_j, samples, beta, log_P_H):\n",
    "    log_probs = log_likelihood_vectorized(samples, Phi_tau_i, Phi_tau_j, beta)\n",
    "    if np.any(np.isnan(log_probs)) or np.any(np.isinf(log_probs)):\n",
    "        raise ValueError(\"Log probabilities contain NaN or Inf values.\")\n",
    "    \n",
    "    log_prior_probs = log_prior_vectorized(samples)\n",
    "    first_term = -np.mean(log_probs)\n",
    "    prior_term = -np.mean(log_prior_probs)\n",
    "    print(f\"First term: {first_term:.4f}\")\n",
    "    print(f\"Prior term: {prior_term:.4f}\")\n",
    "    \n",
    "    entropy = first_term + prior_term + log_P_H\n",
    "    return entropy, log_probs\n",
    "\n",
    "def bayesian_rex_mcmc(trex_model, Phi_tau_i, Phi_tau_j, num_samples=10000, burn_in=1000, beta=1.0, proposal_std=0.005):\n",
    "    w_current = trex_model.model.weight.data.clone().squeeze().to(device)\n",
    "    w_current = w_current / torch.norm(w_current, p=2)\n",
    "    \n",
    "    def log_prior(w):\n",
    "        return -0.5 * torch.sum(w ** 2)\n",
    "    \n",
    "    def log_likelihood(w, Phi_tau_i, Phi_tau_j, beta):\n",
    "        R_i = torch.matmul(Phi_tau_i, w)\n",
    "        R_j = torch.matmul(Phi_tau_j, w)\n",
    "        terms = beta * R_j - torch.log(torch.exp(beta * R_i) + torch.exp(beta * R_j))\n",
    "        return torch.sum(terms)\n",
    "    \n",
    "    samples = []\n",
    "    accepted = 0\n",
    "    w_map = w_current.clone()\n",
    "    log_posterior_map = log_prior(w_current) + log_likelihood(w_current, Phi_tau_i, Phi_tau_j, beta)\n",
    "    \n",
    "    for step in range(num_samples + burn_in):\n",
    "        w_proposed = w_current + torch.normal(mean=0, std=proposal_std, size=w_current.shape).to(device)\n",
    "        w_proposed = w_proposed / torch.norm(w_proposed, p=2)\n",
    "        \n",
    "        log_posterior_current = log_prior(w_current) + log_likelihood(w_current, Phi_tau_i, Phi_tau_j, beta)\n",
    "        log_posterior_proposed = log_prior(w_proposed) + log_likelihood(w_proposed, Phi_tau_i, Phi_tau_j, beta)\n",
    "        \n",
    "        if log_posterior_proposed > log_posterior_map:\n",
    "            w_map = w_proposed.clone()\n",
    "            log_posterior_map = log_posterior_proposed\n",
    "        \n",
    "        log_alpha = log_posterior_proposed - log_posterior_current\n",
    "        alpha = torch.exp(log_alpha)\n",
    "        \n",
    "        if torch.rand(1, device=device) < alpha:\n",
    "            w_current = w_proposed\n",
    "            accepted += 1\n",
    "            if step >= burn_in:\n",
    "                samples.append(w_current.cpu().detach().numpy())\n",
    "        \n",
    "        if step % 5000 == 0:\n",
    "            acceptance_rate = accepted / (step + 1) if step > 0 else 0\n",
    "            print(f\"MCMC Step {step}, Acceptance Rate: {acceptance_rate:.4f}\")\n",
    "    \n",
    "    thinning = 20\n",
    "    samples = samples[::thinning]\n",
    "\n",
    "    print(f\"Final Acceptance Rate: {accepted / (num_samples + burn_in):.4f}\")\n",
    "    print(f\"MAP Log Posterior: {log_posterior_map.item()}\")\n",
    "    \n",
    "    samples = np.array(samples)\n",
    "    return samples, w_map\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    feedback_path = 'ppo_merge-v0_1377.pkl'\n",
    "    with open(feedback_path, 'rb') as file:\n",
    "        feedback_data = pickle.load(file)\n",
    "    \n",
    "    segments = feedback_data['segments']\n",
    "    preferences = feedback_data['preferences']\n",
    "    preferences = np.random.permutation(preferences).tolist()\n",
    "    print(\"Preferences have been shuffled.\")\n",
    "    \n",
    "    state_dim = 5\n",
    "    feature_dim = 10\n",
    "    encoder = FeatureEncoder(input_dim=state_dim, feature_dim=feature_dim)\n",
    "    trex_model = TREXRewardPredictor(feature_dim=feature_dim)\n",
    "    \n",
    "    encoder.load_state_dict(torch.load(\"feature_encoder_pretrain.pth\", map_location=device))\n",
    "    trex_model.load_state_dict(torch.load(\"trex_model_finetune.pth\", map_location=device))\n",
    "    \n",
    "    encoder = encoder.to(device)\n",
    "    trex_model = trex_model.to(device)\n",
    "    encoder.eval()\n",
    "    trex_model.eval()\n",
    "\n",
    "    print(\"Models loaded from 'feature_encoder_pretrain.pth' and 'trex_model_finetune.pth'\")\n",
    "\n",
    "    n_prior_samples = 20000\n",
    "    prior_samples = np.random.normal(0, 1, (n_prior_samples, feature_dim))\n",
    "    prior_samples = prior_samples / np.linalg.norm(prior_samples, axis=1, keepdims=True)\n",
    "    \n",
    "    preference_sizes = range(1000, len(preferences) + 1000, 1000)\n",
    "    entropies_hm = []\n",
    "    entropies_bridge = []\n",
    "    all_posterior_samples = []\n",
    "    all_map_solutions = []\n",
    "    beta = 1.0\n",
    "    \n",
    "    print(\"Running Bayesian REX MCMC with harmonic mean and corrected bridge sampling for marginal likelihood...\")\n",
    "    for pref_size in preference_sizes:\n",
    "        print(f\"\\nProcessing preferences 0:{pref_size}\")\n",
    "        \n",
    "        Phi_tau_i_full, Phi_tau_j_full = encode_preferences(encoder, preferences[:pref_size], segments)\n",
    "        \n",
    "        proposal_std = 0.008\n",
    "        burn_in = 10000\n",
    "        \n",
    "        posterior_samples, map_solution = bayesian_rex_mcmc(\n",
    "            trex_model,\n",
    "            Phi_tau_i_full,\n",
    "            Phi_tau_j_full,\n",
    "            num_samples=50000,\n",
    "            burn_in=burn_in,\n",
    "            beta=beta,\n",
    "            proposal_std=proposal_std\n",
    "        )\n",
    "        \n",
    "        # Estimate marginal likelihood using harmonic mean\n",
    "        log_P_H_hm = harmonic_mean_marginal_likelihood(\n",
    "            posterior_samples, Phi_tau_i_full, Phi_tau_j_full, beta\n",
    "        )\n",
    "        print(f\"Log P(H_0:{pref_size}) via Harmonic Mean = {log_P_H_hm:.4f}\")\n",
    "        \n",
    "        # Estimate marginal likelihood using corrected bridge sampling\n",
    "        log_P_H_bridge = bridge_sampling_marginal_likelihood(\n",
    "            posterior_samples, prior_samples, Phi_tau_i_full, Phi_tau_j_full, beta\n",
    "        )\n",
    "        print(f\"Log P(H_0:{pref_size}) via Bridge Sampling = {log_P_H_bridge:.4f}\")\n",
    "        \n",
    "        # Compute entropy using both estimates\n",
    "        entropy_hm, log_probs = compute_entropy_importance_sampling(\n",
    "            Phi_tau_i_full, Phi_tau_j_full, posterior_samples, beta, log_P_H_hm\n",
    "        )\n",
    "        entropy_bridge, _ = compute_entropy_importance_sampling(\n",
    "            Phi_tau_i_full, Phi_tau_j_full, posterior_samples, beta, log_P_H_bridge\n",
    "        )\n",
    "        \n",
    "        from numpy import correlate\n",
    "        if len(posterior_samples) > 1:\n",
    "            autocorr = correlate(posterior_samples[:, 0], posterior_samples[:, 0], mode='full')\n",
    "            autocorr = autocorr[len(autocorr)//2:len(autocorr)//2+5] / autocorr[len(autocorr)//2]\n",
    "            print(f\"Autocorrelation for first dimension (lags 0-4): {autocorr}\")\n",
    "        else:\n",
    "            print(\"Not enough samples to compute autocorrelation.\")\n",
    "        \n",
    "        sample_variance = np.var(posterior_samples, axis=0)\n",
    "        print(f\"Sample variance: {sample_variance}\")\n",
    "        \n",
    "        log_likelihood_map = log_likelihood_vectorized(map_solution.reshape(1, -1), Phi_tau_i_full, Phi_tau_j_full, beta)\n",
    "        print(f\"Log likelihood at MAP: {log_likelihood_map[0]:.4f}\")\n",
    "        print(f\"Average log likelihood from samples: {-np.mean(log_probs):.4f}\")\n",
    "        \n",
    "        entropies_hm.append(entropy_hm)\n",
    "        entropies_bridge.append(entropy_bridge)\n",
    "        all_posterior_samples.append(posterior_samples)\n",
    "        all_map_solutions.append(map_solution)\n",
    "        \n",
    "        print(f\"Entropy with {pref_size} preferences (Harmonic Mean): {entropy_hm:.4f}\")\n",
    "        print(f\"Entropy with {pref_size} preferences (Bridge Sampling): {entropy_bridge:.4f}\")\n",
    "    \n",
    "    np.save(\"entropies_hm.npy\", np.array(entropies_hm))\n",
    "    np.save(\"entropies_bridge.npy\", np.array(entropies_bridge))\n",
    "    for i, pref_size in enumerate(preference_sizes):\n",
    "        np.save(f\"posterior_samples_{pref_size}.npy\", all_posterior_samples[i])\n",
    "        map_solution = all_map_solutions[i]\n",
    "        if isinstance(map_solution, torch.Tensor):\n",
    "            map_solution = map_solution.cpu().detach().numpy()\n",
    "        np.save(f\"map_solution_{pref_size}.npy\", map_solution)\n",
    "\n",
    "    print(\"\\nSummary of Entropies:\")\n",
    "    for pref_size, entropy_hm, entropy_bridge in zip(preference_sizes, entropies_hm, entropies_bridge):\n",
    "        print(f\"Preferences 0:{pref_size}: Entropy (Harmonic Mean) = {entropy_hm:.4f}, (Bridge Sampling) = {entropy_bridge:.4f}\")\n",
    "\n",
    "    print(\"Results saved: entropies_hm.npy, entropies_bridge.npy, posterior_samples_*.npy, map_solution_*.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9c15ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preferences have been shuffled.\n",
      "Models loaded from 'feature_encoder_pretrain.pth' and 'trex_model_finetune.pth'\n",
      "Running Bayesian REX MCMC with harmonic mean and corrected bridge sampling for marginal likelihood...\n",
      "\n",
      "Processing preferences 0:1000\n",
      "MCMC Step 5000, Acceptance Rate: 0.4476, Proposal Std: 0.0072\n",
      "MCMC Step 10000, Acceptance Rate: 0.4952, Proposal Std: 0.0065\n",
      "MCMC Step 15000, Acceptance Rate: 0.5416, Proposal Std: 0.0058\n",
      "MCMC Step 20000, Acceptance Rate: 0.5940, Proposal Std: 0.0052\n",
      "MCMC Step 25000, Acceptance Rate: 0.6272, Proposal Std: 0.0047\n",
      "MCMC Step 30000, Acceptance Rate: 0.6510, Proposal Std: 0.0043\n",
      "MCMC Step 35000, Acceptance Rate: 0.6928, Proposal Std: 0.0038\n",
      "MCMC Step 40000, Acceptance Rate: 0.7076, Proposal Std: 0.0034\n",
      "MCMC Step 45000, Acceptance Rate: 0.7414, Proposal Std: 0.0031\n",
      "MCMC Step 50000, Acceptance Rate: 0.7752, Proposal Std: 0.0028\n",
      "MCMC Step 55000, Acceptance Rate: 0.7838, Proposal Std: 0.0025\n",
      "MCMC Step 60000, Acceptance Rate: 0.8314, Proposal Std: 0.0023\n",
      "MCMC Step 65000, Acceptance Rate: 0.8302, Proposal Std: 0.0020\n",
      "MCMC Step 70000, Acceptance Rate: 0.8470, Proposal Std: 0.0018\n",
      "MCMC Step 75000, Acceptance Rate: 0.8626, Proposal Std: 0.0016\n",
      "MCMC Step 80000, Acceptance Rate: 0.8772, Proposal Std: 0.0015\n",
      "MCMC Step 85000, Acceptance Rate: 0.8950, Proposal Std: 0.0013\n",
      "MCMC Step 90000, Acceptance Rate: 0.8982, Proposal Std: 0.0012\n",
      "MCMC Step 95000, Acceptance Rate: 0.9060, Proposal Std: 0.0011\n",
      "MCMC Step 100000, Acceptance Rate: 0.9136, Proposal Std: 0.0010\n",
      "MCMC Step 105000, Acceptance Rate: 0.9328, Proposal Std: 0.0009\n",
      "MCMC Step 110000, Acceptance Rate: 0.9294, Proposal Std: 0.0008\n",
      "MCMC Step 115000, Acceptance Rate: 0.9428, Proposal Std: 0.0007\n",
      "MCMC Step 120000, Acceptance Rate: 0.9484, Proposal Std: 0.0006\n",
      "Final Acceptance Rate: 0.7780\n",
      "MAP Log Posterior: -949.11767578125\n",
      "Log P(H_0:1000) via Harmonic Mean = -958.3387\n",
      "Initial log Z (harmonic mean): -958.3387\n",
      "Iteration 1, log Z = 1239.9613\n",
      "Iteration 2, log Z = -946.2099\n",
      "Iteration 3, log Z = 1238.8058\n",
      "Iteration 4, log Z = -945.0544\n",
      "Iteration 5, log Z = 1238.1906\n",
      "Iteration 6, log Z = -944.4394\n",
      "Iteration 7, log Z = 1237.7807\n",
      "Iteration 8, log Z = -944.0293\n",
      "Iteration 9, log Z = 1237.4765\n",
      "Iteration 10, log Z = -943.7251\n",
      "Iteration 11, log Z = 1237.2362\n",
      "Iteration 12, log Z = -943.4848\n",
      "Iteration 13, log Z = 1237.0384\n",
      "Iteration 14, log Z = -943.2871\n",
      "Iteration 15, log Z = 1236.8707\n",
      "Iteration 16, log Z = -943.1194\n",
      "Iteration 17, log Z = 1236.7254\n",
      "Iteration 18, log Z = -942.9741\n",
      "Iteration 19, log Z = 1236.5973\n",
      "Iteration 20, log Z = -942.8459\n",
      "Iteration 21, log Z = 1236.4829\n",
      "Iteration 22, log Z = -942.7317\n",
      "Iteration 23, log Z = 1236.3797\n",
      "Iteration 24, log Z = -942.6284\n",
      "Iteration 25, log Z = 1236.2856\n",
      "Iteration 26, log Z = -942.5344\n",
      "Iteration 27, log Z = 1236.1993\n",
      "Iteration 28, log Z = -942.4480\n",
      "Iteration 29, log Z = 1236.1194\n",
      "Iteration 30, log Z = -942.3681\n",
      "Iteration 31, log Z = 1236.0453\n",
      "Iteration 32, log Z = -942.2939\n",
      "Iteration 33, log Z = 1235.9760\n",
      "Iteration 34, log Z = -942.2246\n",
      "Iteration 35, log Z = 1235.9110\n",
      "Iteration 36, log Z = -942.1596\n",
      "Iteration 37, log Z = 1235.8498\n",
      "Iteration 38, log Z = -942.0986\n",
      "Iteration 39, log Z = 1235.7922\n",
      "Iteration 40, log Z = -942.0410\n",
      "Iteration 41, log Z = 1235.7376\n",
      "Iteration 42, log Z = -941.9863\n",
      "Iteration 43, log Z = 1235.6857\n",
      "Iteration 44, log Z = -941.9343\n",
      "Iteration 45, log Z = 1235.6362\n",
      "Iteration 46, log Z = -941.8850\n",
      "Iteration 47, log Z = 1235.5891\n",
      "Iteration 48, log Z = -941.8379\n",
      "Iteration 49, log Z = 1235.5441\n",
      "Iteration 50, log Z = -941.7927\n",
      "Log P(H_0:1000) via Bridge Sampling = -941.7927\n",
      "First term: 950.2143\n",
      "Prior term: 3.2387\n",
      "First term: 950.2143\n",
      "Prior term: 3.2387\n",
      "Autocorrelation for first dimension (lags 0-4): [1.         0.72995234 0.6240148  0.545089   0.52078867]\n",
      "Sample variance: [2.0184851e-04 1.5110591e-04 1.6190572e-05 2.7689742e-04 2.5282643e-04\n",
      " 3.0363357e-04 2.7433265e-04 1.9816709e-04 2.8355161e-04 2.4761484e-04]\n",
      "Log likelihood at MAP: -945.8789\n",
      "Average log likelihood from samples: 950.2143\n",
      "Entropy with 1000 preferences (Harmonic Mean): -4.8856\n",
      "Entropy with 1000 preferences (Bridge Sampling): 11.6604\n",
      "\n",
      "Processing preferences 0:2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1267509/109637479.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_batch = torch.tensor(w_batch, dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCMC Step 5000, Acceptance Rate: 0.3024, Proposal Std: 0.0080\n",
      "MCMC Step 10000, Acceptance Rate: 0.2836, Proposal Std: 0.0080\n",
      "MCMC Step 15000, Acceptance Rate: 0.2938, Proposal Std: 0.0080\n",
      "MCMC Step 20000, Acceptance Rate: 0.2830, Proposal Std: 0.0080\n",
      "MCMC Step 25000, Acceptance Rate: 0.2918, Proposal Std: 0.0080\n",
      "MCMC Step 30000, Acceptance Rate: 0.3008, Proposal Std: 0.0080\n",
      "MCMC Step 35000, Acceptance Rate: 0.3100, Proposal Std: 0.0080\n",
      "MCMC Step 40000, Acceptance Rate: 0.2938, Proposal Std: 0.0080\n",
      "MCMC Step 45000, Acceptance Rate: 0.2904, Proposal Std: 0.0080\n",
      "MCMC Step 50000, Acceptance Rate: 0.3002, Proposal Std: 0.0080\n",
      "MCMC Step 55000, Acceptance Rate: 0.2976, Proposal Std: 0.0080\n",
      "MCMC Step 60000, Acceptance Rate: 0.3078, Proposal Std: 0.0080\n",
      "MCMC Step 65000, Acceptance Rate: 0.2972, Proposal Std: 0.0080\n",
      "MCMC Step 70000, Acceptance Rate: 0.2884, Proposal Std: 0.0080\n",
      "MCMC Step 75000, Acceptance Rate: 0.3008, Proposal Std: 0.0080\n",
      "MCMC Step 80000, Acceptance Rate: 0.3028, Proposal Std: 0.0080\n",
      "MCMC Step 85000, Acceptance Rate: 0.2944, Proposal Std: 0.0080\n",
      "MCMC Step 90000, Acceptance Rate: 0.2878, Proposal Std: 0.0080\n",
      "MCMC Step 95000, Acceptance Rate: 0.2804, Proposal Std: 0.0080\n",
      "MCMC Step 100000, Acceptance Rate: 0.2956, Proposal Std: 0.0080\n",
      "MCMC Step 105000, Acceptance Rate: 0.3014, Proposal Std: 0.0080\n",
      "MCMC Step 110000, Acceptance Rate: 0.2914, Proposal Std: 0.0080\n",
      "MCMC Step 115000, Acceptance Rate: 0.2960, Proposal Std: 0.0080\n",
      "MCMC Step 120000, Acceptance Rate: 0.2986, Proposal Std: 0.0080\n",
      "Final Acceptance Rate: 0.2954\n",
      "MAP Log Posterior: -2026.405029296875\n",
      "Log P(H_0:2000) via Harmonic Mean = -2034.7625\n",
      "Initial log Z (harmonic mean): -2034.7625\n",
      "Iteration 1, log Z = 2548.3598\n",
      "Iteration 2, log Z = -2022.5593\n",
      "Iteration 3, log Z = 2547.2447\n",
      "Iteration 4, log Z = -2021.4441\n",
      "Iteration 5, log Z = 2546.6485\n",
      "Iteration 6, log Z = -2020.8479\n",
      "Iteration 7, log Z = 2546.2499\n",
      "Iteration 8, log Z = -2020.4495\n",
      "Iteration 9, log Z = 2545.9536\n",
      "Iteration 10, log Z = -2020.1531\n",
      "Iteration 11, log Z = 2545.7191\n",
      "Iteration 12, log Z = -2019.9182\n",
      "Iteration 13, log Z = 2545.5254\n",
      "Iteration 14, log Z = -2019.7249\n",
      "Iteration 15, log Z = 2545.3612\n",
      "Iteration 16, log Z = -2019.5603\n",
      "Iteration 17, log Z = 2545.2184\n",
      "Iteration 18, log Z = -2019.4177\n",
      "Iteration 19, log Z = 2545.0926\n",
      "Iteration 20, log Z = -2019.2918\n",
      "Iteration 21, log Z = 2544.9800\n",
      "Iteration 22, log Z = -2019.1795\n",
      "Iteration 23, log Z = 2544.8785\n",
      "Iteration 24, log Z = -2019.0779\n",
      "Iteration 25, log Z = 2544.7859\n",
      "Iteration 26, log Z = -2018.9851\n",
      "Iteration 27, log Z = 2544.7006\n",
      "Iteration 28, log Z = -2018.9002\n",
      "Iteration 29, log Z = 2544.6221\n",
      "Iteration 30, log Z = -2018.8216\n",
      "Iteration 31, log Z = 2544.5490\n",
      "Iteration 32, log Z = -2018.7483\n",
      "Iteration 33, log Z = 2544.4806\n",
      "Iteration 34, log Z = -2018.6799\n",
      "Iteration 35, log Z = 2544.4165\n",
      "Iteration 36, log Z = -2018.6160\n",
      "Iteration 37, log Z = 2544.3563\n",
      "Iteration 38, log Z = -2018.5554\n",
      "Iteration 39, log Z = 2544.2991\n",
      "Iteration 40, log Z = -2018.4983\n",
      "Iteration 41, log Z = 2544.2449\n",
      "Iteration 42, log Z = -2018.4441\n",
      "Iteration 43, log Z = 2544.1934\n",
      "Iteration 44, log Z = -2018.3928\n",
      "Iteration 45, log Z = 2544.1446\n",
      "Iteration 46, log Z = -2018.3440\n",
      "Iteration 47, log Z = 2544.0980\n",
      "Iteration 48, log Z = -2018.2971\n",
      "Iteration 49, log Z = 2544.0532\n",
      "Iteration 50, log Z = -2018.2527\n",
      "Log P(H_0:2000) via Bridge Sampling = -2018.2527\n",
      "First term: 2027.4741\n",
      "Prior term: 3.2387\n",
      "First term: 2027.4741\n",
      "Prior term: 3.2387\n",
      "Autocorrelation for first dimension (lags 0-4): [1.         0.72883844 0.74324715 0.73459554 0.7470475 ]\n",
      "Sample variance: [1.2458018e-04 9.9179109e-05 9.4429424e-06 1.2755793e-04 1.3340656e-04\n",
      " 1.2488602e-04 1.2654747e-04 1.1563674e-04 1.3124166e-04 1.3222024e-04]\n",
      "Log likelihood at MAP: -2023.1663\n",
      "Average log likelihood from samples: 2027.4741\n",
      "Entropy with 2000 preferences (Harmonic Mean): -4.0496\n",
      "Entropy with 2000 preferences (Bridge Sampling): 12.4602\n",
      "\n",
      "Processing preferences 0:3000\n",
      "MCMC Step 5000, Acceptance Rate: 0.2358, Proposal Std: 0.0088\n",
      "MCMC Step 10000, Acceptance Rate: 0.1846, Proposal Std: 0.0097\n",
      "MCMC Step 15000, Acceptance Rate: 0.1464, Proposal Std: 0.0106\n",
      "MCMC Step 20000, Acceptance Rate: 0.1002, Proposal Std: 0.0117\n",
      "MCMC Step 25000, Acceptance Rate: 0.0798, Proposal Std: 0.0129\n",
      "MCMC Step 30000, Acceptance Rate: 0.0614, Proposal Std: 0.0142\n",
      "MCMC Step 35000, Acceptance Rate: 0.0502, Proposal Std: 0.0156\n",
      "MCMC Step 40000, Acceptance Rate: 0.0338, Proposal Std: 0.0171\n",
      "MCMC Step 45000, Acceptance Rate: 0.0182, Proposal Std: 0.0189\n",
      "MCMC Step 50000, Acceptance Rate: 0.0018, Proposal Std: 0.0207\n",
      "MCMC Step 55000, Acceptance Rate: 0.0092, Proposal Std: 0.0228\n",
      "MCMC Step 60000, Acceptance Rate: 0.0044, Proposal Std: 0.0251\n",
      "MCMC Step 65000, Acceptance Rate: 0.0064, Proposal Std: 0.0276\n",
      "MCMC Step 70000, Acceptance Rate: 0.0000, Proposal Std: 0.0304\n",
      "MCMC Step 75000, Acceptance Rate: 0.0004, Proposal Std: 0.0334\n",
      "MCMC Step 80000, Acceptance Rate: 0.0006, Proposal Std: 0.0368\n",
      "MCMC Step 85000, Acceptance Rate: 0.0004, Proposal Std: 0.0404\n",
      "MCMC Step 90000, Acceptance Rate: 0.0000, Proposal Std: 0.0445\n",
      "MCMC Step 95000, Acceptance Rate: 0.0000, Proposal Std: 0.0489\n",
      "MCMC Step 100000, Acceptance Rate: 0.0000, Proposal Std: 0.0538\n",
      "MCMC Step 105000, Acceptance Rate: 0.0000, Proposal Std: 0.0592\n",
      "MCMC Step 110000, Acceptance Rate: 0.0000, Proposal Std: 0.0651\n",
      "MCMC Step 115000, Acceptance Rate: 0.0000, Proposal Std: 0.0716\n",
      "MCMC Step 120000, Acceptance Rate: 0.0000, Proposal Std: 0.0788\n",
      "Final Acceptance Rate: 0.0389\n",
      "MAP Log Posterior: -3063.53515625\n",
      "Log P(H_0:3000) via Harmonic Mean = -3071.0862\n",
      "Initial log Z (harmonic mean): -3071.0862\n",
      "Iteration 1, log Z = 3765.7729\n",
      "Iteration 2, log Z = -3057.5271\n",
      "Iteration 3, log Z = 3764.4691\n",
      "Iteration 4, log Z = -3056.2229\n",
      "Iteration 5, log Z = 3763.8229\n",
      "Iteration 6, log Z = -3055.5769\n",
      "Iteration 7, log Z = 3763.3993\n",
      "Iteration 8, log Z = -3055.1536\n",
      "Iteration 9, log Z = 3763.0874\n",
      "Iteration 10, log Z = -3054.8416\n",
      "Iteration 11, log Z = 3762.8420\n",
      "Iteration 12, log Z = -3054.5960\n",
      "Iteration 13, log Z = 3762.6404\n",
      "Iteration 14, log Z = -3054.3943\n",
      "Iteration 15, log Z = 3762.4698\n",
      "Iteration 16, log Z = -3054.2239\n",
      "Iteration 17, log Z = 3762.3224\n",
      "Iteration 18, log Z = -3054.0764\n",
      "Iteration 19, log Z = 3762.1926\n",
      "Iteration 20, log Z = -3053.9465\n",
      "Iteration 21, log Z = 3762.0767\n",
      "Iteration 22, log Z = -3053.8308\n",
      "Iteration 23, log Z = 3761.9723\n",
      "Iteration 24, log Z = -3053.7263\n",
      "Iteration 25, log Z = 3761.8772\n",
      "Iteration 26, log Z = -3053.6311\n",
      "Iteration 27, log Z = 3761.7899\n",
      "Iteration 28, log Z = -3053.5437\n",
      "Iteration 29, log Z = 3761.7091\n",
      "Iteration 30, log Z = -3053.4632\n",
      "Iteration 31, log Z = 3761.6343\n",
      "Iteration 32, log Z = -3053.3884\n",
      "Iteration 33, log Z = 3761.5646\n",
      "Iteration 34, log Z = -3053.3186\n",
      "Iteration 35, log Z = 3761.4992\n",
      "Iteration 36, log Z = -3053.2532\n",
      "Iteration 37, log Z = 3761.4376\n",
      "Iteration 38, log Z = -3053.1917\n",
      "Iteration 39, log Z = 3761.3795\n",
      "Iteration 40, log Z = -3053.1336\n",
      "Iteration 41, log Z = 3761.3245\n",
      "Iteration 42, log Z = -3053.0784\n",
      "Iteration 43, log Z = 3761.2721\n",
      "Iteration 44, log Z = -3053.0261\n",
      "Iteration 45, log Z = 3761.2224\n",
      "Iteration 46, log Z = -3052.9763\n",
      "Iteration 47, log Z = 3761.1749\n",
      "Iteration 48, log Z = -3052.9290\n",
      "Iteration 49, log Z = 3761.1296\n",
      "Iteration 50, log Z = -3052.8836\n",
      "Log P(H_0:3000) via Bridge Sampling = -3052.8836\n",
      "First term: 3066.1262\n",
      "Prior term: 3.2387\n",
      "First term: 3066.1262\n",
      "Prior term: 3.2387\n",
      "Autocorrelation for first dimension (lags 0-4): [ 1.          0.2505813   0.04569497 -0.02928341 -0.18026502]\n",
      "Sample variance: [1.00952202e-04 8.99739607e-05 7.53203813e-06 1.23669219e-04\n",
      " 9.05438137e-05 1.10478737e-04 1.14180009e-04 7.06985084e-05\n",
      " 1.21052886e-04 1.24942846e-04]\n",
      "Log likelihood at MAP: -3060.2964\n",
      "Average log likelihood from samples: 3066.1262\n",
      "Entropy with 3000 preferences (Harmonic Mean): -1.7213\n",
      "Entropy with 3000 preferences (Bridge Sampling): 16.4814\n",
      "\n",
      "Processing preferences 0:4000\n",
      "MCMC Step 5000, Acceptance Rate: 0.1600, Proposal Std: 0.0088\n",
      "MCMC Step 10000, Acceptance Rate: 0.1116, Proposal Std: 0.0097\n",
      "MCMC Step 15000, Acceptance Rate: 0.1060, Proposal Std: 0.0106\n",
      "MCMC Step 20000, Acceptance Rate: 0.0624, Proposal Std: 0.0117\n",
      "MCMC Step 25000, Acceptance Rate: 0.0506, Proposal Std: 0.0129\n",
      "MCMC Step 30000, Acceptance Rate: 0.0360, Proposal Std: 0.0142\n",
      "MCMC Step 35000, Acceptance Rate: 0.0272, Proposal Std: 0.0156\n",
      "MCMC Step 40000, Acceptance Rate: 0.0170, Proposal Std: 0.0171\n",
      "MCMC Step 45000, Acceptance Rate: 0.0062, Proposal Std: 0.0189\n",
      "MCMC Step 50000, Acceptance Rate: 0.0084, Proposal Std: 0.0207\n",
      "MCMC Step 55000, Acceptance Rate: 0.0064, Proposal Std: 0.0228\n",
      "MCMC Step 60000, Acceptance Rate: 0.0006, Proposal Std: 0.0251\n",
      "MCMC Step 65000, Acceptance Rate: 0.0000, Proposal Std: 0.0276\n",
      "MCMC Step 70000, Acceptance Rate: 0.0000, Proposal Std: 0.0304\n",
      "MCMC Step 75000, Acceptance Rate: 0.0000, Proposal Std: 0.0334\n",
      "MCMC Step 80000, Acceptance Rate: 0.0000, Proposal Std: 0.0368\n",
      "MCMC Step 85000, Acceptance Rate: 0.0000, Proposal Std: 0.0404\n",
      "MCMC Step 90000, Acceptance Rate: 0.0000, Proposal Std: 0.0445\n",
      "MCMC Step 95000, Acceptance Rate: 0.0000, Proposal Std: 0.0489\n",
      "MCMC Step 100000, Acceptance Rate: 0.0000, Proposal Std: 0.0538\n",
      "MCMC Step 105000, Acceptance Rate: 0.0000, Proposal Std: 0.0592\n",
      "MCMC Step 110000, Acceptance Rate: 0.0000, Proposal Std: 0.0651\n",
      "MCMC Step 115000, Acceptance Rate: 0.0000, Proposal Std: 0.0716\n",
      "MCMC Step 120000, Acceptance Rate: 0.0000, Proposal Std: 0.0788\n",
      "Final Acceptance Rate: 0.0247\n",
      "MAP Log Posterior: -4022.075439453125\n",
      "Log P(H_0:4000) via Harmonic Mean = -4030.3854\n",
      "Initial log Z (harmonic mean): -4030.3854\n",
      "Iteration 1, log Z = 5068.6043\n",
      "Iteration 2, log Z = -4014.9746\n",
      "Iteration 3, log Z = 5067.3106\n",
      "Iteration 4, log Z = -4013.6796\n",
      "Iteration 5, log Z = 5066.6696\n",
      "Iteration 6, log Z = -4013.0390\n",
      "Iteration 7, log Z = 5066.2604\n",
      "Iteration 8, log Z = -4012.6298\n",
      "Iteration 9, log Z = 5065.9633\n",
      "Iteration 10, log Z = -4012.3330\n",
      "Iteration 11, log Z = 5065.7313\n",
      "Iteration 12, log Z = -4012.1015\n",
      "Iteration 13, log Z = 5065.5418\n",
      "Iteration 14, log Z = -4011.9121\n",
      "Iteration 15, log Z = 5065.3817\n",
      "Iteration 16, log Z = -4011.7519\n",
      "Iteration 17, log Z = 5065.2431\n",
      "Iteration 18, log Z = -4011.6132\n",
      "Iteration 19, log Z = 5065.1210\n",
      "Iteration 20, log Z = -4011.4902\n",
      "Iteration 21, log Z = 5065.0111\n",
      "Iteration 22, log Z = -4011.3808\n",
      "Iteration 23, log Z = 5064.9123\n",
      "Iteration 24, log Z = -4011.2812\n",
      "Iteration 25, log Z = 5064.8215\n",
      "Iteration 26, log Z = -4011.1914\n",
      "Iteration 27, log Z = 5064.7390\n",
      "Iteration 28, log Z = -4011.1084\n",
      "Iteration 29, log Z = 5064.6623\n",
      "Iteration 30, log Z = -4011.0312\n",
      "Iteration 31, log Z = 5064.5905\n",
      "Iteration 32, log Z = -4010.9599\n",
      "Iteration 33, log Z = 5064.5239\n",
      "Iteration 34, log Z = -4010.8935\n",
      "Iteration 35, log Z = 5064.4616\n",
      "Iteration 36, log Z = -4010.8310\n",
      "Iteration 37, log Z = 5064.4028\n",
      "Iteration 38, log Z = -4010.7724\n",
      "Iteration 39, log Z = 5064.3474\n",
      "Iteration 40, log Z = -4010.7168\n",
      "Iteration 41, log Z = 5064.2946\n",
      "Iteration 42, log Z = -4010.6640\n",
      "Iteration 43, log Z = 5064.2445\n",
      "Iteration 44, log Z = -4010.6142\n",
      "Iteration 45, log Z = 5064.1971\n",
      "Iteration 46, log Z = -4010.5664\n",
      "Iteration 47, log Z = 5064.1514\n",
      "Iteration 48, log Z = -4010.5214\n",
      "Iteration 49, log Z = 5064.1084\n",
      "Iteration 50, log Z = -4010.4785\n",
      "Log P(H_0:4000) via Bridge Sampling = -4010.4785\n",
      "First term: 4024.1360\n",
      "Prior term: 3.2387\n",
      "First term: 4024.1360\n",
      "Prior term: 3.2387\n",
      "Autocorrelation for first dimension (lags 0-4): [1.         0.78386956 0.60560805 0.53391284 0.3763719 ]\n",
      "Sample variance: [5.3485797e-05 5.7215067e-05 6.7530082e-06 1.0097909e-04 1.0180379e-04\n",
      " 5.8529589e-05 5.9839535e-05 9.6457610e-05 3.9775405e-05 2.4825391e-05]\n",
      "Log likelihood at MAP: -4018.8364\n",
      "Average log likelihood from samples: 4024.1360\n",
      "Entropy with 4000 preferences (Harmonic Mean): -3.0106\n",
      "Entropy with 4000 preferences (Bridge Sampling): 16.8963\n",
      "\n",
      "Processing preferences 0:5000\n",
      "MCMC Step 5000, Acceptance Rate: 0.1338, Proposal Std: 0.0088\n",
      "MCMC Step 10000, Acceptance Rate: 0.1014, Proposal Std: 0.0097\n",
      "MCMC Step 15000, Acceptance Rate: 0.0730, Proposal Std: 0.0106\n",
      "MCMC Step 20000, Acceptance Rate: 0.0540, Proposal Std: 0.0117\n",
      "MCMC Step 25000, Acceptance Rate: 0.0348, Proposal Std: 0.0129\n",
      "MCMC Step 30000, Acceptance Rate: 0.0170, Proposal Std: 0.0142\n",
      "MCMC Step 35000, Acceptance Rate: 0.0156, Proposal Std: 0.0156\n",
      "MCMC Step 40000, Acceptance Rate: 0.0078, Proposal Std: 0.0171\n",
      "MCMC Step 45000, Acceptance Rate: 0.0048, Proposal Std: 0.0189\n",
      "MCMC Step 50000, Acceptance Rate: 0.0050, Proposal Std: 0.0207\n",
      "MCMC Step 55000, Acceptance Rate: 0.0008, Proposal Std: 0.0228\n",
      "MCMC Step 60000, Acceptance Rate: 0.0010, Proposal Std: 0.0251\n",
      "MCMC Step 65000, Acceptance Rate: 0.0002, Proposal Std: 0.0276\n",
      "MCMC Step 70000, Acceptance Rate: 0.0000, Proposal Std: 0.0304\n",
      "MCMC Step 75000, Acceptance Rate: 0.0000, Proposal Std: 0.0334\n",
      "MCMC Step 80000, Acceptance Rate: 0.0000, Proposal Std: 0.0368\n",
      "MCMC Step 85000, Acceptance Rate: 0.0000, Proposal Std: 0.0404\n",
      "MCMC Step 90000, Acceptance Rate: 0.0000, Proposal Std: 0.0445\n",
      "MCMC Step 95000, Acceptance Rate: 0.0000, Proposal Std: 0.0489\n",
      "MCMC Step 100000, Acceptance Rate: 0.0000, Proposal Std: 0.0538\n",
      "MCMC Step 105000, Acceptance Rate: 0.0000, Proposal Std: 0.0592\n",
      "MCMC Step 110000, Acceptance Rate: 0.0000, Proposal Std: 0.0651\n",
      "MCMC Step 115000, Acceptance Rate: 0.0000, Proposal Std: 0.0716\n",
      "MCMC Step 120000, Acceptance Rate: 0.0000, Proposal Std: 0.0788\n",
      "Final Acceptance Rate: 0.0187\n",
      "MAP Log Posterior: -4982.14404296875\n",
      "Log P(H_0:5000) via Harmonic Mean = -4989.5337\n",
      "Initial log Z (harmonic mean): -4989.5337\n",
      "Iteration 1, log Z = 6007.6553\n",
      "Iteration 2, log Z = -4974.4578\n",
      "Iteration 3, log Z = 6006.7544\n",
      "Iteration 4, log Z = -4973.5564\n",
      "Iteration 5, log Z = 6006.2612\n",
      "Iteration 6, log Z = -4973.0633\n",
      "Iteration 7, log Z = 6005.9227\n",
      "Iteration 8, log Z = -4972.7254\n",
      "Iteration 9, log Z = 6005.6662\n",
      "Iteration 10, log Z = -4972.4685\n",
      "Iteration 11, log Z = 6005.4597\n",
      "Iteration 12, log Z = -4972.2615\n",
      "Iteration 13, log Z = 6005.2867\n",
      "Iteration 14, log Z = -4972.0887\n",
      "Iteration 15, log Z = 6005.1385\n",
      "Iteration 16, log Z = -4971.9412\n",
      "Iteration 17, log Z = 6005.0094\n",
      "Iteration 18, log Z = -4971.8113\n",
      "Iteration 19, log Z = 6004.8940\n",
      "Iteration 20, log Z = -4971.6961\n",
      "Iteration 21, log Z = 6004.7903\n",
      "Iteration 22, log Z = -4971.5926\n",
      "Iteration 23, log Z = 6004.6963\n",
      "Iteration 24, log Z = -4971.4988\n",
      "Iteration 25, log Z = 6004.6104\n",
      "Iteration 26, log Z = -4971.4129\n",
      "Iteration 27, log Z = 6004.5311\n",
      "Iteration 28, log Z = -4971.3338\n",
      "Iteration 29, log Z = 6004.4578\n",
      "Iteration 30, log Z = -4971.2596\n",
      "Iteration 31, log Z = 6004.3885\n",
      "Iteration 32, log Z = -4971.1912\n",
      "Iteration 33, log Z = 6004.3245\n",
      "Iteration 34, log Z = -4971.1267\n",
      "Iteration 35, log Z = 6004.2639\n",
      "Iteration 36, log Z = -4971.0662\n",
      "Iteration 37, log Z = 6004.2067\n",
      "Iteration 38, log Z = -4971.0086\n",
      "Iteration 39, log Z = 6004.1522\n",
      "Iteration 40, log Z = -4970.9549\n",
      "Iteration 41, log Z = 6004.1012\n",
      "Iteration 42, log Z = -4970.9031\n",
      "Iteration 43, log Z = 6004.0519\n",
      "Iteration 44, log Z = -4970.8543\n",
      "Iteration 45, log Z = 6004.0053\n",
      "Iteration 46, log Z = -4970.8074\n",
      "Iteration 47, log Z = 6003.9605\n",
      "Iteration 48, log Z = -4970.7625\n",
      "Iteration 49, log Z = 6003.9175\n",
      "Iteration 50, log Z = -4970.7195\n",
      "Log P(H_0:5000) via Bridge Sampling = -4970.7195\n",
      "First term: 4983.2734\n",
      "Prior term: 3.2387\n",
      "First term: 4983.2734\n",
      "Prior term: 3.2387\n",
      "Autocorrelation for first dimension (lags 0-4): [1.         0.82241946 0.68050635 0.58576953 0.4744783 ]\n",
      "Sample variance: [3.6191730e-05 5.8637375e-05 9.8256105e-06 4.0628172e-05 9.5181822e-05\n",
      " 3.8747912e-05 5.9521110e-05 2.6085250e-05 6.2753672e-05 1.1597279e-05]\n",
      "Log likelihood at MAP: -4978.9053\n",
      "Average log likelihood from samples: 4983.2734\n",
      "Entropy with 5000 preferences (Harmonic Mean): -3.0215\n",
      "Entropy with 5000 preferences (Bridge Sampling): 15.7927\n",
      "\n",
      "Processing preferences 0:6000\n",
      "MCMC Step 5000, Acceptance Rate: 0.0990, Proposal Std: 0.0088\n",
      "MCMC Step 10000, Acceptance Rate: 0.0752, Proposal Std: 0.0097\n",
      "MCMC Step 15000, Acceptance Rate: 0.0590, Proposal Std: 0.0106\n",
      "MCMC Step 20000, Acceptance Rate: 0.0366, Proposal Std: 0.0117\n",
      "MCMC Step 25000, Acceptance Rate: 0.0248, Proposal Std: 0.0129\n",
      "MCMC Step 30000, Acceptance Rate: 0.0138, Proposal Std: 0.0142\n",
      "MCMC Step 35000, Acceptance Rate: 0.0122, Proposal Std: 0.0156\n",
      "MCMC Step 40000, Acceptance Rate: 0.0050, Proposal Std: 0.0171\n",
      "MCMC Step 45000, Acceptance Rate: 0.0052, Proposal Std: 0.0189\n",
      "MCMC Step 50000, Acceptance Rate: 0.0026, Proposal Std: 0.0207\n",
      "MCMC Step 55000, Acceptance Rate: 0.0020, Proposal Std: 0.0228\n",
      "MCMC Step 60000, Acceptance Rate: 0.0010, Proposal Std: 0.0251\n",
      "MCMC Step 65000, Acceptance Rate: 0.0016, Proposal Std: 0.0276\n",
      "MCMC Step 70000, Acceptance Rate: 0.0004, Proposal Std: 0.0304\n",
      "MCMC Step 75000, Acceptance Rate: 0.0002, Proposal Std: 0.0334\n",
      "MCMC Step 80000, Acceptance Rate: 0.0000, Proposal Std: 0.0368\n",
      "MCMC Step 85000, Acceptance Rate: 0.0002, Proposal Std: 0.0404\n",
      "MCMC Step 90000, Acceptance Rate: 0.0002, Proposal Std: 0.0445\n",
      "MCMC Step 95000, Acceptance Rate: 0.0000, Proposal Std: 0.0489\n",
      "MCMC Step 100000, Acceptance Rate: 0.0000, Proposal Std: 0.0538\n",
      "MCMC Step 105000, Acceptance Rate: 0.0000, Proposal Std: 0.0592\n",
      "MCMC Step 110000, Acceptance Rate: 0.0000, Proposal Std: 0.0651\n",
      "MCMC Step 115000, Acceptance Rate: 0.0000, Proposal Std: 0.0716\n",
      "MCMC Step 120000, Acceptance Rate: 0.0000, Proposal Std: 0.0788\n",
      "Final Acceptance Rate: 0.0141\n",
      "MAP Log Posterior: -5879.89111328125\n",
      "Log P(H_0:6000) via Harmonic Mean = -5889.6845\n",
      "Initial log Z (harmonic mean): -5889.6845\n",
      "Iteration 1, log Z = 7233.1217\n",
      "Iteration 2, log Z = -5872.9194\n",
      "Iteration 3, log Z = 7231.6225\n",
      "Iteration 4, log Z = -5871.4204\n",
      "Iteration 5, log Z = 7230.9467\n",
      "Iteration 6, log Z = -5870.7446\n",
      "Iteration 7, log Z = 7230.5285\n",
      "Iteration 8, log Z = -5870.3256\n",
      "Iteration 9, log Z = 7230.2281\n",
      "Iteration 10, log Z = -5870.0258\n",
      "Iteration 11, log Z = 7229.9952\n",
      "Iteration 12, log Z = -5869.7924\n",
      "Iteration 13, log Z = 7229.8047\n",
      "Iteration 14, log Z = -5869.6030\n",
      "Iteration 15, log Z = 7229.6448\n",
      "Iteration 16, log Z = -5869.4428\n",
      "Iteration 17, log Z = 7229.5064\n",
      "Iteration 18, log Z = -5869.3041\n",
      "Iteration 19, log Z = 7229.3843\n",
      "Iteration 20, log Z = -5869.1821\n",
      "Iteration 21, log Z = 7229.2753\n",
      "Iteration 22, log Z = -5869.0737\n",
      "Iteration 23, log Z = 7229.1774\n",
      "Iteration 24, log Z = -5868.9750\n",
      "Iteration 25, log Z = 7229.0875\n",
      "Iteration 26, log Z = -5868.8852\n",
      "Iteration 27, log Z = 7229.0049\n",
      "Iteration 28, log Z = -5868.8022\n",
      "Iteration 29, log Z = 7228.9282\n",
      "Iteration 30, log Z = -5868.7260\n",
      "Iteration 31, log Z = 7228.8573\n",
      "Iteration 32, log Z = -5868.6557\n",
      "Iteration 33, log Z = 7228.7916\n",
      "Iteration 34, log Z = -5868.5893\n",
      "Iteration 35, log Z = 7228.7293\n",
      "Iteration 36, log Z = -5868.5268\n",
      "Iteration 37, log Z = 7228.6704\n",
      "Iteration 38, log Z = -5868.4682\n",
      "Iteration 39, log Z = 7228.6151\n",
      "Iteration 40, log Z = -5868.4135\n",
      "Iteration 41, log Z = 7228.5632\n",
      "Iteration 42, log Z = -5868.3608\n",
      "Iteration 43, log Z = 7228.5131\n",
      "Iteration 44, log Z = -5868.3110\n",
      "Iteration 45, log Z = 7228.4656\n",
      "Iteration 46, log Z = -5868.2631\n",
      "Iteration 47, log Z = 7228.4199\n",
      "Iteration 48, log Z = -5868.2182\n",
      "Iteration 49, log Z = 7228.3770\n",
      "Iteration 50, log Z = -5868.1752\n",
      "Log P(H_0:6000) via Bridge Sampling = -5868.1752\n",
      "First term: 5883.8789\n",
      "Prior term: 3.2387\n",
      "First term: 5883.8789\n",
      "Prior term: 3.2387\n",
      "Autocorrelation for first dimension (lags 0-4): [1.         0.68203104 0.5413178  0.39027345 0.44263384]\n",
      "Sample variance: [7.8694153e-05 8.5416577e-05 9.3545232e-06 1.0508824e-04 1.7288939e-05\n",
      " 4.9932911e-05 3.5773613e-05 2.5887688e-05 6.5015207e-05 6.9070622e-05]\n",
      "Log likelihood at MAP: -5876.6523\n",
      "Average log likelihood from samples: 5883.8789\n",
      "Entropy with 6000 preferences (Harmonic Mean): -2.5668\n",
      "Entropy with 6000 preferences (Bridge Sampling): 18.9424\n",
      "\n",
      "Processing preferences 0:7000\n",
      "MCMC Step 5000, Acceptance Rate: 0.1050, Proposal Std: 0.0088\n",
      "MCMC Step 10000, Acceptance Rate: 0.0538, Proposal Std: 0.0097\n",
      "MCMC Step 15000, Acceptance Rate: 0.0392, Proposal Std: 0.0106\n",
      "MCMC Step 20000, Acceptance Rate: 0.0202, Proposal Std: 0.0117\n",
      "MCMC Step 25000, Acceptance Rate: 0.0138, Proposal Std: 0.0129\n",
      "MCMC Step 30000, Acceptance Rate: 0.0088, Proposal Std: 0.0142\n",
      "MCMC Step 35000, Acceptance Rate: 0.0062, Proposal Std: 0.0156\n",
      "MCMC Step 40000, Acceptance Rate: 0.0044, Proposal Std: 0.0171\n",
      "MCMC Step 45000, Acceptance Rate: 0.0002, Proposal Std: 0.0189\n",
      "MCMC Step 50000, Acceptance Rate: 0.0002, Proposal Std: 0.0207\n",
      "MCMC Step 55000, Acceptance Rate: 0.0004, Proposal Std: 0.0228\n",
      "MCMC Step 60000, Acceptance Rate: 0.0000, Proposal Std: 0.0251\n",
      "MCMC Step 65000, Acceptance Rate: 0.0002, Proposal Std: 0.0276\n",
      "MCMC Step 70000, Acceptance Rate: 0.0000, Proposal Std: 0.0304\n",
      "MCMC Step 75000, Acceptance Rate: 0.0000, Proposal Std: 0.0334\n",
      "MCMC Step 80000, Acceptance Rate: 0.0000, Proposal Std: 0.0368\n",
      "MCMC Step 85000, Acceptance Rate: 0.0000, Proposal Std: 0.0404\n",
      "MCMC Step 90000, Acceptance Rate: 0.0000, Proposal Std: 0.0445\n",
      "MCMC Step 95000, Acceptance Rate: 0.0000, Proposal Std: 0.0489\n",
      "MCMC Step 100000, Acceptance Rate: 0.0000, Proposal Std: 0.0538\n",
      "MCMC Step 105000, Acceptance Rate: 0.0000, Proposal Std: 0.0592\n",
      "MCMC Step 110000, Acceptance Rate: 0.0000, Proposal Std: 0.0651\n",
      "MCMC Step 115000, Acceptance Rate: 0.0000, Proposal Std: 0.0716\n",
      "MCMC Step 120000, Acceptance Rate: 0.0000, Proposal Std: 0.0788\n",
      "Final Acceptance Rate: 0.0105\n",
      "MAP Log Posterior: -6734.02197265625\n",
      "Log P(H_0:7000) via Harmonic Mean = -6738.8935\n",
      "Initial log Z (harmonic mean): -6738.8935\n",
      "Iteration 1, log Z = 8545.3368\n",
      "Iteration 2, log Z = -6725.3129\n",
      "Iteration 3, log Z = 8544.1346\n",
      "Iteration 4, log Z = -6724.1107\n",
      "Iteration 5, log Z = 8543.5306\n",
      "Iteration 6, log Z = -6723.5062\n",
      "Iteration 7, log Z = 8543.1425\n",
      "Iteration 8, log Z = -6723.1185\n",
      "Iteration 9, log Z = 8542.8594\n",
      "Iteration 10, log Z = -6722.8353\n",
      "Iteration 11, log Z = 8542.6370\n",
      "Iteration 12, log Z = -6722.6127\n",
      "Iteration 13, log Z = 8542.4539\n",
      "Iteration 14, log Z = -6722.4301\n",
      "Iteration 15, log Z = 8542.2990\n",
      "Iteration 16, log Z = -6722.2748\n",
      "Iteration 17, log Z = 8542.1642\n",
      "Iteration 18, log Z = -6722.1400\n",
      "Iteration 19, log Z = 8542.0452\n",
      "Iteration 20, log Z = -6722.0209\n",
      "Iteration 21, log Z = 8541.9386\n",
      "Iteration 22, log Z = -6721.9144\n",
      "Iteration 23, log Z = 8541.8423\n",
      "Iteration 24, log Z = -6721.8178\n",
      "Iteration 25, log Z = 8541.7540\n",
      "Iteration 26, log Z = -6721.7299\n",
      "Iteration 27, log Z = 8541.6731\n",
      "Iteration 28, log Z = -6721.6488\n",
      "Iteration 29, log Z = 8541.5981\n",
      "Iteration 30, log Z = -6721.5736\n",
      "Iteration 31, log Z = 8541.5281\n",
      "Iteration 32, log Z = -6721.5043\n",
      "Iteration 33, log Z = 8541.4632\n",
      "Iteration 34, log Z = -6721.4389\n",
      "Iteration 35, log Z = 8541.4018\n",
      "Iteration 36, log Z = -6721.3773\n",
      "Iteration 37, log Z = 8541.3438\n",
      "Iteration 38, log Z = -6721.3197\n",
      "Iteration 39, log Z = 8541.2893\n",
      "Iteration 40, log Z = -6721.2650\n",
      "Iteration 41, log Z = 8541.2374\n",
      "Iteration 42, log Z = -6721.2133\n",
      "Iteration 43, log Z = 8541.1882\n",
      "Iteration 44, log Z = -6721.1644\n",
      "Iteration 45, log Z = 8541.1416\n",
      "Iteration 46, log Z = -6721.1176\n",
      "Iteration 47, log Z = 8541.0968\n",
      "Iteration 48, log Z = -6721.0727\n",
      "Iteration 49, log Z = 8541.0538\n",
      "Iteration 50, log Z = -6721.0297\n",
      "Log P(H_0:7000) via Bridge Sampling = -6721.0297\n",
      "First term: 6734.9609\n",
      "Prior term: 3.2387\n",
      "First term: 6734.9609\n",
      "Prior term: 3.2387\n",
      "Autocorrelation for first dimension (lags 0-4): [1.         0.49932373 0.48480317 0.2516751 ]\n",
      "Sample variance: [1.9433906e-05 1.7219703e-05 8.6832406e-06 8.8894503e-06 7.1687224e-05\n",
      " 9.6938056e-06 9.4396319e-06 8.8923269e-05 2.1164584e-05 3.3911317e-06]\n",
      "Log likelihood at MAP: -6730.7832\n",
      "Average log likelihood from samples: 6734.9609\n",
      "Entropy with 7000 preferences (Harmonic Mean): -0.6938\n",
      "Entropy with 7000 preferences (Bridge Sampling): 17.1700\n",
      "\n",
      "Processing preferences 0:8000\n",
      "MCMC Step 5000, Acceptance Rate: 0.0752, Proposal Std: 0.0088\n",
      "MCMC Step 10000, Acceptance Rate: 0.0470, Proposal Std: 0.0097\n",
      "MCMC Step 15000, Acceptance Rate: 0.0274, Proposal Std: 0.0106\n",
      "MCMC Step 20000, Acceptance Rate: 0.0134, Proposal Std: 0.0117\n",
      "MCMC Step 25000, Acceptance Rate: 0.0166, Proposal Std: 0.0129\n",
      "MCMC Step 30000, Acceptance Rate: 0.0078, Proposal Std: 0.0142\n",
      "MCMC Step 35000, Acceptance Rate: 0.0074, Proposal Std: 0.0156\n",
      "MCMC Step 40000, Acceptance Rate: 0.0048, Proposal Std: 0.0171\n",
      "MCMC Step 45000, Acceptance Rate: 0.0032, Proposal Std: 0.0189\n",
      "MCMC Step 50000, Acceptance Rate: 0.0006, Proposal Std: 0.0207\n",
      "MCMC Step 55000, Acceptance Rate: 0.0006, Proposal Std: 0.0228\n",
      "MCMC Step 60000, Acceptance Rate: 0.0000, Proposal Std: 0.0251\n",
      "MCMC Step 65000, Acceptance Rate: 0.0000, Proposal Std: 0.0276\n",
      "MCMC Step 70000, Acceptance Rate: 0.0000, Proposal Std: 0.0304\n",
      "MCMC Step 75000, Acceptance Rate: 0.0000, Proposal Std: 0.0334\n",
      "MCMC Step 80000, Acceptance Rate: 0.0000, Proposal Std: 0.0368\n",
      "MCMC Step 85000, Acceptance Rate: 0.0000, Proposal Std: 0.0404\n",
      "MCMC Step 90000, Acceptance Rate: 0.0000, Proposal Std: 0.0445\n",
      "MCMC Step 95000, Acceptance Rate: 0.0000, Proposal Std: 0.0489\n",
      "MCMC Step 100000, Acceptance Rate: 0.0000, Proposal Std: 0.0538\n",
      "MCMC Step 105000, Acceptance Rate: 0.0000, Proposal Std: 0.0592\n",
      "MCMC Step 110000, Acceptance Rate: 0.0000, Proposal Std: 0.0651\n",
      "MCMC Step 115000, Acceptance Rate: 0.0000, Proposal Std: 0.0716\n",
      "MCMC Step 120000, Acceptance Rate: 0.0000, Proposal Std: 0.0788\n",
      "Final Acceptance Rate: 0.0085\n",
      "MAP Log Posterior: -8030.65185546875\n",
      "Log P(H_0:8000) via Harmonic Mean = -8036.5522\n",
      "Initial log Z (harmonic mean): -8036.5522\n",
      "Iteration 1, log Z = 9900.2422\n",
      "Iteration 2, log Z = -8023.0458\n",
      "Iteration 3, log Z = 9899.3250\n",
      "Iteration 4, log Z = -8022.1298\n",
      "Iteration 5, log Z = 9898.8406\n",
      "Iteration 6, log Z = -8021.6454\n",
      "Iteration 7, log Z = 9898.5122\n",
      "Iteration 8, log Z = -8021.3173\n",
      "Iteration 9, log Z = 9898.2643\n",
      "Iteration 10, log Z = -8021.0692\n",
      "Iteration 11, log Z = 9898.0651\n",
      "Iteration 12, log Z = -8020.8700\n",
      "Iteration 13, log Z = 9897.8988\n",
      "Iteration 14, log Z = -8020.7020\n",
      "Iteration 15, log Z = 9897.7546\n",
      "Iteration 16, log Z = -8020.5595\n",
      "Iteration 17, log Z = 9897.6298\n",
      "Iteration 18, log Z = -8020.4345\n",
      "Iteration 19, log Z = 9897.5186\n",
      "Iteration 20, log Z = -8020.3231\n",
      "Iteration 21, log Z = 9897.4184\n",
      "Iteration 22, log Z = -8020.2216\n",
      "Iteration 23, log Z = 9897.3260\n",
      "Iteration 24, log Z = -8020.1298\n",
      "Iteration 25, log Z = 9897.2419\n",
      "Iteration 26, log Z = -8020.0458\n",
      "Iteration 27, log Z = 9897.1643\n",
      "Iteration 28, log Z = -8019.9677\n",
      "Iteration 29, log Z = 9897.0918\n",
      "Iteration 30, log Z = -8019.8954\n",
      "Iteration 31, log Z = 9897.0243\n",
      "Iteration 32, log Z = -8019.8290\n",
      "Iteration 33, log Z = 9896.9620\n",
      "Iteration 34, log Z = -8019.7665\n",
      "Iteration 35, log Z = 9896.9032\n",
      "Iteration 36, log Z = -8019.7079\n",
      "Iteration 37, log Z = 9896.8479\n",
      "Iteration 38, log Z = -8019.6513\n",
      "Iteration 39, log Z = 9896.7942\n",
      "Iteration 40, log Z = -8019.5985\n",
      "Iteration 41, log Z = 9896.7441\n",
      "Iteration 42, log Z = -8019.5477\n",
      "Iteration 43, log Z = 9896.6957\n",
      "Iteration 44, log Z = -8019.4989\n",
      "Iteration 45, log Z = 9896.6491\n",
      "Iteration 46, log Z = -8019.4540\n",
      "Iteration 47, log Z = 9896.6062\n",
      "Iteration 48, log Z = -8019.4110\n",
      "Iteration 49, log Z = 9896.5650\n",
      "Iteration 50, log Z = -8019.3700\n",
      "Log P(H_0:8000) via Bridge Sampling = -8019.3700\n",
      "First term: 8032.0986\n",
      "Prior term: 3.2387\n",
      "First term: 8032.0986\n",
      "Prior term: 3.2387\n",
      "Autocorrelation for first dimension (lags 0-4): [1.         0.5539323  0.35481814 0.43363035 0.27495256]\n",
      "Sample variance: [2.84338857e-05 1.31594525e-05 4.38164716e-06 2.45260071e-05\n",
      " 1.32419937e-05 4.12945810e-05 5.99533305e-05 2.53116341e-05\n",
      " 3.59441983e-05 1.06974212e-05]\n",
      "Log likelihood at MAP: -8027.4131\n",
      "Average log likelihood from samples: 8032.0986\n",
      "Entropy with 8000 preferences (Harmonic Mean): -1.2148\n",
      "Entropy with 8000 preferences (Bridge Sampling): 15.9674\n",
      "\n",
      "Processing preferences 0:9000\n",
      "MCMC Step 5000, Acceptance Rate: 0.0666, Proposal Std: 0.0088\n",
      "MCMC Step 10000, Acceptance Rate: 0.0320, Proposal Std: 0.0097\n",
      "MCMC Step 15000, Acceptance Rate: 0.0244, Proposal Std: 0.0106\n",
      "MCMC Step 20000, Acceptance Rate: 0.0134, Proposal Std: 0.0117\n",
      "MCMC Step 25000, Acceptance Rate: 0.0082, Proposal Std: 0.0129\n",
      "MCMC Step 30000, Acceptance Rate: 0.0030, Proposal Std: 0.0142\n",
      "MCMC Step 35000, Acceptance Rate: 0.0014, Proposal Std: 0.0156\n",
      "MCMC Step 40000, Acceptance Rate: 0.0004, Proposal Std: 0.0171\n",
      "MCMC Step 45000, Acceptance Rate: 0.0006, Proposal Std: 0.0189\n",
      "MCMC Step 50000, Acceptance Rate: 0.0002, Proposal Std: 0.0207\n",
      "MCMC Step 55000, Acceptance Rate: 0.0000, Proposal Std: 0.0228\n",
      "MCMC Step 60000, Acceptance Rate: 0.0000, Proposal Std: 0.0251\n",
      "MCMC Step 65000, Acceptance Rate: 0.0000, Proposal Std: 0.0276\n",
      "MCMC Step 70000, Acceptance Rate: 0.0000, Proposal Std: 0.0304\n",
      "MCMC Step 75000, Acceptance Rate: 0.0000, Proposal Std: 0.0334\n",
      "MCMC Step 80000, Acceptance Rate: 0.0000, Proposal Std: 0.0368\n",
      "MCMC Step 85000, Acceptance Rate: 0.0000, Proposal Std: 0.0404\n",
      "MCMC Step 90000, Acceptance Rate: 0.0000, Proposal Std: 0.0445\n",
      "MCMC Step 95000, Acceptance Rate: 0.0000, Proposal Std: 0.0489\n",
      "MCMC Step 100000, Acceptance Rate: 0.0000, Proposal Std: 0.0538\n",
      "MCMC Step 105000, Acceptance Rate: 0.0000, Proposal Std: 0.0592\n",
      "MCMC Step 110000, Acceptance Rate: 0.0000, Proposal Std: 0.0651\n",
      "MCMC Step 115000, Acceptance Rate: 0.0000, Proposal Std: 0.0716\n",
      "MCMC Step 120000, Acceptance Rate: 0.0000, Proposal Std: 0.0788\n",
      "Final Acceptance Rate: 0.0063\n",
      "MAP Log Posterior: -8882.73828125\n",
      "Log P(H_0:9000) via Harmonic Mean = -8889.3635\n",
      "Initial log Z (harmonic mean): -8889.3635\n",
      "Iteration 1, log Z = 11049.4932\n",
      "Iteration 2, log Z = -8875.4928\n",
      "Iteration 3, log Z = 11048.5333\n",
      "Iteration 4, log Z = -8874.5338\n",
      "Iteration 5, log Z = 11048.0335\n",
      "Iteration 6, log Z = -8874.0338\n",
      "Iteration 7, log Z = 11047.6978\n",
      "Iteration 8, log Z = -8873.6998\n",
      "Iteration 9, log Z = 11047.4469\n",
      "Iteration 10, log Z = -8873.4479\n",
      "Iteration 11, log Z = 11047.2453\n",
      "Iteration 12, log Z = -8873.2467\n",
      "Iteration 13, log Z = 11047.0777\n",
      "Iteration 14, log Z = -8873.0787\n",
      "Iteration 15, log Z = 11046.9339\n",
      "Iteration 16, log Z = -8872.9342\n",
      "Iteration 17, log Z = 11046.8075\n",
      "Iteration 18, log Z = -8872.8092\n",
      "Iteration 19, log Z = 11046.6965\n",
      "Iteration 20, log Z = -8872.6959\n",
      "Iteration 21, log Z = 11046.5946\n",
      "Iteration 22, log Z = -8872.5943\n",
      "Iteration 23, log Z = 11046.5023\n",
      "Iteration 24, log Z = -8872.5026\n",
      "Iteration 25, log Z = 11046.4182\n",
      "Iteration 26, log Z = -8872.4186\n",
      "Iteration 27, log Z = 11046.3407\n",
      "Iteration 28, log Z = -8872.3404\n",
      "Iteration 29, log Z = 11046.2682\n",
      "Iteration 30, log Z = -8872.2701\n",
      "Iteration 31, log Z = 11046.2026\n",
      "Iteration 32, log Z = -8872.2037\n",
      "Iteration 33, log Z = 11046.1404\n",
      "Iteration 34, log Z = -8872.1412\n",
      "Iteration 35, log Z = 11046.0816\n",
      "Iteration 36, log Z = -8872.0826\n",
      "Iteration 37, log Z = 11046.0263\n",
      "Iteration 38, log Z = -8872.0279\n",
      "Iteration 39, log Z = 11045.9745\n",
      "Iteration 40, log Z = -8871.9752\n",
      "Iteration 41, log Z = 11045.9245\n",
      "Iteration 42, log Z = -8871.9264\n",
      "Iteration 43, log Z = 11045.8780\n",
      "Iteration 44, log Z = -8871.8795\n",
      "Iteration 45, log Z = 11045.8333\n",
      "Iteration 46, log Z = -8871.8326\n",
      "Iteration 47, log Z = 11045.7885\n",
      "Iteration 48, log Z = -8871.7897\n",
      "Iteration 49, log Z = 11045.7473\n",
      "Iteration 50, log Z = -8871.7467\n",
      "Log P(H_0:9000) via Bridge Sampling = -8871.7467\n",
      "First term: 8885.4150\n",
      "Prior term: 3.2387\n",
      "First term: 8885.4150\n",
      "Prior term: 3.2387\n",
      "Autocorrelation for first dimension (lags 0-4): [1.        0.3981945]\n",
      "Sample variance: [2.0835951e-06 2.0281006e-05 1.7720546e-07 1.2429082e-05 4.0575957e-05\n",
      " 9.0099147e-06 5.2419557e-05 5.3954474e-05 9.4084935e-06 5.1593862e-08]\n",
      "Log likelihood at MAP: -8879.5000\n",
      "Average log likelihood from samples: 8885.4150\n",
      "Entropy with 9000 preferences (Harmonic Mean): -0.7102\n",
      "Entropy with 9000 preferences (Bridge Sampling): 16.9066\n",
      "\n",
      "Processing preferences 0:10000\n",
      "MCMC Step 5000, Acceptance Rate: 0.0636, Proposal Std: 0.0088\n",
      "MCMC Step 10000, Acceptance Rate: 0.0236, Proposal Std: 0.0097\n",
      "MCMC Step 15000, Acceptance Rate: 0.0144, Proposal Std: 0.0106\n",
      "MCMC Step 20000, Acceptance Rate: 0.0082, Proposal Std: 0.0117\n",
      "MCMC Step 25000, Acceptance Rate: 0.0060, Proposal Std: 0.0129\n",
      "MCMC Step 30000, Acceptance Rate: 0.0026, Proposal Std: 0.0142\n",
      "MCMC Step 35000, Acceptance Rate: 0.0022, Proposal Std: 0.0156\n",
      "MCMC Step 40000, Acceptance Rate: 0.0008, Proposal Std: 0.0171\n",
      "MCMC Step 45000, Acceptance Rate: 0.0006, Proposal Std: 0.0189\n",
      "MCMC Step 50000, Acceptance Rate: 0.0004, Proposal Std: 0.0207\n",
      "MCMC Step 55000, Acceptance Rate: 0.0000, Proposal Std: 0.0228\n",
      "MCMC Step 60000, Acceptance Rate: 0.0002, Proposal Std: 0.0251\n",
      "MCMC Step 65000, Acceptance Rate: 0.0000, Proposal Std: 0.0276\n",
      "MCMC Step 70000, Acceptance Rate: 0.0002, Proposal Std: 0.0304\n",
      "MCMC Step 75000, Acceptance Rate: 0.0000, Proposal Std: 0.0334\n",
      "MCMC Step 80000, Acceptance Rate: 0.0000, Proposal Std: 0.0368\n",
      "MCMC Step 85000, Acceptance Rate: 0.0000, Proposal Std: 0.0404\n",
      "MCMC Step 90000, Acceptance Rate: 0.0000, Proposal Std: 0.0445\n",
      "MCMC Step 95000, Acceptance Rate: 0.0000, Proposal Std: 0.0489\n",
      "MCMC Step 100000, Acceptance Rate: 0.0000, Proposal Std: 0.0538\n",
      "MCMC Step 105000, Acceptance Rate: 0.0000, Proposal Std: 0.0592\n",
      "MCMC Step 110000, Acceptance Rate: 0.0000, Proposal Std: 0.0651\n",
      "MCMC Step 115000, Acceptance Rate: 0.0000, Proposal Std: 0.0716\n",
      "MCMC Step 120000, Acceptance Rate: 0.0000, Proposal Std: 0.0788\n",
      "Final Acceptance Rate: 0.0051\n",
      "MAP Log Posterior: -9840.984375\n",
      "Log P(H_0:10000) via Harmonic Mean = -9844.6867\n",
      "Initial log Z (harmonic mean): -9844.6867\n",
      "Iteration 1, log Z = 12198.9278\n",
      "Iteration 2, log Z = -9831.2984\n",
      "Iteration 3, log Z = 12198.0374\n",
      "Iteration 4, log Z = -9830.4078\n",
      "Iteration 5, log Z = 12197.5523\n",
      "Iteration 6, log Z = -9829.9234\n",
      "Iteration 7, log Z = 12197.2227\n",
      "Iteration 8, log Z = -9829.5914\n",
      "Iteration 9, log Z = 12196.9718\n",
      "Iteration 10, log Z = -9829.3414\n",
      "Iteration 11, log Z = 12196.7712\n",
      "Iteration 12, log Z = -9829.1422\n",
      "Iteration 13, log Z = 12196.6050\n",
      "Iteration 14, log Z = -9828.9742\n",
      "Iteration 15, log Z = 12196.4609\n",
      "Iteration 16, log Z = -9828.8297\n",
      "Iteration 17, log Z = 12196.3345\n",
      "Iteration 18, log Z = -9828.7047\n",
      "Iteration 19, log Z = 12196.2234\n",
      "Iteration 20, log Z = -9828.5934\n",
      "Iteration 21, log Z = 12196.1233\n",
      "Iteration 22, log Z = -9828.4938\n",
      "Iteration 23, log Z = 12196.0328\n",
      "Iteration 24, log Z = -9828.4039\n",
      "Iteration 25, log Z = 12195.9505\n",
      "Iteration 26, log Z = -9828.3199\n",
      "Iteration 27, log Z = 12195.8730\n",
      "Iteration 28, log Z = -9828.2438\n",
      "Iteration 29, log Z = 12195.8023\n",
      "Iteration 30, log Z = -9828.1734\n",
      "Iteration 31, log Z = 12195.7368\n",
      "Iteration 32, log Z = -9828.1070\n",
      "Iteration 33, log Z = 12195.6746\n",
      "Iteration 34, log Z = -9828.0445\n",
      "Iteration 35, log Z = 12195.6158\n",
      "Iteration 36, log Z = -9827.9859\n",
      "Iteration 37, log Z = 12195.5605\n",
      "Iteration 38, log Z = -9827.9313\n",
      "Iteration 39, log Z = 12195.5088\n",
      "Iteration 40, log Z = -9827.8785\n",
      "Iteration 41, log Z = 12195.4587\n",
      "Iteration 42, log Z = -9827.8297\n",
      "Iteration 43, log Z = 12195.4122\n",
      "Iteration 44, log Z = -9827.7828\n",
      "Iteration 45, log Z = 12195.3675\n",
      "Iteration 46, log Z = -9827.7359\n",
      "Iteration 47, log Z = 12195.3227\n",
      "Iteration 48, log Z = -9827.6930\n",
      "Iteration 49, log Z = 12195.2816\n",
      "Iteration 50, log Z = -9827.6500\n",
      "Log P(H_0:10000) via Bridge Sampling = -9827.6500\n",
      "First term: 9840.9785\n",
      "Prior term: 3.2387\n",
      "First term: 9840.9785\n",
      "Prior term: 3.2387\n",
      "Autocorrelation for first dimension (lags 0-4): [1.         0.48701367]\n",
      "Sample variance: [2.4248784e-06 2.2991580e-06 1.6875398e-06 1.0988049e-06 7.1141864e-07\n",
      " 3.7631573e-05 1.1657823e-04 9.0981139e-06 2.9776774e-07 4.6882833e-06]\n",
      "Log likelihood at MAP: -9837.7461\n",
      "Average log likelihood from samples: 9840.9785\n",
      "Entropy with 10000 preferences (Harmonic Mean): -0.4699\n",
      "Entropy with 10000 preferences (Bridge Sampling): 16.5668\n",
      "\n",
      "Summary of Entropies:\n",
      "Preferences 0:1000: Entropy (Harmonic Mean) = -4.8856, (Bridge Sampling) = 11.6604\n",
      "Preferences 0:2000: Entropy (Harmonic Mean) = -4.0496, (Bridge Sampling) = 12.4602\n",
      "Preferences 0:3000: Entropy (Harmonic Mean) = -1.7213, (Bridge Sampling) = 16.4814\n",
      "Preferences 0:4000: Entropy (Harmonic Mean) = -3.0106, (Bridge Sampling) = 16.8963\n",
      "Preferences 0:5000: Entropy (Harmonic Mean) = -3.0215, (Bridge Sampling) = 15.7927\n",
      "Preferences 0:6000: Entropy (Harmonic Mean) = -2.5668, (Bridge Sampling) = 18.9424\n",
      "Preferences 0:7000: Entropy (Harmonic Mean) = -0.6938, (Bridge Sampling) = 17.1700\n",
      "Preferences 0:8000: Entropy (Harmonic Mean) = -1.2148, (Bridge Sampling) = 15.9674\n",
      "Preferences 0:9000: Entropy (Harmonic Mean) = -0.7102, (Bridge Sampling) = 16.9066\n",
      "Preferences 0:10000: Entropy (Harmonic Mean) = -0.4699, (Bridge Sampling) = 16.5668\n",
      "Results saved: entropies_hm.npy, entropies_bridge.npy, posterior_samples_*.npy, map_solution_*.npy\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "import pickle\n",
    "from scipy.special import gammaln\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def encode_preferences(encoder, preferences, segments):\n",
    "    for param in encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    segment_states = [\n",
    "        torch.tensor(\n",
    "            np.array([state_t[0, 0, :] for state_t, _, _, done_t in segment if not done_t]),\n",
    "            dtype=torch.float32\n",
    "        ).to(device) if any(not done_t for _, _, _, done_t in segment) else None\n",
    "        for segment in segments\n",
    "    ]\n",
    "    Phi_tau_list = []\n",
    "    for seg_states in segment_states:\n",
    "        if seg_states is not None:\n",
    "            with torch.no_grad():\n",
    "                phi, _, _ = encoder(seg_states)\n",
    "            Phi_tau = phi.sum(dim=0)\n",
    "            Phi_tau_list.append(Phi_tau)\n",
    "        else:\n",
    "            Phi_tau_list.append(None)\n",
    "    \n",
    "    valid_pairs = [(i, j) for i, j, _ in preferences if Phi_tau_list[i] is not None and Phi_tau_list[j] is not None]\n",
    "    if not valid_pairs:\n",
    "        raise ValueError(\"No valid preference pairs found.\")\n",
    "    \n",
    "    Phi_tau_i = torch.stack([Phi_tau_list[i] for i, j in valid_pairs])\n",
    "    Phi_tau_j = torch.stack([Phi_tau_list[j] for i, j in valid_pairs])\n",
    "    return Phi_tau_i, Phi_tau_j\n",
    "\n",
    "def log_likelihood_vectorized(w_batch, Phi_tau_i, Phi_tau_j, beta):\n",
    "    w_batch = torch.tensor(w_batch, dtype=torch.float32).to(device)\n",
    "    R_i = torch.matmul(Phi_tau_i, w_batch.T)\n",
    "    R_j = torch.matmul(Phi_tau_j, w_batch.T)\n",
    "    \n",
    "    beta_R_i = beta * R_i\n",
    "    beta_R_j = beta * R_j\n",
    "    max_val = torch.maximum(beta_R_i, beta_R_j)\n",
    "    log_sum_exp = max_val + torch.log(\n",
    "        torch.exp(beta_R_i - max_val) + torch.exp(beta_R_j - max_val)\n",
    "    )\n",
    "    \n",
    "    terms = beta * R_j - log_sum_exp\n",
    "    log_likelihoods = torch.sum(terms, dim=0)\n",
    "    \n",
    "    if torch.any(log_likelihoods > 0):\n",
    "        print(\"Warning: Positive log likelihood detected.\")\n",
    "        print(f\"Sample terms: {terms[:5, :5]}\")\n",
    "        print(f\"Log likelihoods: {log_likelihoods[:5]}\")\n",
    "    \n",
    "    return log_likelihoods.cpu().numpy()\n",
    "\n",
    "def log_prior_vectorized(w_batch):\n",
    "    # Uniform prior on the unit sphere (d-1 dimensions)\n",
    "    # Log of the surface area of a (d-1)-sphere: log(2 * pi^(d/2) / Gamma(d/2))\n",
    "    w_batch = torch.tensor(w_batch, dtype=torch.float32).to(device)\n",
    "    d = w_batch.shape[1]  # feature_dim\n",
    "    log_surface_area = np.log(2) + (d/2) * np.log(np.pi) - gammaln(d/2)\n",
    "    return -log_surface_area * torch.ones(w_batch.shape[0]).to(device).cpu().numpy()\n",
    "\n",
    "def harmonic_mean_marginal_likelihood(posterior_samples, Phi_tau_i, Phi_tau_j, beta):\n",
    "    log_likelihoods = log_likelihood_vectorized(posterior_samples, Phi_tau_i, Phi_tau_j, beta)\n",
    "    log_priors = log_prior_vectorized(posterior_samples)\n",
    "    log_unnorm = log_likelihoods + log_priors\n",
    "    inverse_terms = -log_unnorm\n",
    "    log_inverse_sum = logsumexp(inverse_terms) - np.log(len(posterior_samples))\n",
    "    log_marginal = -log_inverse_sum\n",
    "    return log_marginal\n",
    "\n",
    "def bridge_sampling_marginal_likelihood(posterior_samples, prior_samples, Phi_tau_i, Phi_tau_j, beta, max_iter=50, tol=1e-4):\n",
    "    # Compute unnormalized posterior for posterior samples\n",
    "    post_log_likelihoods = log_likelihood_vectorized(posterior_samples, Phi_tau_i, Phi_tau_j, beta)\n",
    "    post_log_priors = log_prior_vectorized(posterior_samples)\n",
    "    post_log_unnorm = post_log_likelihoods + post_log_priors\n",
    "\n",
    "    # Compute unnormalized posterior for prior samples\n",
    "    prior_log_likelihoods = log_likelihood_vectorized(prior_samples, Phi_tau_i, Phi_tau_j, beta)\n",
    "    prior_log_priors = log_prior_vectorized(prior_samples)\n",
    "    prior_log_unnorm = prior_log_likelihoods + prior_log_priors\n",
    "\n",
    "    # Initial guess using harmonic mean\n",
    "    log_Z = harmonic_mean_marginal_likelihood(posterior_samples, Phi_tau_i, Phi_tau_j, beta)\n",
    "    print(f\"Initial log Z (harmonic mean): {log_Z:.4f}\")\n",
    "\n",
    "    # Iterative bridge sampling with adjusted alpha\n",
    "    m_post = len(posterior_samples)\n",
    "    m_prior = len(prior_samples)\n",
    "    alpha = m_post / (m_post + m_prior)  # Optimal alpha for balanced sampling\n",
    "\n",
    "    for iteration in range(max_iter):\n",
    "        # Compute bridge function h(w) for posterior samples\n",
    "        log_h_post = np.log(alpha / m_prior + (1 - alpha) / m_post) + logsumexp([\n",
    "            np.log(alpha) + post_log_priors,\n",
    "            np.log(1 - alpha) + post_log_unnorm - log_Z\n",
    "        ], axis=0)\n",
    "\n",
    "        # Compute bridge function h(w) for prior samples\n",
    "        log_h_prior = np.log(alpha / m_prior + (1 - alpha) / m_post) + logsumexp([\n",
    "            np.log(alpha) + prior_log_priors,\n",
    "            np.log(1 - alpha) + prior_log_unnorm - log_Z\n",
    "        ], axis=0)\n",
    "\n",
    "        # Numerator: E_h [ P(w | D, P) / h(w) ]\n",
    "        num_terms = (post_log_unnorm - log_Z) - log_h_post\n",
    "        numerator = logsumexp(num_terms) - np.log(m_post)\n",
    "\n",
    "        # Denominator: E_h [ P(D, P | w) P(w) / h(w) ]\n",
    "        denom_terms = prior_log_unnorm - log_h_prior\n",
    "        denominator = logsumexp(denom_terms) - np.log(m_prior)\n",
    "\n",
    "        # Update log Z\n",
    "        new_log_Z = numerator - denominator\n",
    "        if abs(new_log_Z - log_Z) < tol:\n",
    "            break\n",
    "        log_Z = new_log_Z\n",
    "        print(f\"Iteration {iteration+1}, log Z = {log_Z:.4f}\")\n",
    "\n",
    "    if log_Z > 0:\n",
    "        print(\"Warning: log P(D, P) is positive, which is incorrect. Check likelihood and prior computations.\")\n",
    "    return log_Z\n",
    "\n",
    "def compute_entropy_importance_sampling(Phi_tau_i, Phi_tau_j, samples, beta, log_P_H):\n",
    "    log_probs = log_likelihood_vectorized(samples, Phi_tau_i, Phi_tau_j, beta)\n",
    "    if np.any(np.isnan(log_probs)) or np.any(np.isinf(log_probs)):\n",
    "        raise ValueError(\"Log probabilities contain NaN or Inf values.\")\n",
    "    \n",
    "    log_prior_probs = log_prior_vectorized(samples)\n",
    "    first_term = -np.mean(log_probs)\n",
    "    prior_term = -np.mean(log_prior_probs)  # Should be 0 for uniform prior\n",
    "    print(f\"First term: {first_term:.4f}\")\n",
    "    print(f\"Prior term: {prior_term:.4f}\")\n",
    "    \n",
    "    entropy = first_term + prior_term + log_P_H\n",
    "    return entropy, log_probs\n",
    "\n",
    "def bayesian_rex_mcmc(trex_model, Phi_tau_i, Phi_tau_j, num_samples=100000, burn_in=20000, beta=1.0, initial_proposal_std=0.008):\n",
    "    w_current = trex_model.model.weight.data.clone().squeeze().to(device)\n",
    "    w_current = w_current / torch.norm(w_current, p=2)\n",
    "    \n",
    "    def log_prior(w):\n",
    "        d = w.shape[0]\n",
    "        log_surface_area = np.log(2) + (d/2) * np.log(np.pi) - gammaln(d/2)\n",
    "        return -log_surface_area\n",
    "    \n",
    "    def log_likelihood(w, Phi_tau_i, Phi_tau_j, beta):\n",
    "        R_i = torch.matmul(Phi_tau_i, w)\n",
    "        R_j = torch.matmul(Phi_tau_j, w)\n",
    "        terms = beta * R_j - torch.log(torch.exp(beta * R_i) + torch.exp(beta * R_j))\n",
    "        return torch.sum(terms)\n",
    "    \n",
    "    samples = []\n",
    "    accepted = 0\n",
    "    w_map = w_current.clone()\n",
    "    log_posterior_map = log_prior(w_current) + log_likelihood(w_current, Phi_tau_i, Phi_tau_j, beta)\n",
    "    \n",
    "    # Adaptive proposal standard deviation\n",
    "    proposal_std = initial_proposal_std\n",
    "    target_acceptance = 0.3\n",
    "    adjust_interval = 5000\n",
    "    accepted_in_interval = 0\n",
    "    \n",
    "    for step in range(num_samples + burn_in):\n",
    "        w_proposed = w_current + torch.normal(mean=0, std=proposal_std, size=w_current.shape).to(device)\n",
    "        w_proposed = w_proposed / torch.norm(w_proposed, p=2)\n",
    "        \n",
    "        log_posterior_current = log_prior(w_current) + log_likelihood(w_current, Phi_tau_i, Phi_tau_j, beta)\n",
    "        log_posterior_proposed = log_prior(w_proposed) + log_likelihood(w_proposed, Phi_tau_i, Phi_tau_j, beta)\n",
    "        \n",
    "        if log_posterior_proposed > log_posterior_map:\n",
    "            w_map = w_proposed.clone()\n",
    "            log_posterior_map = log_posterior_proposed\n",
    "        \n",
    "        log_alpha = log_posterior_proposed - log_posterior_current\n",
    "        alpha = torch.exp(log_alpha)\n",
    "        \n",
    "        if torch.rand(1, device=device) < alpha:\n",
    "            w_current = w_proposed\n",
    "            accepted += 1\n",
    "            accepted_in_interval += 1\n",
    "            if step >= burn_in:\n",
    "                samples.append(w_current.cpu().detach().numpy())\n",
    "        \n",
    "        if (step + 1) % adjust_interval == 0:\n",
    "            acceptance_rate = accepted_in_interval / adjust_interval if step > 0 else 0\n",
    "            if acceptance_rate < target_acceptance * 0.8:\n",
    "                proposal_std *= 1.1\n",
    "            elif acceptance_rate > target_acceptance * 1.2:\n",
    "                proposal_std *= 0.9\n",
    "            accepted_in_interval = 0\n",
    "            print(f\"MCMC Step {step+1}, Acceptance Rate: {acceptance_rate:.4f}, Proposal Std: {proposal_std:.4f}\")\n",
    "    \n",
    "    thinning = 50  # Increased thinning to reduce autocorrelation\n",
    "    samples = samples[::thinning]\n",
    "    \n",
    "    print(f\"Final Acceptance Rate: {accepted / (num_samples + burn_in):.4f}\")\n",
    "    print(f\"MAP Log Posterior: {log_posterior_map.item()}\")\n",
    "    \n",
    "    samples = np.array(samples)\n",
    "    return samples, w_map\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    feedback_path = 'ppo_merge-v0_1377.pkl'\n",
    "    with open(feedback_path, 'rb') as file:\n",
    "        feedback_data = pickle.load(file)\n",
    "    \n",
    "    segments = feedback_data['segments']\n",
    "    preferences = feedback_data['preferences']\n",
    "    preferences = np.random.permutation(preferences).tolist()\n",
    "    print(\"Preferences have been shuffled.\")\n",
    "    \n",
    "    state_dim = 5\n",
    "    feature_dim = 10\n",
    "    encoder = FeatureEncoder(input_dim=state_dim, feature_dim=feature_dim)\n",
    "    trex_model = TREXRewardPredictor(feature_dim=feature_dim)\n",
    "    \n",
    "    encoder.load_state_dict(torch.load(\"feature_encoder_pretrain.pth\", map_location=device))\n",
    "    trex_model.load_state_dict(torch.load(\"trex_model_finetune.pth\", map_location=device))\n",
    "    \n",
    "    encoder = encoder.to(device)\n",
    "    trex_model = trex_model.to(device)\n",
    "    encoder.eval()\n",
    "    trex_model.eval()\n",
    "    \n",
    "    print(\"Models loaded from 'feature_encoder_pretrain.pth' and 'trex_model_finetune.pth'\")\n",
    "    \n",
    "    n_prior_samples = 20000\n",
    "    prior_samples = np.random.normal(0, 1, (n_prior_samples, feature_dim))\n",
    "    prior_samples = prior_samples / np.linalg.norm(prior_samples, axis=1, keepdims=True)\n",
    "    \n",
    "    preference_sizes = range(1000, len(preferences) + 1000, 1000)\n",
    "    entropies_hm = []\n",
    "    entropies_bridge = []\n",
    "    all_posterior_samples = []\n",
    "    all_map_solutions = []\n",
    "    beta = 1.0\n",
    "    \n",
    "    print(\"Running Bayesian REX MCMC with harmonic mean and corrected bridge sampling for marginal likelihood...\")\n",
    "    for pref_size in preference_sizes:\n",
    "        print(f\"\\nProcessing preferences 0:{pref_size}\")\n",
    "        \n",
    "        Phi_tau_i_full, Phi_tau_j_full = encode_preferences(encoder, preferences[:pref_size], segments)\n",
    "        \n",
    "        initial_proposal_std = 0.008\n",
    "        burn_in = 20000\n",
    "        \n",
    "        posterior_samples, map_solution = bayesian_rex_mcmc(\n",
    "            trex_model,\n",
    "            Phi_tau_i_full,\n",
    "            Phi_tau_j_full,\n",
    "            num_samples=100000,\n",
    "            burn_in=burn_in,\n",
    "            beta=beta,\n",
    "            initial_proposal_std=initial_proposal_std\n",
    "        )\n",
    "        \n",
    "        # Estimate marginal likelihood using harmonic mean\n",
    "        log_P_H_hm = harmonic_mean_marginal_likelihood(\n",
    "            posterior_samples, Phi_tau_i_full, Phi_tau_j_full, beta\n",
    "        )\n",
    "        print(f\"Log P(H_0:{pref_size}) via Harmonic Mean = {log_P_H_hm:.4f}\")\n",
    "        \n",
    "        # Estimate marginal likelihood using corrected bridge sampling\n",
    "        log_P_H_bridge = bridge_sampling_marginal_likelihood(\n",
    "            posterior_samples, prior_samples, Phi_tau_i_full, Phi_tau_j_full, beta\n",
    "        )\n",
    "        print(f\"Log P(H_0:{pref_size}) via Bridge Sampling = {log_P_H_bridge:.4f}\")\n",
    "        \n",
    "        # Compute entropy using both estimates\n",
    "        entropy_hm, log_probs = compute_entropy_importance_sampling(\n",
    "            Phi_tau_i_full, Phi_tau_j_full, posterior_samples, beta, log_P_H_hm\n",
    "        )\n",
    "        entropy_bridge, _ = compute_entropy_importance_sampling(\n",
    "            Phi_tau_i_full, Phi_tau_j_full, posterior_samples, beta, log_P_H_bridge\n",
    "        )\n",
    "        \n",
    "        from numpy import correlate\n",
    "        if len(posterior_samples) > 1:\n",
    "            autocorr = correlate(posterior_samples[:, 0], posterior_samples[:, 0], mode='full')\n",
    "            autocorr = autocorr[len(autocorr)//2:len(autocorr)//2+5] / autocorr[len(autocorr)//2]\n",
    "            print(f\"Autocorrelation for first dimension (lags 0-4): {autocorr}\")\n",
    "        else:\n",
    "            print(\"Not enough samples to compute autocorrelation.\")\n",
    "        \n",
    "        sample_variance = np.var(posterior_samples, axis=0)\n",
    "        print(f\"Sample variance: {sample_variance}\")\n",
    "        \n",
    "        log_likelihood_map = log_likelihood_vectorized(map_solution.reshape(1, -1), Phi_tau_i_full, Phi_tau_j_full, beta)\n",
    "        print(f\"Log likelihood at MAP: {log_likelihood_map[0]:.4f}\")\n",
    "        print(f\"Average log likelihood from samples: {-np.mean(log_probs):.4f}\")\n",
    "        \n",
    "        entropies_hm.append(entropy_hm)\n",
    "        entropies_bridge.append(entropy_bridge)\n",
    "        all_posterior_samples.append(posterior_samples)\n",
    "        all_map_solutions.append(map_solution)\n",
    "        \n",
    "        print(f\"Entropy with {pref_size} preferences (Harmonic Mean): {entropy_hm:.4f}\")\n",
    "        print(f\"Entropy with {pref_size} preferences (Bridge Sampling): {entropy_bridge:.4f}\")\n",
    "            \n",
    "    np.save(\"entropies_hm.npy\", np.array(entropies_hm))\n",
    "    np.save(\"entropies_bridge.npy\", np.array(entropies_bridge))\n",
    "    for i, pref_size in enumerate(preference_sizes):\n",
    "        np.save(f\"posterior_samples_{pref_size}.npy\", all_posterior_samples[i])\n",
    "        map_solution = all_map_solutions[i]\n",
    "        if isinstance(map_solution, torch.Tensor):\n",
    "            map_solution = map_solution.cpu().detach().numpy()\n",
    "        np.save(f\"map_solution_{pref_size}.npy\", map_solution)\n",
    "\n",
    "    print(\"\\nSummary of Entropies:\")\n",
    "    for pref_size, entropy_hm, entropy_bridge in zip(preference_sizes, entropies_hm, entropies_bridge):\n",
    "        print(f\"Preferences 0:{pref_size}: Entropy (Harmonic Mean) = {entropy_hm:.4f}, (Bridge Sampling) = {entropy_bridge:.4f}\")\n",
    "\n",
    "    print(\"Results saved: entropies_hm.npy, entropies_bridge.npy, posterior_samples_*.npy, map_solution_*.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "285b9d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x76c6da8b7fd0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(entropies_hm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aada959f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
