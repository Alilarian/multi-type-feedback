{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "105ee81b-a62d-4021-80dd-ee6d534b9d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mymetz\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/metz/iclr_multi_feedback/mutli-type-feedback/wandb/run-20241008_200340-abo7kva9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ymetz/your_project_name/runs/abo7kva9' target=\"_blank\">expert-mountain-53</a></strong> to <a href='https://wandb.ai/ymetz/your_project_name' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ymetz/your_project_name' target=\"_blank\">https://wandb.ai/ymetz/your_project_name</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ymetz/your_project_name/runs/abo7kva9' target=\"_blank\">https://wandb.ai/ymetz/your_project_name/runs/abo7kva9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import wandb\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(project=\"your_project_name\")\n",
    "\n",
    "# Fetch runs from your project\n",
    "api = wandb.Api()\n",
    "runs = api.runs(\"multi_reward_feedback_final_lul\", filters={\"display_name\": {\"$regex\": \"^RL_.*\"}})\n",
    "#runs_orig = api.runs(\"multi_reward_feedback_final\", filters={\"display_name\": {\"$regex\": \"^RL_.*\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8516eb46-50d9-4978-955e-b6ab46c474ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store data from filtered runs\n",
    "filtered_run_data = []\n",
    "\n",
    "# Iterate through the runs\n",
    "for run in runs:\n",
    "    # Check if the run name starts with \"ppo_\"\n",
    "    if run.name.startswith(\"RL_\") and \"ensemble\" not in run.name:\n",
    "        # Get the summary statistics (includes final values of metrics)\n",
    "        summary = run.summary._json_dict\n",
    "\n",
    "        # Get the history (includes all logged metrics)\n",
    "        history = run.history(keys=[\"eval/mean_reward\", \"global_step\"])\n",
    "\n",
    "        # Combine summary and history data\n",
    "        run_data = {\n",
    "            \"run_id\": run.id,\n",
    "            \"run_name\": run.name,\n",
    "            **summary,\n",
    "            **{f\"{k}_history\": v.tolist() for k, v in history.items()}\n",
    "        }\n",
    "\n",
    "        filtered_run_data.append(run_data)\n",
    "\n",
    "\"\"\"for run in runs_orig:\n",
    "    # Check if the run name starts with \"ppo_\"\n",
    "    if run.name.startswith(\"RL_\") and \"ensemble\" not in run.name:\n",
    "        # Get the summary statistics (includes final values of metrics)\n",
    "        summary = run.summary._json_dict\n",
    "\n",
    "        # Get the history (includes all logged metrics)\n",
    "        history = run.history(keys=[\"eval/mean_reward\", \"global_step\"])\n",
    "\n",
    "        # Combine summary and history data\n",
    "        run_data = {\n",
    "            \"run_id\": run.id,\n",
    "            \"run_name\": run.name,\n",
    "            **summary,\n",
    "            **{f\"{k}_history\": v.tolist() for k, v in history.items()}\n",
    "        }\n",
    "\n",
    "        filtered_run_data.append(run_data)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Create a DataFrame from filtered run data\n",
    "orig_df = pd.DataFrame(filtered_run_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a602d039-ad57-4472-a062-59bda636ce3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss curves for Swimmer-v5 have been saved to noise_rl_curves_Swimmer-v5.png\n",
      "Loss curves for HalfCheetah-v5 have been saved to noise_rl_curves_HalfCheetah-v5.png\n",
      "Loss curves for Ant-v5 have been saved to noise_rl_curves_Ant-v5.png\n",
      "Loss curves for Humanoid-v5 have been saved to noise_rl_curves_Humanoid-v5.png\n",
      "Loss curves for Hopper-v5 have been saved to noise_rl_curves_Hopper-v5.png\n",
      "Loss curves for Walker2d-v5 have been saved to noise_rl_curves_Walker2d-v5.png\n"
     ]
    }
   ],
   "source": [
    "import colorsys\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Existing color scale\n",
    "color_scale = OrderedDict([\n",
    "    ('evaluative', '#1f77b4'),     # blue\n",
    "    ('comparative', '#ff7f0e'),    # orange\n",
    "    ('demonstrative', '#2ca02c'),  # green\n",
    "    ('corrective', '#d62728'),     # red\n",
    "    ('descriptive', '#9467bd'),    # purple\n",
    "    ('preference', '#8c564b'),     # brown\n",
    "    ('descriptive_preference', '#e377c2'),  # pink\n",
    "])\n",
    "\n",
    "# Function to create color variations with stronger fading\n",
    "def create_color_variations(base_color, num_variations=5):\n",
    "    rgb = plt.matplotlib.colors.to_rgb(base_color)\n",
    "    hsv = colorsys.rgb_to_hsv(*rgb)\n",
    "    colors = []\n",
    "    for i in range(num_variations):\n",
    "        s = max(0.1, hsv[1] * (1 - i * 0.25))\n",
    "        v = min(1.0, hsv[2] * (1 + i * 0.25))\n",
    "        colors.append(colorsys.hsv_to_rgb(hsv[0], s, v))\n",
    "    return colors\n",
    "\n",
    "# Function to extract environment, feedback type, and noise level from run name\n",
    "def extract_info(run_name):\n",
    "    parts = run_name.split('_')\n",
    "    env = parts[2]\n",
    "    \n",
    "    # Handle the special case of \"descriptive_preference\"\n",
    "    if \"descriptive_preference\" in run_name:\n",
    "        feedback = \"descriptive_preference\"\n",
    "        noise = parts[-1] if parts[-2] == \"noise\" else \"0.0\"\n",
    "    else:\n",
    "        feedback = parts[4]\n",
    "        noise = parts[-1] if parts[-2] == \"noise\" else \"0.0\"\n",
    "\n",
    "    return env, feedback, float(noise)\n",
    "\n",
    "def safe_convert_to_float(value):\n",
    "    try:\n",
    "        return float(value)\n",
    "    except (ValueError, TypeError):\n",
    "        return np.nan\n",
    "\n",
    "# Function to interpolate NaN values in a series\n",
    "def interpolate_nans(series):\n",
    "    return pd.Series(series).interpolate().values\n",
    "\n",
    "# Group runs by environment, feedback type, and noise level\n",
    "grouped_runs = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "for _, row in orig_df.iterrows():\n",
    "    env, feedback, noise = extract_info(row['run_name'])\n",
    "    if isinstance(row['eval/mean_reward_history'], float):\n",
    "        continue\n",
    "    row['eval/mean_reward_history'] = [np.nan if x == \"nan\" else x for x in row['eval/mean_reward_history']]\n",
    "    #row['eval/mean_reward_history'] = interpolate_nans(row['eval/mean_reward_history'])\n",
    "    grouped_runs[env][feedback][noise].append(row)\n",
    "\n",
    "# Plotting function\n",
    "def plot_environment(env, feedback_runs):\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "    #fig.suptitle(f\"Reward Model: Validation Loss Curves for {env}\", fontsize=18)\n",
    "    \n",
    "    feedback_types = [\"evaluative\", \"comparative\", \"demonstrative\", \"corrective\", \"descriptive\", \"descriptive_preference\"]\n",
    "    noise_levels = [0.0, 0.1, 0.25, 0.5, 0.75]\n",
    "    \n",
    "    for idx, feedback in enumerate(feedback_types):\n",
    "        ax = axes[idx // 3, idx % 3]\n",
    "        base_color = color_scale[feedback]\n",
    "        color_variations = create_color_variations(base_color)\n",
    "        \n",
    "        for noise, color in zip(noise_levels, color_variations):\n",
    "            if noise not in feedback_runs[feedback]:\n",
    "                continue\n",
    "            \n",
    "            runs = feedback_runs[feedback][noise]\n",
    "            max_steps = max(max(map(safe_convert_to_float, run['global_step_history'])) for run in runs)\n",
    "            \n",
    "            # Create a common x-axis (steps) based on the actual step values\n",
    "            common_steps = np.arange(0, int(max_steps) + 1, 1000)  # Adjust step size as needed\n",
    "            \n",
    "            all_losses = np.full((len(runs), len(common_steps)), np.nan)\n",
    "            \n",
    "            for i, run in enumerate(runs):\n",
    "                steps = np.array([safe_convert_to_float(step) for step in run['global_step_history']])\n",
    "                losses = np.array([safe_convert_to_float(loss) for loss in run['eval/mean_reward_history']])\n",
    "                \n",
    "                # Remove any NaN values\n",
    "                valid = ~np.isnan(steps) & ~np.isnan(losses)\n",
    "                steps = steps[valid]\n",
    "                losses = losses[valid]\n",
    "                \n",
    "                if len(steps) > 0 and len(losses) > 0:\n",
    "                    # Interpolate the losses to the common step range\n",
    "                    interpolated_losses = np.interp(common_steps, steps, losses)\n",
    "                    all_losses[i] = interpolated_losses\n",
    "            \n",
    "            mean_loss = np.nanmean(all_losses, axis=0)\n",
    "            std_loss = np.nanstd(all_losses, axis=0)\n",
    "            \n",
    "            ax.plot(common_steps, mean_loss, label=f\"Noise {noise}\", color=color, linewidth=2)\n",
    "            #ax.fill_between(common_steps, mean_loss - std_loss, mean_loss + std_loss, color=color, alpha=0.2)\n",
    "        \n",
    "        ax.set_title(f\"{feedback.capitalize()}\", fontsize=14)\n",
    "        ax.set_xlabel(\"Global Steps\", fontsize=14)\n",
    "        ax.set_ylabel(\"Reward\", fontsize=14)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "        ax.legend(fontsize=12, loc='lower right')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.92, hspace=0.4, wspace=0.3)\n",
    "    plt.savefig(f\"noise_rl_curves_{env}.png\", dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"Loss curves for {env} have been saved to noise_rl_curves_{env}.png\")\n",
    "\n",
    "# Create plots for each environment\n",
    "for env, feedback_runs in grouped_runs.items():\n",
    "    plot_environment(env, feedback_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10039da-2105-433a-9448-ceeb80b8c318",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlhfblender",
   "language": "python",
   "name": "rlhfblender"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
