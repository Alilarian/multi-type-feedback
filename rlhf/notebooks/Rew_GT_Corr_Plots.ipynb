{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3ced19b-fd55-42a9-9082-9f7c361371b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Define color scale for feedback types (matching your existing code)\n",
    "color_scale = OrderedDict([\n",
    "    ('evaluative', '#1f77b4'),     # blue\n",
    "    ('comparative', '#ff7f0e'),    # orange\n",
    "    ('demonstrative', '#2ca02c'),  # green\n",
    "    ('corrective', '#d62728'),     # red\n",
    "    ('descriptive', '#9467bd'),    # purple\n",
    "    ('descriptive_preference', '#8c564b'),     # brown\n",
    "])\n",
    "\n",
    "def extract_info(run_name):\n",
    "    \"\"\"Extract environment and feedback type from run name.\"\"\"\n",
    "    parts = run_name.split('_')\n",
    "    if \"descriptive_preference\" in run_name:\n",
    "        return parts[2], parts[3], \"descriptive_preference\", \n",
    "    \n",
    "    return parts[2], parts[3], parts[-2]\n",
    "\n",
    "def get_final_rewards(df, score_field=\"eval/mean_reward\"):\n",
    "    \"\"\"Extract final rewards from history.\"\"\"\n",
    "    final_rewards = {}\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        env, seed, feedback = extract_info(row['run_name'])\n",
    "        if env not in final_rewards:\n",
    "            final_rewards[env] = {}\n",
    "        if feedback not in final_rewards[env]:\n",
    "            final_rewards[env][feedback] = {}\n",
    "        final_rewards[env][feedback][seed] = row[score_field]\n",
    "    \n",
    "    return final_rewards\n",
    "\n",
    "def normalize_rewards(final_rewards, eval_df, environments, feedback_types):\n",
    "    \"\"\"Normalize rewards by mean expert score for each environment.\"\"\"\n",
    "    normalized_rewards = {}\n",
    "    \n",
    "    for env in environments:\n",
    "        env_scores = eval_df[eval_df['env'] == env]['eval_score']\n",
    "        if env not in normalized_rewards:\n",
    "            normalized_rewards[env] = {}\n",
    "        for feedback in feedback_types:\n",
    "            if feedback not in normalized_rewards[env]:\n",
    "                normalized_rewards[env][feedback] = {}\n",
    "            for seed in final_rewards[env][feedback].keys():\n",
    "                if not env_scores.empty:\n",
    "                    mean_expert_score = env_scores.mean()\n",
    "                    reward = final_rewards[env][feedback][seed]\n",
    "                    normalized_rewards[env][feedback][seed] = reward / mean_expert_score\n",
    "    \n",
    "    return normalized_rewards\n",
    "\n",
    "def create_scatter_plot(corr_data, rewards_data, environments, feedback_types, fb_type_list, env_list):\n",
    "    \"\"\"Create scatter plot of normalized rewards vs correlations with separate legends for feedback types and environments.\"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Increase font size\n",
    "    plt.rcParams.update({'font.size': 14})\n",
    "    \n",
    "    # Define markers for different environments\n",
    "    env_markers = {\n",
    "        env: marker for env, marker in zip(\n",
    "            env_list, \n",
    "            ['o', 's', '^', 'D', 'v', '<', '>', 'p', 'h', '8']  # Add more markers if needed\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Plot all points\n",
    "    for i in range(len(corr_data)):\n",
    "        plt.scatter(\n",
    "            corr_data[i],\n",
    "            rewards_data[i],\n",
    "            c=color_scale[feedback_types[i]],\n",
    "            marker=env_markers[environments[i]],\n",
    "            s=100,\n",
    "            alpha=0.7\n",
    "        )\n",
    "    \n",
    "    # Create two separate legend handles\n",
    "    feedback_handles = [plt.scatter([], [], c=color_scale[fb_type], \n",
    "                                  label=fb_type.capitalize().replace(\"Descriptive_preference\", \"Desc.Pref.\"), marker='o') \n",
    "                       for fb_type in fb_type_list]\n",
    "    \n",
    "    env_handles = [plt.scatter([], [], c='gray', \n",
    "                             marker=env_markers[env], \n",
    "                             label=env) \n",
    "                  for env in env_list]\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel('Correlation with Ground Truth')\n",
    "    plt.ylabel('Normalized Mean Reward')\n",
    "    #plt.title('Normalized Rewards vs Ground Truth Correlation')\n",
    "    \n",
    "    # Add grid\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add two legends\n",
    "    first_legend = plt.legend(handles=feedback_handles,\n",
    "                            title=\"Feedback Types\",\n",
    "                            bbox_to_anchor=(1.05, 1), \n",
    "                            loc='upper left',\n",
    "                            borderaxespad=0.)\n",
    "    \n",
    "    # Add the first legend manually to the plot\n",
    "    plt.gca().add_artist(first_legend)\n",
    "    \n",
    "    # Add second legend for environments\n",
    "    plt.legend(handles=env_handles,\n",
    "              title=\"Environments\",\n",
    "              bbox_to_anchor=(1.05, 0.5),  # Positioned below the first legend\n",
    "              loc='upper left',\n",
    "              borderaxespad=0.)\n",
    "    \n",
    "    # Set axis limits\n",
    "    plt.xlim(-0.1, 1.1)\n",
    "    \n",
    "    # Add horizontal line at y=1 (expert performance)\n",
    "    plt.axhline(y=1, color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Adjust layout to prevent legend cutoff\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    plt.savefig('reward_correlation_scatter.png', bbox_inches='tight', dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3aab9956-db19-47b3-9bd1-71ac5c8af3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correlations(normalized_rewards, correlations, seeds):\n",
    "    \"\"\"Extract correlations with ground truth for each run.\"\"\"\n",
    "    out_correlations = {}\n",
    "    \n",
    "    # Map feedback types to indices in the correlation matrix\n",
    "    feedback_indices = {\n",
    "        \"evaluative\": 1,\n",
    "        \"comparative\": 2,\n",
    "        \"demonstrative\": 3,\n",
    "        \"corrective\": 4,\n",
    "        \"descriptive\": 5,\n",
    "        \"descriptive_preference\": 6\n",
    "    }\n",
    "    \n",
    "    for env in normalized_rewards.keys():\n",
    "        out_correlations[env] = {}\n",
    "        for feedback in normalized_rewards[env].keys():\n",
    "            out_correlations[env][feedback] = {}\n",
    "            for seed in seeds:    \n",
    "                if env in all_env_rewards:\n",
    "                    corr, _ = pearsonr(correlations[env][int(seed)][0], correlations[env][int(seed)][feedback_indices[feedback]])\n",
    "                    out_correlations[env][feedback][seed] = corr\n",
    "    \n",
    "    return out_correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd780651-1545-4c93-a425-b2b10bdfd299",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mymetz\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/pfs/data5/home/kn/kn_kn/kn_pop257914/multi-type-feedback/rlhf/notebooks/wandb/run-20241124_061134-q34vcgm6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ymetz/your_project_name/runs/q34vcgm6' target=\"_blank\">prime-shadow-72</a></strong> to <a href='https://wandb.ai/ymetz/your_project_name' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ymetz/your_project_name' target=\"_blank\">https://wandb.ai/ymetz/your_project_name</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ymetz/your_project_name/runs/q34vcgm6' target=\"_blank\">https://wandb.ai/ymetz/your_project_name/runs/q34vcgm6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(project=\"your_project_name\")\n",
    "\n",
    "# Fetch runs from your project\n",
    "api = wandb.Api()\n",
    "runs = api.runs(\"multi_reward_feedback_final_lul\", filters={\"display_name\": {\"$regex\": \"^RL_.*\"}})\n",
    "\n",
    "# Create a list to store data from filtered runs\n",
    "filtered_run_data = []\n",
    "\n",
    "# Iterate through the runs\n",
    "for run in runs:\n",
    "    # Check if the run name starts with \"ppo_\"\n",
    "    if run.name.startswith(\"RL_\") and \"noise\" not in run.name and \"ensemble\" not in run.name:\n",
    "        # Get the summary statistics (includes final values of metrics)\n",
    "        summary = run.summary._json_dict\n",
    "\n",
    "        # Get the history (includes all logged metrics)\n",
    "        history = run.history(keys=[\"eval/mean_reward\", \"global_step\"])\n",
    "\n",
    "        # Combine summary and history data\n",
    "        run_data = {\n",
    "            \"run_id\": run.id,\n",
    "            \"run_name\": run.name,\n",
    "            **summary,\n",
    "            **{f\"{k}_history\": v.tolist() for k, v in history.items()}\n",
    "        }\n",
    "\n",
    "        filtered_run_data.append(run_data)\n",
    "\n",
    "# Create a DataFrame from filtered run data\n",
    "orig_df = pd.DataFrame(filtered_run_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "945a1907-12b2-40e5-9a30-a9692243ed76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import pickle\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Load evaluation scores\n",
    "eval_df = pd.read_csv(\"../../main/gt_agents/collected_results.csv\")\n",
    "\n",
    "# Get final rewards from history\n",
    "final_rewards = get_final_rewards(orig_df)\n",
    "\n",
    "# Extract environments and feedback types\n",
    "environments = [\"HalfCheetah-v5\", \"Walker2d-v5\", \"Swimmer-v5\", \"Ant-v5\", \"Hopper-v5\", \"Humanoid-v5\"]\n",
    "feedback_types = [\"evaluative\", \"comparative\", \"demonstrative\", \"corrective\", \"descriptive\", \"descriptive_preference\"]\n",
    "\n",
    "# Normalize rewards\n",
    "normalized_rewards = normalize_rewards(final_rewards, eval_df, environments, feedback_types)\n",
    "\n",
    "# Load correlation data\n",
    "environments_dict = {\"ppo\": [\"HalfCheetah-v5\", \"Walker2d-v5\", \"Swimmer-v5\"], \n",
    "                    \"sac\": [\"Ant-v5\", \"Hopper-v5\", \"Humanoid-v5\"]}\n",
    "algo = [\"ppo\", \"sac\"]\n",
    "#noise = [0.0, 0.1, 0.25, 0.5, 0.75]\n",
    "noise = 0.0\n",
    "seeds = [1687123, 1789, 12]\n",
    "\n",
    "# Load correlation data\n",
    "all_env_rewards = {}\n",
    "for i, alg in enumerate(algo):\n",
    "    for env in environments_dict[alg]:\n",
    "        all_env_rewards[env] = {}\n",
    "        for seed in seeds:\n",
    "            file_name = f\"corr_{env}_{alg}_noise_{noise}_{seed}.pkl\"\n",
    "            with open(os.path.join(\"../correlation_data\", file_name), \"rb\") as load_file:\n",
    "                load_rewards = pickle.load(load_file)\n",
    "                pred_rewards = np.array(load_rewards)\n",
    "            all_env_rewards[env][seed] = pred_rewards\n",
    "\n",
    "# Get correlations\n",
    "correlations = get_correlations(normalized_rewards, all_env_rewards, seeds)\n",
    "\n",
    "plot_correlations = []\n",
    "plot_rewards = []\n",
    "plot_envs = []\n",
    "plot_fb_types = []\n",
    "for env in environments:\n",
    "    for feedback_type in feedback_types:\n",
    "        for seed in seeds:\n",
    "                if str(seed) not in normalized_rewards[env][feedback_type]:\n",
    "                    continue\n",
    "                plot_correlations.append(correlations[env][feedback_type][seed])\n",
    "                plot_rewards.append(normalized_rewards[env][feedback_type][str(seed)])\n",
    "                plot_envs.append(env)\n",
    "                plot_fb_types.append(feedback_type)\n",
    "\n",
    "# Create scatter plot\n",
    "create_scatter_plot(plot_correlations, plot_rewards, plot_envs, plot_fb_types, feedback_types, environments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlhf-env",
   "language": "python",
   "name": "rlhf-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
